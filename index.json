[{"content":"Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.\nWhy is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:\n  It is reliant on long-standing credentials being stored in Github Actions. Some environments are unable to generate such long-standing credentials without serious admin intervention (e.g. environments using CloudTamer/Kion).\n  It grants any user with write-access to the repo with full use of the possibly wide-scoped credentials. Currently, within Github there are not a sufficient ways to limit who or what can be done with the credentials. For example, any user with write-access to the repo could create a workflow action that references the production credentials and uses them to teardown the production environment. This is due to the combination of two factors:\n The instructions run during the build are entirely specified within the Github repo. This means that anyone can alter them as they see fit. The cloud providers lacks information about the context of a build (e.g. git branch or github user), and is therefore unable to apply or enforce any sort of restrictions regarding what a build can do.  Github Environments solves some of these problems, however at time of writing it is only available on public repositories or private Github Enterprise account repositories, making it an unvialable solution for many of our partners.\n  Solution: OpenID Connect On Nov 23, 2021 Github Actions announced the general availability of support for OpenID Connect (OIDC). For an in-depth understanding of this, I recommend reviewing the following links:\n Announcement: Secure deployments with OpenID Connect \u0026amp; GitHub Actions now generally available Docs: Security hardening your deployments [with OpenID Connect]  High level summary With OIDC, you can register Github as an Identity Provider within your cloud platform of choice. When your Github Action workflows run, they can be setup to request short-lived credentials from your cloud provider. When the cloud provider grants the access token, it will be associated with a particular IAM Role. That IAM Role should be set up with the permissions necessary for deploying your application.\n No need to store credentials. Github Actions workflows will request a short-lived access token at runtime. When a short-lived access token is requested, Github Actions sends an OIDC token with claims describing the context of the workflow (link). These claims can be interogated by the cloud provider and used to determine whether or not a token should be granted. This allows us to hardcode security requirements (e.g. limiting particular IAM Roles to specific Github branches, only allowing executions triggered by specified github usernames) in the cloud provider, providing guard-rails to limit what can be done by any particular user with write-access to a Github repository.  Example with AWS Setup AWS Setting up OIDC with AWS is described in depth here, however the following is a quick summary:\n  Add Github as an Identity provider (docs).\n Console Screenshot    Create an IAM Policy for deployment executions. See recommendations for tips on how to craft this policy. Below is an example policy for frontend static website deployments:\n Example policy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;SyncS3Bucket\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-staging-bucket\u0026#34;, \u0026#34;arn:aws:s3:::my-staging-bucket/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;InvalidateCloudfrontDistribution\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudfront:CreateInvalidation\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }     Create role for deployment executions.\n Console Screenshot  Attach your revelant policies. Optionally, specify the role\u0026rsquo;s permission boundaries.\n Console Screenshot  For this example, we\u0026rsquo;ll be naming our role Frontend-Staging-Deployment-Role\n Console Screenshot    By default, the IAM Role created for our OIDC Web Identity contains a condition where the aud claim in our token should match sts.amazonaws.com. However, by default the aud will be the URL of the repository owner. As such, we need to customize our trust relationship to encorce custom conditions.\n Console Screenshot  In the following example, we configure the trust relationship to enforce that this role can only be used on builds on the my-org/my-repo repository\u0026rsquo;s staging branch:\n Example conditions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::123456789000:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:my-org/my-repo:ref:refs/heads/staging\u0026#34; } } } ] }      Setup Workflow Workflows utilizing OIDC need a few particular elements.\n  We need to customize the permissions of our GITHUB_TOKEN via the permissions block. The workflow will need to be able to write an id-token along with the default permissions of reading the contents of the repository:\n Permissions block 1 2 3  permissions:id-token:writecontents:read      Add tooling to request the an access token from AWS. For this, the AWS' offical \u0026ldquo;Configure AWS Credentials\u0026rdquo; Action works well. To use this, you must provide the ARN of the role that you would like to assume in your execution:\n AWS Configuration step 1 2 3 4 5  - name:Configure AWS credentialsuses:aws-actions/configure-aws-credentials@v1with:role-to-assume:arn:aws:iam::123456789000:role/Frontend-Staging-Deployment-Roleaws-region:us-west-2     Full example  A complete example workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  name:Deploy Staging Frontendon:push:branches:- stagingpermissions:id-token:writecontents:readjobs:build:runs-on:ubuntu-lateststeps:- name:Setup Node.jsuses:actions/setup-node@v2with:node-version:12- name:Check out repository codeuses:actions/checkout@v2- name:Install dependenciesrun:npm install- name:Build coderun:npm run build- name:Configure AWS credentialsuses:aws-actions/configure-aws-credentials@v1with:role-to-assume:arn:aws:iam::123456789000:role/Frontend-Staging-Deployment-Roleaws-region:us-west-2- name:Sync with S3 bucketenv:BUCKET:my-staging-bucketrun:|aws s3 sync \\ ./build \u0026#34;s3://${BUCKET}\u0026#34; \\ --acl public-read \\ --follow-symlinks \\ --delete- name:Invalidate CloudFrontenv:DISTRIBUTION:EDFDVBD6EXAMPLErun:|aws cloudfront create-invalidation \\ --distribution-id $DISTRIBUTION \\ --paths \u0026#34;/*\u0026#34;  In the above example, we have hardcoded the Role ARN, S3 Bucket, and Cloudfront Distribution ID in the workflow file. However, you may prefer to store these values as Github Secrets. This allows the values to be changed without a code change and additionally helps avoid data-leak. An example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  - name:Configure AWS credentialsuses:aws-actions/configure-aws-credentials@v1with:role-to-assume:${{ secrets.STAGING_CD_ROLE_ARN }}aws-region:us-west-2- name:Sync with S3 bucketenv:BUCKET:${{ secrets.STAGING_BUCKET_NAME }}run:|aws s3 sync \\ ./build \u0026#34;s3://${BUCKET}\u0026#34; \\ --acl public-read \\ --follow-symlinks \\ --delete- name:Invalidate CloudFrontenv:DISTRIBUTION:${{ secrets.STAGING_DISTRIBUTION_ID }}run:|aws cloudfront create-invalidation \\ --distribution-id $DISTRIBUTION \\ --paths \u0026#34;/*\u0026#34;   Recommendations  Each IAM role should relate to a single deployment. For example, you may have a Service-X-Frontend-Staging-Deployment role and a Service-X-Frontend-Production-Deployment role, each referencing specific IAM policies that specify the minimal permissions needed to deploy to its respective environment. Each role should specify which repositories and branches can use the role. Configuring the IAM role\u0026rsquo;s trust relationship is key to enforcing logic around deployment permissions. Understanding the Condition block and the Github OIDC token is paramount.  ","permalink":"https://alukach.com/posts/oidc-github-actions/","summary":"Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.\nWhy is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:","title":"Security-conscious cloud deployments from Github Actions via OpenID Connect"},{"content":"Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.\nOther Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.\nBackground: Project Infrastructure Our production frontend is a React application (using Next.js). We build this application into static HTML/CSS/JS files and upload them to an S3 bucket. This bucket has been setup to serve static websites and is served via HTTPS by CloudFront (see #270).\nThe frontend accesses multiple backend APIs (e.g. a STAC API, a FastAPI REST API). Deployment of those APIs is outside of the scope of the frontend codebase.\nCI: Cloud Infrastructure Our goal is to mimic the production frontend deployment to a reasonable degree.\n1. Setup a bucket First, we will need an S3 bucket to store our builds. We created a bucket (s3://project-frontend-ci) and configured it to serve static websites. As a sanity check, we wrote a simple message to a public file that we place at the root of the bucket:\n1  echo \u0026#34;Project frontend CI builds\u0026#34; | aws s3 cp - s3://project-frontend-ci/index.html --acl public-read --content-type text/html   We can verify that everything is configured properly by visiting the bucket\u0026rsquo;s website url: http://project-frontend-ci.s3-website.us-east-1.amazonaws.com/\n2. Setup SSL via API Gateway Unfortunately, S3 does not support serving content via SSL (i.e. over HTTPS). As such, we need to use something like API Gateway or CloudFront to accept traffic over HTTPS and to route it to our bucket\u0026rsquo;s content over HTTP. For the sake of simplicity, we chose to use API Gateway for this purpose. For our actual staging environment, we utilize CloudFront to more closely mimic the production environment, however for these temporary CI builds we felt that API Gateway was close-enough.\nWe created an API Gateway HTTP API configured to direct all traffic to our S3 bucket\u0026rsquo;s website URL: We can now visit our bucket over SSL via the API Gateway endpoint: https://0zy9z5ko27.execute-api.us-east-1.amazonaws.com/\n3. Setup a URL for the CI builds The downside of API Gateway or CloudFront is that it produces very unmemorable URLs. Just to be a bit fancy 💅 , we set up a custom URL on domain.com. We settled on ci.project-staging.domain.com to pair nicely with our staging environment (project-staging.domain.com).\n Setting this up is a bit of a multi-step process. Click here to see the details... a. Create SSL Certificate On the AWS account owns the API Gateway HTTP API we just setup, we created an SSL Certificate via AWS Certificate Manager (ACM):\nb. Verify ownership of domain ACM requires that you verify that you have control of a domain before it will grant you an SSL certificate. After creating an SSL certificate, you\u0026rsquo;ll see that it is in \u0026ldquo;Pending Validation\u0026rdquo; status.\nTo verify that we control domain.com, we add a CNAME record to the domain.com hosted zone. Once this is done, we frantically refresh the ACM status page until it states that our domain has been verified.\nc. Setup API Gateway custom domain Back over to API Gateway, we set up a custom domain.\nAfter creating the custom domain, we add an API mapping to our HTTP API.\nd. Creating a DNS entry for our new URL We now want to instruct Route53 to direct all traffic sent to our URL (ci.project-staging.domain.com) to our new API Gateway custom domain. To do this, we copy the API Gateway domain name.\nWe use the copied API Gateway domain name to create a new DNS entry to facilitate this mapping:\n After this, we should be able to see our sanity-check message at https://ci.project-staging.domain.com.\nCI: Workflows Building \u0026amp; Deploying Our goal is to build a version of our frontend application for every pull request and have it available at https://ci.project-staging.domain.com. Our chosen strategy was to build each PR and to place the build in a path prefixed with the PR number (i.e. PR 208 should be available at https://ci.project-staging.domain.com/208). To facilitate this, your frontend application must be configured to allow it to be served from non-route paths. In the case of NextJS, this is done via the Base Path configuration.\nBuilding and Deploying the application via Github actions is pretty straightforward.\n Example of a simple build/deploy Github Actions workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  name:Deploy to CI environmenton:pull_request:jobs:build-and-deploy:runs-on:ubuntu-lateststeps:- name:Cancel Previous Runsuses:styfle/cancel-workflow-action@0.8.0with:access_token:${{ github.token }}- name:Checkoutuses:actions/checkout@v2- name:Use Node.js 14uses:actions/setup-node@v1with:node-version:14- name:Cache node modulesuses:actions/cache@v2env:cache-name:cache-node-moduleswith:path:node_moduleskey:${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles(\u0026#39;**/yarn.lock\u0026#39;) }}restore-keys:|${{ runner.os }}-build-${{ env.cache-name }}- ${{ runner.os }}-build- ${{ runner.os }}-- name:Build and Exportid:buildenv:NEXT_PUBLIC_BASE_URL:https://ci.project-staging.domain.com/${{ github.event.pull_request.number }}NEXT_PUBLIC_STAC_API:${{ \u0026#39;https://project-staging.domain.com/stac\u0026#39; }}NEXT_PUBLIC_ORDERS_API:${{ \u0026#39;https://project-staging.domain.com/api\u0026#39; }}run:|yarn install yarn build yarn run next export- name:Configure AWS credentials from staging accountuses:aws-actions/configure-aws-credentials@v1with:aws-access-key-id:${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}aws-secret-access-key:${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}aws-region:us-east-1- name:Deploy 🚀run:|aws s3 sync \\ ./out \\ s3://project-frontend-ci/${{ github.event.pull_request.number }} \\ --delete \\ --acl public-read  You can see that we pass in our Base URL and external APIs via the env at build time and that we have our AWS credentials available as encrypted secrets.\nNote that, as per the Github docs, the pull_request event only triggers when a PR is opened, updated, or re-opened:\n By default, a workflow only runs when a pull_request\u0026rsquo;s activity type is opened, synchronize, or reopened.\n  Adding a comment on our PR to notify others of the build Once the CI has built and deployed a new instance of our frontend, we want to notify others (e.g. those reviewing PRs) where they can view the build. To do this, we add the following steps to our Github Actions workflow to take place after our build:\n Example of jobs to add comments to a PR 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  jobs:build-and-deploy:steps:# ...- name:Get current timeuses:gerred/actions/current-time@masterid:current-time- name:Find Commentuses:peter-evans/find-comment@v1id:find-commentwith:issue-number:${{ github.event.pull_request.number }}comment-author:\u0026#34;github-actions[bot]\u0026#34;body-includes:Latest commit deployed to- name:Create or update commentuses:peter-evans/create-or-update-comment@v1with:comment-id:${{ steps.find-comment.outputs.comment-id }}issue-number:${{ github.event.pull_request.number }}body:|🚀 Latest commit deployed to https://ci.project-staging.domain.com/${{ github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` * Commit: ${{ github.sha }} (Merging ${{ github.event.pull_request.head.sha }} into ${{ github.event.pull_request.base.sha }})edit-mode:replace   These steps add a new comment to PRs, looking something like this:\nFor later commits to the PR, the original comment will be replaced rather than creating another comment. This helps us avoid littering user\u0026rsquo;s notifications and keeps a clean comment thread.\nAllow users to customize configuration As previously mentioned, the frontend connects to multiple backing APIs. By default, the CI builds point to our staging APIs. However, it\u0026rsquo;s a realistic scenario that a developer would want their custom environment to point to a different API release (e.g. a developer is working on frontend changes in tandem with changes being made to the backend API). To support this, we want to allow developers to manually override certain configurations. To do this, we added the workflow_dispatch trigger to our workflow, allowing for manual workflow runs. We also add inputs for each configuration we want to allow a developer to specify.\n Example of adding workflow_dispatch event with inputs to your workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  name:Deploy to CI environmenton:pull_request:workflow_dispatch:inputs:stac-api-url:description:Override STAC API URLdefault:https://project-staging.domain.com/stacorders-api-url:description:Override Orders API URLdefault:https://project-staging.domain.com/apideployment-id:description:Unique identifier for build (used to construct path for upload)required:truejobs:build-and-deploy:runs-on:ubuntu-lateststeps:# ...- name:Build and Exportid:buildenv:NEXT_PUBLIC_BASE_URL:https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}NEXT_PUBLIC_STAC_API:${{ github.event.inputs.stac-api-url || \u0026#39;https://project-staging.domain.com/stac\u0026#39; }}NEXT_PUBLIC_ORDERS_API:${{ github.event.inputs.orders-api-url || \u0026#39;https://project-staging.domain.com/api\u0026#39; }}NEXT_PUBLIC_MB_TOKEN:pk.eyJ1IjoiZGV2c2VlZCIsImEiOiJjazB6YXU2bDUwMWNkM2VvNGNpMnFhOXMxIn0.c30a2TQIfCDF3GlqMdSQ_gNEXT_PUBLIC_GA_ID:GTM-WNP7MLFrun:|yarn install yarn build yarn run next export- name:Get current timeuses:gerred/actions/current-time@masterif:${{ github.event.pull_request.number }}# ...- name:Find Commentuses:peter-evans/find-comment@v1if:${{ github.event.pull_request.number }}# ...- name:Create or update commentuses:peter-evans/create-or-update-comment@v1if:${{ github.event.pull_request.number }}# ...   You\u0026rsquo;ll note that any place where we originally specified our API configuration or had a dependency on a PR number, we now first try to retrieve the value from github.event.inputs (only available during manual workflow_dispatch events) and otherwise fall back to values used during standard PR builds. This can be achieved by utilizing the OR operator as so: ${{ github.event.inputs.deployment-id || github.event.pull_request.number }}.\nAdditionally, we will only want to comment on a PR during PR builds, so we avoid running the comment steps by adding an if: ${{ github.event.pull_request.number }} clause to each step that we want to skip.\nCleanup After each PR is merged, we want to clean up the past build to avoid unnecessary storage in our CI bucket. This can be achieved with another workflow that cleans up the build whenever a pull request is closed.\n Example of a workflow to destroy CI builds 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  name:Destroy PR Previewon:pull_request:types:[closed]workflow_dispatch:inputs:deployment-id:description:Unique identifier of CI build to be deletedrequired:truejobs:build-and-deploy:runs-on:ubuntu-lateststeps:# ...- name:Destroy 💣run:|aws s3 rm --recursive s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}/- name:Get current timeuses:gerred/actions/current-time@masterif:${{ github.event.pull_request.number }}id:current-time- name:Find Commentuses:peter-evans/find-comment@v1if:${{ github.event.pull_request.number }}id:find-commentwith:issue-number:${{ github.event.pull_request.number }}comment-author:\u0026#34;github-actions[bot]\u0026#34;body-includes:Latest commit deployed to- name:Create or update commentuses:peter-evans/create-or-update-comment@v1if:${{ github.event.pull_request.number }}with:comment-id:${{ steps.find-comment.outputs.comment-id }}issue-number:${{ github.event.pull_request.number }}body:|--- 🧹 Deleted build at https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}`edit-mode:append   Our pull request message is then appended with information to let others know that the build environment is no longer available.\nPutting it all together To achieve our goals of deployment, notification, customization, and cleanup, we have settled on these two Github workflows:\n .github/workflows/deploy-pr-preview.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100  name:Deploy to CI environmenton:pull_request:workflow_dispatch:inputs:stac-api-url:description:Override STAC API URLdefault:https://project-staging.domain.com/stacorders-api-url:description:Override Orders API URLdefault:https://project-staging.domain.com/apideployment-id:description:Unique identifier for build (used to construct path for upload)required:truejobs:build-and-deploy:runs-on:ubuntu-lateststeps:- name:Cancel Previous Runsuses:styfle/cancel-workflow-action@0.8.0with:access_token:${{ github.token }}- name:Checkoutuses:actions/checkout@v2- name:Use Node.js 14uses:actions/setup-node@v1with:node-version:14- name:Cache node modulesuses:actions/cache@v2env:cache-name:cache-node-moduleswith:path:node_moduleskey:${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles(\u0026#39;**/yarn.lock\u0026#39;) }}restore-keys:|${{ runner.os }}-build-${{ env.cache-name }}- ${{ runner.os }}-build- ${{ runner.os }}-- name:Build and Exportid:buildenv:NEXT_PUBLIC_BASE_URL:https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}NEXT_PUBLIC_STAC_API:${{ github.event.inputs.stac-api-url || \u0026#39;https://project-staging.domain.com/stac\u0026#39; }}NEXT_PUBLIC_ORDERS_API:${{ github.event.inputs.orders-api-url || \u0026#39;https://project-staging.domain.com/api\u0026#39; }}NEXT_PUBLIC_MB_TOKEN:pk.eyJ1IjoiZGV2c2VlZCIsImEiOiJjazB6YXU2bDUwMWNkM2VvNGNpMnFhOXMxIn0.c30a2TQIfCDF3GlqMdSQ_gNEXT_PUBLIC_GA_ID:GTM-WNP7MLFrun:|yarn install yarn build yarn run next export- name:Configure AWS credentials from staging accountuses:aws-actions/configure-aws-credentials@v1with:aws-access-key-id:${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}aws-secret-access-key:${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}aws-region:us-east-1- name:Deploy 🚀run:|aws s3 sync \\ ./out \\ s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} \\ --delete \\ --acl public-read- name:Get current timeuses:gerred/actions/current-time@masterif:${{ github.event.pull_request.number }}id:current-time- name:Find Commentuses:peter-evans/find-comment@v1if:${{ github.event.pull_request.number }}id:find-commentwith:issue-number:${{ github.event.pull_request.number }}comment-author:\u0026#34;github-actions[bot]\u0026#34;body-includes:Latest commit deployed to- name:Create or update commentuses:peter-evans/create-or-update-comment@v1if:${{ github.event.pull_request.number }}with:comment-id:${{ steps.find-comment.outputs.comment-id }}issue-number:${{ github.event.pull_request.number }}body:|🚀 Latest commit deployed to https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` * Commit: ${{ github.sha }} (merging ${{ github.event.pull_request.head.sha }} into ${{ github.event.pull_request.base.sha }})edit-mode:replace    .github/workflows/destroy-pr-preview.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  name:Destroy PR Previewon:pull_request:types:[closed]workflow_dispatch:inputs:deployment-id:description:Unique identifier of CI build to be deletedrequired:truejobs:build-and-deploy:runs-on:ubuntu-lateststeps:- name:Cancel Previous Runsuses:styfle/cancel-workflow-action@0.8.0with:access_token:${{ github.token }}- name:Configure AWS credentials from staging accountuses:aws-actions/configure-aws-credentials@v1with:aws-access-key-id:${{ secrets.STAGING_AWS_ACCESS_KEY_ID }}aws-secret-access-key:${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }}aws-region:us-east-1- name:Destroy 💣run:|aws s3 rm --recursive s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}/- name:Get current timeuses:gerred/actions/current-time@masterif:${{ github.event.pull_request.number }}id:current-time- name:Find Commentuses:peter-evans/find-comment@v1if:${{ github.event.pull_request.number }}id:find-commentwith:issue-number:${{ github.event.pull_request.number }}comment-author:\u0026#34;github-actions[bot]\u0026#34;body-includes:Latest commit deployed to- name:Create or update commentuses:peter-evans/create-or-update-comment@v1if:${{ github.event.pull_request.number }}with:comment-id:${{ steps.find-comment.outputs.comment-id }}issue-number:${{ github.event.pull_request.number }}body:|--- 🧹 Deleted build at https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}`edit-mode:append    This system is very new for us and required some additional changes to other services (i.e. updating CORS rules on our APIs to allow us to use this new URL), but so far it seems to be operating as expected. Hoping that this can help others who are looking to build out better CI preview environments for their applications.\n Followup  this is super cool but was there a reason we couldn’t use netlify or another third party service for this?\n This is a fair question. We opted to roll our own solution being that the general idea (uploading builds to S3) was something that we were already doing for deployments to our Production and Staging environments. However, if you\u0026rsquo;re starting a new project, you may be interested in achieving per-PR deployments with a third party tool. It appears that most common hosting solutions offer something for this:\n Netlify offers Deploy Previews Vercel offers Preview URLs via its Github Integration AWS Amplify offers Web Previews Surge.sh can be configured to preview URLs via the surge-preview Github Action  ","permalink":"https://alukach.com/posts/diy-pr-previews/","summary":"Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.\nOther Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.","title":"Roll your own PR preview CI pipeline"},{"content":"Have you just written a new ✨fancy CLI✨ and want to demo it in your Github Readme? Recording your terminal output is a nice way to demonstrate the experience.\nHere\u0026rsquo;s an example of what we\u0026rsquo;re going to make:\nSteps Install Dependencies  asciinema: brew install asciinema svg-term-cli: npm install -g svg-term-cli  Setup your terminal Some tips:\n Font/screen size matters. The asciinema output will look just as it does in your terminal. You\u0026rsquo;ll probably want to bump up the font-size and shrink down the terminal so that the text is legible in your README. Choose a nice, generic terminal theme. If using ohmyzsh, I recommend the arrow theme.  Lights, camera, action 🎬 The asciinema rec command will start a process to capture the stdout of your terminal session. When you\u0026rsquo;re done, pressing ctrl+d will end recording. You can then opt to save it to the asciinema server or locally. For our needs, we\u0026rsquo;ll save locally.\nPost Production ✂️🎞 asciinema will place the file in a temporary directory. The output \u0026ldquo;cast\u0026rdquo; is a simple newline-delimeted JSON file, which a savvy user could edit by hand to scrub out any mistakes or personal information.\n example asciinema cast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369  {\u0026#34;version\u0026#34;: 2, \u0026#34;width\u0026#34;: 82, \u0026#34;height\u0026#34;: 20, \u0026#34;timestamp\u0026#34;: 1632512435, \u0026#34;env\u0026#34;: {\u0026#34;SHELL\u0026#34;: \u0026#34;/bin/zsh\u0026#34;, \u0026#34;TERM\u0026#34;: \u0026#34;xterm-256color\u0026#34;}} [0.3937, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [0.401958, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [0.402028, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [1.397944, \u0026#34;o\u0026#34;, \u0026#34;asciinema rec\u0026#34;] [2.038988, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\u0026#34;] [2.039271, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [2.040939, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;asciinema rec\\u0007\\u001b]1;asciinema\\u0007\u0026#34;] [2.12754, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: recording asciicast to /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[0m\\r\\n\\u001b[0;32masciinema: press \u0026lt;ctrl-d\u0026gt; or type \\\u0026#34;exit\\\u0026#34; when you\u0026#39;re done\\u001b[0m\\r\\n\u0026#34;] [2.531341, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [2.539224, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [2.539294, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [3.437282, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [3.573817, \u0026#34;o\u0026#34;, \u0026#34;\\bpy\u0026#34;] [3.751574, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [3.813398, \u0026#34;o\u0026#34;, \u0026#34;h\u0026#34;] [3.893245, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [3.993249, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [4.091641, \u0026#34;o\u0026#34;, \u0026#34;3\u0026#34;] [4.227577, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [4.631549, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [4.759173, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [4.858853, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [4.992652, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [5.028749, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00mpython3 /tmp\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[63C\\u001b[33m \\u001b[00m\\u001b[64D\u0026#34;] [5.257958, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/e\u0026#34;] [5.436696, \u0026#34;o\u0026#34;, \u0026#34;x\u0026#34;] [5.7675, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [5.776614, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00mpython3 /tmp/example.py\\u001b[1m \\u001b[0m\\u001b[K\\u001b[52C\\u001b[33m \\u001b[00m\\u001b[53D\u0026#34;] [6.590538, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m \\b\u0026#34;] [6.590824, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\\u001b[?2004l\u0026#34;] [6.590975, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\n\u0026#34;] [6.592687, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;python3 /tmp/example.py\\u0007\\u001b]1;python3\\u0007\u0026#34;] [6.656795, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\u0026#34;] [6.656895, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.656981, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\u0026#34;] [6.65715, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.657262, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\u0026#34;] [6.657347, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.657453, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.657583, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r#4, est. 0.83s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.657683, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.657779, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\u0026#34;] [6.657931, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658118, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658213, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658368, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658434, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658526, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658623, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.757006, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.757143, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 13%|███ | 647/5000 [00:00\u0026lt;00:00, 6468.12it/s]\\u001b[A\u0026#34;] [6.757228, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 7%|█▌ | 334/5000 [00:00\u0026lt;00:01, 3327.45it/s]\u0026#34;] [6.757531, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.757582, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 5%|█▏ | 254/5000 [00:00\u0026lt;00:01, 2533.69it/s]\\u001b[A\\u001b[A\u0026#34;] [6.757652, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.757714, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 10%|██▎ | 492/5000 [00:00\u0026lt;00:00, 4915.24it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.75778, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.75784, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 3%|▊ | 169/5000 [00:00\u0026lt;00:02, 1683.67it/s]\u0026#34;] [6.757884, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758049, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758097, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 11%|██▋ | 567/5000 [00:00\u0026lt;00:00, 5665.80it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758387, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758455, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 8%|█▉ | 409/5000 [00:00\u0026lt;00:01, 4082.16it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758509, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758555, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 14%|███▍ | 716/5000 [00:00\u0026lt;00:00, 7159.09it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758726, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758759, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 16%|███▊ | 790/5000 [00:00\u0026lt;00:00, 7890.77it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.857141, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.85724, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 26%|█████▉ | 1302/5000 [00:00\u0026lt;00:00, 6511.76it/s]\\u001b[A\u0026#34;] [6.857278, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 13%|███▏ | 674/5000 [00:00\u0026lt;00:01, 3368.38it/s]\u0026#34;] [6.85771, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.857762, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 20%|████▋ | 986/5000 [00:00\u0026lt;00:00, 4928.32it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858337, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858396, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 23%|█████▏ | 1134/5000 [00:00\u0026lt;00:00, 5658.36it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858422, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858496, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 29%|██████▌ | 1434/5000 [00:00\u0026lt;00:00, 7170.93it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858554, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858584, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 16%|███▉ | 819/5000 [00:00\u0026lt;00:01, 4089.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858642, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858674, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 7%|█▌ | 338/5000 [00:00\u0026lt;00:02, 1679.15it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858728, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.858775, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 10%|██▍ | 508/5000 [00:00\u0026lt;00:01, 2519.60it/s]\\u001b[A\\u001b[A\u0026#34;] [6.858828, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.85887, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 32%|███████▎ | 1585/5000 [00:00\u0026lt;00:00, 7921.04it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.957251, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.957276, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 39%|████████▉ | 1954/5000 [00:00\u0026lt;00:00, 6513.12it/s]\\u001b[A\u0026#34;] [6.957367, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 20%|████▋ | 1012/5000 [00:00\u0026lt;00:01, 3372.64it/s]\u0026#34;] [6.958144, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958194, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 30%|██████▊ | 1479/5000 [00:00\u0026lt;00:00, 4919.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958383, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958432, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 34%|███████▊ | 1703/5000 [00:00\u0026lt;00:00, 5671.37it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958528, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958576, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 43%|█████████▉ | 2161/5000 [00:00\u0026lt;00:00, 7212.74it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958631, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.95866, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 25%|█████▋ | 1233/5000 [00:00\u0026lt;00:00, 4110.89it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958857, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958912, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 48%|██████████▉ | 2384/5000 [00:00\u0026lt;00:00, 7950.66it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.959242, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.959296, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 10%|██▍ | 506/5000 [00:00\u0026lt;00:02, 1674.81it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.959352, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.959393, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 15%|███▋ | 760/5000 [00:00\u0026lt;00:01, 2512.95it/s]\\u001b[A\\u001b[A\u0026#34;] [7.057416, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.057472, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 52%|███████████▉ | 2608/5000 [00:00\u0026lt;00:00, 6519.76it/s]\\u001b[A\u0026#34;] [7.057578, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 27%|██████▏ | 1351/5000 [00:00\u0026lt;00:01, 3376.65it/s]\u0026#34;] [7.058204, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058246, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 39%|█████████ | 1974/5000 [00:00\u0026lt;00:00, 4930.20it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058631, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058679, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 58%|█████████████▎ | 2890/5000 [00:00\u0026lt;00:00, 7240.83it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058766, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058825, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 45%|██████████▍ | 2271/5000 [00:00\u0026lt;00:00, 5666.41it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058875, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 33%|███████▌ | 1647/5000 [00:00\u0026lt;00:00, 4118.91it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058936, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058983, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 64%|██████████████▋ | 3182/5000 [00:00\u0026lt;00:00, 7961.79it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.059486, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.059523, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 20%|████▋ | 1012/5000 [00:00\u0026lt;00:01, 2514.31it/s]\\u001b[A\\u001b[A\u0026#34;] [7.059677, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.059715, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 14%|███▏ | 676/5000 [00:00\u0026lt;00:02, 1681.78it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.157472, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.157542, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 65%|███████████████ | 3266/5000 [00:00\u0026lt;00:00, 6538.76it/s]\\u001b[A\u0026#34;] [7.157691, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 34%|███████▊ | 1689/5000 [00:00\u0026lt;00:00, 3376.30it/s]\u0026#34;] [7.158976, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159029, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 49%|███████████▎ | 2468/5000 [00:00\u0026lt;00:00, 4919.98it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159074, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159127, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 41%|█████████▍ | 2061/5000 [00:00\u0026lt;00:00, 4123.10it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159181, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159211, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 57%|█████████████ | 2838/5000 [00:00\u0026lt;00:00, 5659.82it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159252, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159291, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 80%|██████████████████▎ | 3979/5000 [00:00\u0026lt;00:00, 7954.54it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159639, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159686, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 72%|████████████████▋ | 3615/5000 [00:00\u0026lt;00:00, 7217.32it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159745, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.159798, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 25%|█████▊ | 1264/5000 [00:00\u0026lt;00:01, 2513.88it/s]\\u001b[A\\u001b[A\u0026#34;] [7.159854, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159911, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 17%|████ | 845/5000 [00:00\u0026lt;00:02, 1683.62it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.25807, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.258125, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 78%|██████████████████ | 3920/5000 [00:00\u0026lt;00:00, 6526.80it/s]\\u001b[A\u0026#34;] [7.25849, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 41%|█████████▎ | 2027/5000 [00:00\u0026lt;00:00, 3368.46it/s]\u0026#34;] [7.259193, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259222, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 59%|█████████████▋ | 2962/5000 [00:00\u0026lt;00:00, 4923.60it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259282, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259315, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 68%|███████████████▋ | 3404/5000 [00:00\u0026lt;00:00, 5657.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259372, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259421, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 96%|█████████████████████▉ | 4775/5000 [00:00\u0026lt;00:00, 7952.80it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259472, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259508, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 49%|███████████▍ | 2474/5000 [00:00\u0026lt;00:00, 4120.16it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259866, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.259917, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 30%|██████▉ | 1516/5000 [00:00\u0026lt;00:01, 2514.82it/s]\\u001b[A\\u001b[A\u0026#34;] [7.259962, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.260017, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 87%|███████████████████▉ | 4337/5000 [00:00\u0026lt;00:00, 7210.60it/s]\u0026#34;] [7.26007, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.260299, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.260351, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 20%|████▋ | 1014/5000 [00:00\u0026lt;00:02, 1683.28it/s]\u0026#34;] [7.260394, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.288047, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 7944.93it/s]\\r\\n\u0026#34;] [7.353676, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 7192.40it/s]\\r\\n\u0026#34;] [7.359093, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 47%|██████████▊ | 2364/5000 [00:00\u0026lt;00:00, 3363.69it/s]\u0026#34;] [7.359563, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.359669, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 91%|█████████████████████ | 4573/5000 [00:00\u0026lt;00:00, 6494.58it/s]\\u001b[A\u0026#34;] [7.359716, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.359768, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 58%|█████████████▎ | 2887/5000 [00:00\u0026lt;00:00, 4120.15it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.359776, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.359835, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 79%|██████████████████▎ | 3970/5000 [00:00\u0026lt;00:00, 5648.96it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.360318, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.360379, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 35%|████████▏ | 1768/5000 [00:00\u0026lt;00:01, 2512.84it/s]\\u001b[A\\u001b[A\u0026#34;] [7.360572, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.360613, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 24%|█████▍ | 1183/5000 [00:00\u0026lt;00:02, 1683.97it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.361952, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.362002, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 69%|███████████████▉ | 3455/5000 [00:00\u0026lt;00:00, 4881.44it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.427585, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 6488.86it/s]\\r\\n\u0026#34;] [7.459805, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.459951, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 66%|███████████████▎ | 3322/5000 [00:00\u0026lt;00:00, 4192.55it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r#5, est. 0.71s: 91%|████████████████████▉ | 4557/5000 [00:00\u0026lt;00:00, 5717.60it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.460767, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r#1, est. 1.70s: 40%|█████████▎ | 2021/5000 [00:00\u0026lt;00:01, 2515.40it/s]\\u001b[A\\u001b[A\u0026#34;] [7.462941, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.463183, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 27%|██████▏ | 1352/5000 [00:00\u0026lt;00:02, 1674.75it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.464172, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r#4, est. 0.83s: 79%|██████████████████▏ | 3944/5000 [00:00\u0026lt;00:00, 4851.69it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.464692, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 54%|████████████▍ | 2701/5000 [00:00\u0026lt;00:00, 3306.24it/s]\u0026#34;] [7.547702, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 5620.61it/s]\\r\\n\u0026#34;] [7.566987, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.567052, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 45%|██████████▍ | 2273/5000 [00:00\u0026lt;00:01, 2468.99it/s]\\u001b[A\\u001b[A\u0026#34;] [7.567509, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.567547, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 30%|██████▉ | 1520/5000 [00:00\u0026lt;00:02, 1651.79it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.568657, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.568709, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 75%|█████████████████▏ | 3742/5000 [00:00\u0026lt;00:00, 4081.31it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.570132, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.570181, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 89%|████████████████████▍ | 4430/5000 [00:00\u0026lt;00:00, 4765.18it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.570768, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 61%|█████████████▉ | 3032/5000 [00:00\u0026lt;00:00, 3245.98it/s]\u0026#34;] [7.668, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.668081, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 34%|███████▊ | 1689/5000 [00:01\u0026lt;00:01, 1661.05it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.668858, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.668916, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 83%|███████████████████▏ | 4170/5000 [00:01\u0026lt;00:00, 4138.65it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.669846, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.669918, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 50%|███████████▌ | 2521/5000 [00:01\u0026lt;00:01, 2450.60it/s]\\u001b[A\\u001b[A\u0026#34;] [7.670477, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.670541, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 98%|██████████████████████▌| 4907/5000 [00:01\u0026lt;00:00, 4761.68it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.671051, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 67%|███████████████▍ | 3358/5000 [00:01\u0026lt;00:00, 3247.82it/s]\u0026#34;] [7.690332, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 4841.91it/s]\\r\\n\u0026#34;] [7.768065, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.76816, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 37%|████████▌ | 1860/5000 [00:01\u0026lt;00:01, 1675.78it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.76985, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.769912, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 55%|████████████▋ | 2767/5000 [00:01\u0026lt;00:00, 2453.22it/s]\u0026#34;] [7.769943, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [7.771104, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 74%|████████████████▉ | 3684/5000 [00:01\u0026lt;00:00, 3250.68it/s]\u0026#34;] [7.773417, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.77352, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 92%|█████████████████████ | 4585/5000 [00:01\u0026lt;00:00, 4085.90it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.868367, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.868446, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 41%|█████████▎ | 2031/5000 [00:01\u0026lt;00:01, 1684.31it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.871357, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 80%|██████████████████▍ | 4012/5000 [00:01\u0026lt;00:00, 3257.22it/s]\u0026#34;] [7.871835, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.871897, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 60%|█████████████▊ | 3013/5000 [00:01\u0026lt;00:00, 2440.74it/s]\\u001b[A\\u001b[A\u0026#34;] [7.8774, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.877474, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 100%|██████████████████████▉| 4995/5000 [00:01\u0026lt;00:00, 4042.20it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.878852, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 4096.34it/s]\\r\\n\u0026#34;] [7.973438, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 87%|███████████████████▉ | 4338/5000 [00:01\u0026lt;00:00, 3239.50it/s]\u0026#34;] [7.974947, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.975032, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 44%|██████████ | 2200/5000 [00:01\u0026lt;00:01, 1653.04it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.975125, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.975179, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 65%|██████████████▉ | 3258/5000 [00:01\u0026lt;00:00, 2419.88it/s]\\u001b[A\\u001b[A\u0026#34;] [8.076423, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 93%|█████████████████████▍ | 4663/5000 [00:01\u0026lt;00:00, 3213.92it/s]\u0026#34;] [8.077467, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.077557, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 70%|████████████████ | 3501/5000 [00:01\u0026lt;00:00, 2406.05it/s]\\u001b[A\\u001b[A\u0026#34;] [8.079458, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.079529, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 47%|██████████▉ | 2366/5000 [00:01\u0026lt;00:01, 1633.21it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.178546, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 100%|██████████████████████▉| 4985/5000 [00:01\u0026lt;00:00, 3195.52it/s]\u0026#34;] [8.179391, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.179504, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 75%|█████████████████▏ | 3742/5000 [00:01\u0026lt;00:00, 2393.54it/s]\\u001b[A\\u001b[A\u0026#34;] [8.182663, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.182739, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 51%|███████████▋ | 2530/5000 [00:01\u0026lt;00:01, 1619.95it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.183485, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 3275.13it/s]\\r\\n\u0026#34;] [8.281006, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.281153, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 80%|██████████████████▎ | 3982/5000 [00:01\u0026lt;00:00, 2384.53it/s]\\u001b[A\\u001b[A\u0026#34;] [8.285647, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.285786, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 54%|████████████▍ | 2693/5000 [00:01\u0026lt;00:01, 1608.86it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.382223, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.382445, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 84%|███████████████████▍ | 4221/5000 [00:01\u0026lt;00:00, 2377.93it/s]\\u001b[A\\u001b[A\u0026#34;] [8.387564, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.387724, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 57%|█████████████▏ | 2854/5000 [00:01\u0026lt;00:01, 1600.17it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.483547, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.483742, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 89%|████████████████████▌ | 4459/5000 [00:01\u0026lt;00:00, 2369.39it/s]\\u001b[A\\u001b[A\u0026#34;] [8.489688, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.489909, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 60%|█████████████▊ | 3015/5000 [00:01\u0026lt;00:01, 1593.19it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.584999, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.585144, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 94%|█████████████████████▌ | 4696/5000 [00:01\u0026lt;00:00, 2359.34it/s]\u0026#34;] [8.585323, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [8.59157, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.591749, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 64%|██████████████▌ | 3175/5000 [00:01\u0026lt;00:01, 1586.38it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.685366, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.685433, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 99%|██████████████████████▋| 4932/5000 [00:02\u0026lt;00:00, 2356.50it/s]\u0026#34;] [8.685557, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [8.692435, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.692538, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 67%|███████████████▎ | 3334/5000 [00:02\u0026lt;00:01, 1583.22it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.714479, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 100%|███████████████████████| 5000/5000 [00:02\u0026lt;00:00, 2430.65it/s]\\r\\n\u0026#34;] [8.792931, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.793065, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 70%|████████████████ | 3493/5000 [00:02\u0026lt;00:00, 1583.51it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.893839, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.894077, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 73%|████████████████▊ | 3652/5000 [00:02\u0026lt;00:00, 1581.30it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.995009, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.995199, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 76%|█████████████████▌ | 3811/5000 [00:02\u0026lt;00:00, 1578.37it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.095512, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.095724, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 79%|██████████████████▎ | 3969/5000 [00:02\u0026lt;00:00, 1576.50it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.196695, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.19733, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 83%|██████████████████▉ | 4127/5000 [00:02\u0026lt;00:00, 1573.48it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.299529, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.300196, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 86%|███████████████████▋ | 4285/5000 [00:02\u0026lt;00:00, 1562.31it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.401289, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.401869, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 89%|████████████████████▍ | 4442/5000 [00:02\u0026lt;00:00, 1556.92it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.502155, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.502554, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 92%|█████████████████████▏ | 4598/5000 [00:02\u0026lt;00:00, 1553.27it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.602158, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.602435, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 95%|█████████████████████▊ | 4754/5000 [00:02\u0026lt;00:00, 1553.92it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.703135, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.704004, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 98%|██████████████████████▌| 4911/5000 [00:03\u0026lt;00:00, 1555.83it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.761857, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 100%|███████████████████████| 5000/5000 [00:03\u0026lt;00:00, 1610.96it/s]\\r\\n\u0026#34;] [9.776273, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [9.776549, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [9.790267, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [9.790433, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [11.262646, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [11.329092, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: recording finished\\u001b[0m\\r\\n\u0026#34;] [11.329167, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: press \u0026lt;enter\u0026gt; to upload to asciinema.org, \u0026lt;ctrl-c\u0026gt; to save locally\\u001b[0m\\r\\n\u0026#34;] [12.316311, \u0026#34;o\u0026#34;, \u0026#34;^C\u0026#34;] [12.316399, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0;32masciinema: asciicast saved to /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[0m\\r\\n\u0026#34;] [12.326841, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [12.326974, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [12.338523, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [12.338637, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [13.935243, \u0026#34;o\u0026#34;, \u0026#34;s\u0026#34;] [14.097389, \u0026#34;o\u0026#34;, \u0026#34;\\bsv\u0026#34;] [14.314987, \u0026#34;o\u0026#34;, \u0026#34;g\u0026#34;] [14.46684, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [15.209977, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [15.350363, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [15.428225, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [15.45927, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term\\u001b[1m \\u001b[0m\\u001b[K\\u001b[67C\\u001b[33m \\u001b[00m\\u001b[68D\u0026#34;] [16.030755, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m -\u0026#34;] [16.194477, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [16.425867, \u0026#34;o\u0026#34;, \u0026#34;i\u0026#34;] [16.508727, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [16.668521, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [16.876205, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [17.019105, \u0026#34;o\u0026#34;, \u0026#34;v\u0026#34;] [17.145906, \u0026#34;o\u0026#34;, \u0026#34;a\u0026#34;] [17.246531, \u0026#34;o\u0026#34;, \u0026#34;r\u0026#34;] [17.421279, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [17.427675, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[57C\\u001b[33m \\u001b[00m\\u001b[58D\u0026#34;] [17.909204, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/f\u0026#34;] [17.994984, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [18.158413, \u0026#34;o\u0026#34;, \u0026#34;l\u0026#34;] [18.209194, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [18.509358, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [18.515598, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[49C\\u001b[33m \\u001b[00m\\u001b[50D\u0026#34;] [19.542164, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/3\u0026#34;] [19.598027, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [19.790912, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [20.75741, \u0026#34;o\u0026#34;, \u0026#34;3\u0026#34;] [21.118463, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [21.375089, \u0026#34;o\u0026#34;, \u0026#34;c\u0026#34;] [21.564873, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [21.829205, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [21.837442, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[15C\\u001b[33m \\u001b[00m\\u001b[16D\u0026#34;] [23.183211, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/T\u0026#34;] [23.603098, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [23.928987, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [24.172937, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [24.489561, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [25.657377, \u0026#34;o\u0026#34;, \u0026#34;0\u0026#34;] [26.161103, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [26.404008, \u0026#34;o\u0026#34;, \u0026#34;0\u0026#34;] [26.603732, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [26.618252, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[1m \\u001b[0m\\u001b[K\u0026#34;] [27.742006, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m -\u0026#34;] [27.890346, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [28.099862, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [28.166638, \u0026#34;o\u0026#34;, \u0026#34;u\u0026#34;] [28.250838, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [28.3748, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [28.551619, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [29.509291, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [29.603613, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [29.712099, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [29.955365, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [30.657315, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [30.843608, \u0026#34;o\u0026#34;, \u0026#34;x\u0026#34;] [31.023734, \u0026#34;o\u0026#34;, \u0026#34;a\u0026#34;] [31.108809, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [31.159277, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [31.341015, \u0026#34;o\u0026#34;, \u0026#34;l\u0026#34;] [31.43148, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [31.533268, \u0026#34;o\u0026#34;, \u0026#34;.\u0026#34;] [31.649616, \u0026#34;o\u0026#34;, \u0026#34;s\u0026#34;] [31.831633, \u0026#34;o\u0026#34;, \u0026#34;v\u0026#34;] [32.007106, \u0026#34;o\u0026#34;, \u0026#34;g\u0026#34;] [32.651262, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [32.923743, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [33.065427, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [33.208844, \u0026#34;o\u0026#34;, \u0026#34;w\u0026#34;] [33.328877, \u0026#34;o\u0026#34;, \u0026#34;i\u0026#34;] [33.379628, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [33.44376, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [33.562365, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [33.692507, \u0026#34;o\u0026#34;, \u0026#34;w\u0026#34;] [34.074411, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\u0026#34;] [34.074507, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [34.096816, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;svg-term --in --out /tmp/example.svg --window\\u0007\\u001b]1;svg-term\\u0007\u0026#34;] [35.927703, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [35.927783, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [35.934493, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [35.934557, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [36.939863, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;]    Transform to SVG Now, for us to place a cast into a Github README or Issue, we need to convert the cast to an animated SVG. svg-term-cli allows us to do this with either a raw cast file or the ID of a cast uploaded to asciinema. Adding the --window flag wraps the cast in a MacOS-style terminal window.\n1  svg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast --out /tmp/example.svg --window   Embedding Now that we have our animated SVG, we just need to put it online and reference it.\nREADME If you\u0026rsquo;re looking to add your SVG to a README, you can include the SVG in the git repo (e.g. in docs/) and reference it in markdown:\n1  ![example cli usage](./docs/example.svg)   Issue Unfortunately, Github does not allow users to drag/drop SVGs into Github Issues.\nInstead, you must figure out your own way to upload the SVG to the internet. One simple solution is to upload it to a gist and then reference the images from your Github Issue.\n1  ![](https://gist.githubusercontent.com/alukach/7c510d45080d5b2c1d42a0309ad25411/raw/9e42a8904b8da9a289d45b44491f39a1586293fb/example.svg)   Example Process Here\u0026rsquo;s an example of the entire process, from start to finish:\nyes, I did use asciinema to make a recording of me using asciinema to make a recording 🤸‍♀️\n","permalink":"https://alukach.com/posts/animated-terminal-output/","summary":"Have you just written a new ✨fancy CLI✨ and want to demo it in your Github Readme? Recording your terminal output is a nice way to demonstrate the experience.\nHere\u0026rsquo;s an example of what we\u0026rsquo;re going to make:\nSteps Install Dependencies  asciinema: brew install asciinema svg-term-cli: npm install -g svg-term-cli  Setup your terminal Some tips:\n Font/screen size matters. The asciinema output will look just as it does in your terminal.","title":"Putting animated SVGs of Terminal Output into Github READMEs"},{"content":"At times, a developer may need to access infrastructure not available on the public internet. A common example of this is accessing a database located in a private subnet, as described in the VPC Scenario docs:\n Instances in the private subnet are back-end servers that don\u0026rsquo;t need to accept incoming traffic from the internet and therefore do not have public IP addresses; however, they can send requests to the internet using the NAT gateway.\n The common strategy for connecting to one of these devices is to tunnel your traffic through a jump box AKA jump server AKA jump host. This can be achieved by SSH Port Forwarding AKA SSH Tunneling.\nFor a recent project, I needed a convenient way to query private databases in Python to do some repeatable data management operations. Tools like DBeaver have built-in support for connecting to databases over SSH tunnels, however I needed something more scriptable. Standing up a service in AWS would have worked however seemed to be overkill for my simple scripting needs. My goals were to 1) get auth credentials from AWS Secrets Manager (RDS places credentials in Secrets Manager by default, or at least when creating RDS instances via CDK); 2) setup a tunnel through a jumpbox to allow access to the RDS Instance; 3) run SQL queries against the DB. Automating this process in Python was not immediately clear until found the sshtunnel module. After playing around with the code for a bit, I was able to put together a utility class with Pydantic and Psycopg2 to conveniently connect to a private RDS instance via SSH tunneling. I figured I would share in the event that someone ever needs such a tool in the future.\nCode Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98  import socket import contextlib import logging from typing import Any, Generator, Tuple, Optional import psycopg2 import psycopg2.extras from pydantic.main import BaseModel from sshtunnel import open_tunnel logger = logging.getLogger(__name__) class Db(BaseModel): dbname: str user: str password: str host: str port: int = 5432 @contextlib.contextmanager def cursor( self, name=None ) -\u0026gt; Generator[Tuple[Any, psycopg2.extras.DictCursor], None, None]: logger.debug(\u0026#34;Connecting to %s\u0026#34;, self.dbname) with psycopg2.connect(**self.dict()) as conn: cursor = conn.cursor(name, cursor_factory=psycopg2.extras.DictCursor) with cursor as curs: logger.debug(\u0026#34;Yielding cursor\u0026#34;) yield conn, curs logger.debug(\u0026#34;Disconnecting from %s\u0026#34;, self.dbname) @contextlib.contextmanager def create_tunnel( self, jumpbox_host: str, local_port: Optional[int] = None, jumpbox_port: int = 22, local_host: str = \u0026#34;127.0.0.1\u0026#34;, jumpbox_username: str = None, ssh_key_password: str = None, ) -\u0026gt; Generator[\u0026#34;Db\u0026#34;, None, None]: \u0026#34;\u0026#34;\u0026#34; Generates an SSH tunnel to DB via jumpbox. \u0026#34;\u0026#34;\u0026#34; if local_port is None: local_port = self._find_free_port() with open_tunnel( (jumpbox_host, jumpbox_port), ssh_username=jumpbox_username, remote_bind_address=(self.host, self.port), local_bind_address=(local_host, local_port), ssh_private_key_password=ssh_key_password, ) as tunnel: logger.debug( \u0026#34;Tunnel to %sthrough %sestablished on port %s\u0026#34;, self.host, jumpbox_host, local_port, ) yield self.copy( update={ \u0026#34;host\u0026#34;: tunnel.local_bind_host, \u0026#34;port\u0026#34;: tunnel.local_bind_port, } ) @classmethod def from_rds_credentials(cls, secret): return cls.parse_obj({\u0026#34;user\u0026#34;: secret.pop(\u0026#34;username\u0026#34;), **secret}) @staticmethod def _find_free_port() -\u0026gt; int: # https://stackoverflow.com/a/45690594/728583 with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s: s.bind((\u0026#34;\u0026#34;, 0)) s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) return s.getsockname()[1] if __name__ == \u0026#34;__main__\u0026#34;: import json import boto3 rds_secret = \u0026#34;myRdsDbSecret\u0026#34; # ARN or Secret ID jumpbox_host = \u0026#34;my-jumpbox-hostname\u0026#34; # hostname/ip address of jumpbox credentials = json.load( boto3.client(\u0026#34;secretsmanager\u0026#34;).get_secret_value(SecretId=rds_secret)[ \u0026#34;SecretString\u0026#34; ] ) private_db = Db.from_rds_credentials(credentials) with private_db.create_tunnel(jumpbox_host) as db: with db.cursor() as (conn, cur): cur.execute(\u0026#34;SELECT COUNT(*) FROM my_table;\u0026#34;) print(cur.fetchone())   ","permalink":"https://alukach.com/posts/ssh-tunnels-in-python/","summary":"At times, a developer may need to access infrastructure not available on the public internet. A common example of this is accessing a database located in a private subnet, as described in the VPC Scenario docs:\n Instances in the private subnet are back-end servers that don\u0026rsquo;t need to accept incoming traffic from the internet and therefore do not have public IP addresses; however, they can send requests to the internet using the NAT gateway.","title":"SSH tunnels in Python"},{"content":"Getting area of geometries in WGS-84/EPSG:4326 in square kilometers:\n1 2 3 4  SELECTST_Area(geometry,false)/10^6sq_kmFROMmy_table  ","permalink":"https://alukach.com/posts/wgs-84-epsg-4326-geometry-area-in-sq-km/","summary":"Getting area of geometries in WGS-84/EPSG:4326 in square kilometers:\n1 2 3 4  SELECTST_Area(geometry,false)/10^6sq_kmFROMmy_table  ","title":"Getting area of WGS-84 geometries in SqKm"},{"content":"Below is a very simple example of a script that I write and re-write more often than I would like to admit. It reads input data from a CSV and processes each row concurrently. A progress bar provides updates. Honestly, it\u0026rsquo;s pretty much just the concurrent.futures ThreadPoolExecutor example plus a progress bar.\n ","permalink":"https://alukach.com/posts/concurrent-python-example-script/","summary":"Below is a very simple example of a script that I write and re-write more often than I would like to admit. It reads input data from a CSV and processes each row concurrently. A progress bar provides updates. Honestly, it\u0026rsquo;s pretty much just the concurrent.futures ThreadPoolExecutor example plus a progress bar.\n ","title":"Concurrent Python Example Script"},{"content":"Below is a simple script to deploy a Docker image to ECR\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo \u0026#34;${bold}${1}${normal}\u0026#34; 1\u0026gt;\u0026amp;2; } if [ -z \u0026#34;${AWS_ACCOUNT}\u0026#34; ]; then log \u0026#34;Missing a valid AWS_ACCOUNT env variable\u0026#34;; exit 1; else log \u0026#34;Using AWS_ACCOUNT \u0026#39;${AWS_ACCOUNT}\u0026#39;\u0026#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log \u0026#34;🔑 Authenticating...\u0026#34; aws ecr get-login-password \\  --region ${AWS_REGION} \\  | docker login \\  --username AWS \\  --password-stdin \\  ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com log \u0026#34;📦 Building image...\u0026#34; docker build -t ${REPO_NAME} . log \u0026#34;🏷️ Tagging image...\u0026#34; docker tag \\  ${REPO_NAME}:latest \\  ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/${REPO_NAME}:latest log \u0026#34;🚀 Pushing to ECR repo...\u0026#34; docker push \\  ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/${REPO_NAME}:latest log \u0026#34;💃 Deployment Successful. 🕺\u0026#34;   ","permalink":"https://alukach.com/posts/ecr-deployment-script/","summary":"Below is a simple script to deploy a Docker image to ECR\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo \u0026#34;${bold}${1}${normal}\u0026#34; 1\u0026gt;\u0026amp;2; } if [ -z \u0026#34;${AWS_ACCOUNT}\u0026#34; ]; then log \u0026#34;Missing a valid AWS_ACCOUNT env variable\u0026#34;; exit 1; else log \u0026#34;Using AWS_ACCOUNT \u0026#39;${AWS_ACCOUNT}\u0026#39;\u0026#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log \u0026#34;🔑 Authenticating.","title":"An ECR Deployment Script"},{"content":"Alternate title: How to be master of your domain.\nThe basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application\u0026rsquo;s content and services from a single domain. This minimizes a project\u0026rsquo;s TLD footprint while providing project organization and performance along the way.\nWhy Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project. This means that utilizing multiple service-specific subdomains (e.g. api.my-project.big-institution.gov or thumbnails.my-project.big-institution.gov) is an arduous process. To avoid this in a recent project, we settled on adopting a pattern where we use CloudFront to proxy all of our domain\u0026rsquo;s incoming requests to their appropriate service.\nHow it works CloudFront has the ability to support multiple origin configurations (i.e. multiple sources of content). We can utilize the Path Pattern setting to direct web requests by URL path to their appropriate service. CloudFront behaves like a typical router libraries, wherein it routes traffic to the first path with a pattern matching the incoming request and routes requests that don\u0026rsquo;t match route patterns to a default route. For example, our current infrastructure looks like this:\nmy-project.big-institution.gov/ ├── api/* \u0026lt;- Application Load Balancer (ALB) that distributes traffic to order │ management API service running on Elastic Container Service (ECS). │ ├── stac/* \u0026lt;- ALB that distributes traffic to STAC API service running on ECS. │ ├── storage/* \u0026lt;- Private S3 bucket storing private data. Only URLs that have been │ signed with our CloudFront keypair will be successful. │ ├── thumbnails/* \u0026lt;- Public S3 bucket storing thumbnail imagery. │ └── * \u0026lt;- Public S3 website bucket storing our single page application frontend. Single Page Applications An S3 bucket configured for website hosting acts as the origin for our default route. If an incoming request\u0026rsquo;s path does not match routes specified elsewhere within the CloudFront distribution, it is routed to the single page application. To configure the single page application to handle any requests provided (i.e. not just requests sent to paths of existing files within the bucket, such as index.html or app.js), the bucket should be configured with a custom error page in response to 404 errors, returning the applications HTML entrypoint (index.html).\nRequirements To enable the usage of a custom error page, the S3 bucket\u0026rsquo;s website endpoint (i.e. \u0026lt;bucket-name\u0026gt;.s3-website-\u0026lt;region\u0026gt;.amazonaws.com, not \u0026lt;bucket-name\u0026gt;.s3.\u0026lt;region\u0026gt;.amazonaws.com) must be configured as a custom origin for the distribution. Additionally, the bucket must be configured for public access. More information: Using Amazon S3 Buckets Configured as Website Endpoints for Your Origin. Being that the S3 website endpoint does not support SSL, the custom origin\u0026rsquo;s Protocol Policy should be set to HTTP Only.\n My bucket is private. Can CloudFront serve a website from this bucket?\n If your bucket is private, the website endpoint will not work (source). You could configure CloudFront to send traffic to the buckets REST API endpoint, however this will prevent you from being able to utilize S3\u0026rsquo;s custom error document feature which may be essential for hosting single page applications on S3. Tools like Next.js and Gatsby.js support rendering HTML documents for all routes, which can avoid the need for custom error pages; however care must be given to ensure that any dynamic portion of the page\u0026rsquo;s routes (e.g. /docs/3, where 3 is the ID of a record to be fetched from an API) must be specified as either a query parameter (e.g. /docs?3) or a hash (e.g. /docs#3).\n CloudFront itself has support for custom error pages. Why can\u0026rsquo;t I use that to enable hosting private S3 buckets as websites?\n While it is true that CloudFront can route error responses to custom pages (e.g. sending all 404 responses the contents of s3://my-website-bucket/index.html), these custom error pages apply to the entirety of your CloudFront distribution. This is likely undesirable for any API services hosted by your CloudFront distribution. For example, if a user accesses a RESTful API at http://my-website.com/api/notes/12345 and the API server responds with a 404 of {\u0026quot;details\u0026quot;: \u0026quot;Record not found\u0026quot;}, the response body will be re-written to contain the contents of s3://my-website-bucket/index.html. At time of writing, I am unaware of any capability of applying custom error pages to only certain content-types. A feature such as this might make distribution-wide custom error pages a viable solution.\nAPIs APIs are served as custom origins, with their Domain Name settings pointing to their an ALB\u0026rsquo;s DNS name.\n Does this work with APIs run with Lambda or EC2?\n Assuming that the service has a DNS name, it can be set up as an origin for CloudFront. This means that for an endpoint handled by a Lambda function, you would need to have it served behind an API Gateway or an ALB.\nRecommended configuration  Disable caching by setting the default, minimum, and maximum TTL to 0 seconds. Set AllowedMethods to forward all requests (i.e. GET, HEAD, OPTIONS, PUT, PATCH, POST, and DELETE). Set ForwardedValues so that querystring and the following headers are fowarded: referer, authorization, origin, accept, host Origin Protocol Policy of HTTP Only.  Data from S3 Buckets Data from a standard S3 bucket can be configured by pointing to the bucket\u0026rsquo;s REST endpoint (e.g. \u0026lt;bucket-name\u0026gt;.s3.\u0026lt;region\u0026gt;.amazonaws.com). More information: Using Amazon S3 Buckets for Your Origin.\nThis can be a public bucket, in which case would benefit from the CDN and caching provided by CloudFront.\nWhen using a private bucket, CloudFront additionally can serve as a \u0026ldquo;trusted signer\u0026rdquo; to enable an application with access to the CloudFront security keys to create signed URLs/cookies to grant temporary access to particular private content. In order for CloudFront to access content within a private bucket, its Origin Access Identity must be given read privileges within the bucket\u0026rsquo;s policy. More information: Restricting Access to Amazon S3 Content by Using an Origin Access Identity\nCaveats The most substantial issue with this technique is the fact that CloudFront does not have the capability to remove portions of a path from a request\u0026rsquo;s URL. For example, if an API is configured as an origin at https://d1234abcde.cloudfront.net/api, it should be configured to respond to URLs starting with /api. This is often a non-issue, as many server frameworks have builtin support to support being hosted at a non-root path.\n Configuring FastAPI to be served under a non-root path 1 2 3 4 5 6 7 8 9 10 11 12  from fastapi import FastAPI, APIRouter API_BASE_PATH = \u0026#39;/api\u0026#39; app = FastAPI( title=\u0026#34;Example API\u0026#34;, docs_url=API_BASE_PATH, swagger_ui_oauth2_redirect_url=f\u0026#34;{API_BASE_PATH}/oauth2-redirect\u0026#34;, openapi_url=f\u0026#34;{API_BASE_PATH}/openapi.json\u0026#34;, ) api_router = APIRouter() app.include_router(router, prefix=API_BASE_PATH)    Furthermore, if you have an S3 bucket serving content from https://d1234abcde.cloudfront.net/bucket, only keys with a prefix of bucket/ will be available to that origin. In the event that keys are not prefixed with a path matching the origins configured path pattern, there are two options:\n Move all of the files, likely utilizing something like S3 Batch (see #253 for more details) Use a Lambda@Edge function to rewrite the path of any incoming request for a non-cached resource to conform to the key structure of the S3 bucket\u0026rsquo;s objects.  Summary After learning this technique, it feels kind of obvious. I\u0026rsquo;m honestly not sure if this is AWS 101 level technique or something that is rarely done; however I never knew of it before this project and therefore felt it was worth sharing.\nA quick summary of some of the advantages that come with using CloudFront for all application endpoints:\n It feels generally tidier to have all your endpoints placed behind a single domain. No more dealing with ugly ALB, API Gateway, or S3 URLs. This additionally pays off when you are dealing with multiple stages (e.g. prod and dev) of the same service 🧹. SSL is managed and terminated at CloudFront. Everything after that is port 80 non-SSL traffic, simplifying the management of certificates 🔒. All non-SSL traffic can be set to auto-redirect to SSL endpoints ↩️. Out of the box, AWS Shield Standard is applied to CloudFront to provide protection against DDoS attacks 🏰. Static content is regionally cached and served from Edge Locations closer to the viewer 🌏. Dynamic content is also served from Edge Locations, which connect to the origin server via AWS' global private network. This is faster than connecting to an origin server over the public internet 🚀. Externally, all data is served from the same domain origin. Goodbye CORS errors 👋! Data egress costs are lower through CloudFront than other services. This can be ensured by only selecting Price Class 100, other price classes can be chosen if enabling a global CDN is worth the higher egress costs 💴.  Example  An example of a reverse-proxy CloudFront Distribution written with CDK in Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125  from aws_cdk import ( aws_s3 as s3, aws_certificatemanager as certmgr, aws_iam as iam, aws_cloudfront as cf, aws_elasticloadbalancingv2 as elbv2, core, ) class CloudfrontDistribution(core.Construct): def __init__( self, scope: core.Construct, id: str, api_lb: elbv2.ApplicationLoadBalancer, assets_bucket: s3.Bucket, website_bucket: s3.Bucket, domain_name: str = None, using_gcc_acct: bool = False, **kwargs, ) -\u0026gt; None: super().__init__(scope, id, **kwargs) oai = cf.OriginAccessIdentity( self, \u0026#34;Identity\u0026#34;, comment=\u0026#34;Allow CloudFront to access S3 Bucket\u0026#34;, ) if not using_gcc_acct: self.grant_oai_read(oai, assets_bucket) certificate = ( certmgr.Certificate(self, \u0026#34;Certificate\u0026#34;, domain_name=domain_name) if domain_name else None ) self.distribution = cf.CloudFrontWebDistribution( self, core.Stack.of(self).stack_name, alias_configuration=( cf.AliasConfiguration( acm_cert_ref=certificate.certificate_arn, names=[domain_name] ) if certificate else None ), comment=core.Stack.of(self).stack_name, origin_configs=[ # Frontend Website cf.SourceConfiguration( # NOTE: Can\u0026#39;t use S3OriginConfig because we want to treat our # bucket as an S3 Website Endpoint rather than an S3 REST API # Endpoint. This allows us to use a custom error document to # direct all requests to a single HTML document (as required # to host an SPA). custom_origin_source=cf.CustomOriginConfig( domain_name=website_bucket.bucket_website_domain_name, origin_protocol_policy=cf.OriginProtocolPolicy.HTTP_ONLY, # In website-mode, S3 only serves HTTP # noqa: E501 ), behaviors=[cf.Behavior(is_default_behavior=True)], ), # API load balancer cf.SourceConfiguration( custom_origin_source=cf.CustomOriginConfig( domain_name=api_lb.load_balancer_dns_name, origin_protocol_policy=cf.OriginProtocolPolicy.HTTP_ONLY, ), behaviors=[ cf.Behavior( path_pattern=\u0026#34;/api*\u0026#34;, # No trailing slash to permit access to root path of API # noqa: E501 allowed_methods=cf.CloudFrontAllowedMethods.ALL, forwarded_values={ \u0026#34;query_string\u0026#34;: True, \u0026#34;headers\u0026#34;: [ \u0026#34;referer\u0026#34;, \u0026#34;authorization\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;accept\u0026#34;, \u0026#34;host\u0026#34;, # Required to prevent API\u0026#39;s redirects on trailing slashes directing users to ALB endpoint # noqa: E501 ], }, # Disable caching default_ttl=core.Duration.seconds(0), min_ttl=core.Duration.seconds(0), max_ttl=core.Duration.seconds(0), ) ], ), # Assets cf.SourceConfiguration( s3_origin_source=cf.S3OriginConfig( s3_bucket_source=assets_bucket, origin_access_identity=oai, ), behaviors=[ cf.Behavior( path_pattern=\u0026#34;/storage/*\u0026#34;, trusted_signers=[\u0026#34;self\u0026#34;], ) ], ), ], ) self.assets_path = f\u0026#34;https://{self.distribution.domain_name}/storage\u0026#34; core.CfnOutput(self, \u0026#34;Endpoint\u0026#34;, value=self.distribution.domain_name) def grant_oai_read(self, oai: cf.OriginAccessIdentity, bucket: s3.Bucket): \u0026#34;\u0026#34;\u0026#34; To grant read access to our OAI, at time of writing we can not simply use `bucket.grant_read(oai)`. This is due to the fact that we are looking up our bucket by its name. For more information, see the following: https://stackoverflow.com/a/60917015/728583. As a work-around, we can manually assigned a policy statement, however this does not work in situations where a policy is already applied to the bucket (e.g. in GCC environments). \u0026#34;\u0026#34;\u0026#34; policy_statement = iam.PolicyStatement( actions=[\u0026#34;s3:GetObject*\u0026#34;, \u0026#34;s3:List*\u0026#34;], resources=[bucket.bucket_arn, f\u0026#34;{bucket.bucket_arn}/storage*\u0026#34;], principals=[], ) policy_statement.add_canonical_user_principal( oai.cloud_front_origin_access_identity_s3_canonical_user_id ) assets_policy = s3.BucketPolicy(self, \u0026#34;AssetsPolicy\u0026#34;, bucket=bucket) assets_policy.document.add_statements(policy_statement)    Additional reading  Amazon S3 + Amazon CloudFront: A Match Made in the Cloud Dynamic Whole Site Delivery with Amazon CloudFront  ","permalink":"https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/","summary":"Alternate title: How to be master of your domain.\nThe basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application\u0026rsquo;s content and services from a single domain. This minimizes a project\u0026rsquo;s TLD footprint while providing project organization and performance along the way.\nWhy Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project.","title":"Using CloudFront as a Reverse Proxy"},{"content":"A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what\u0026rsquo;s provided via a RDS DatabaseInstance\u0026rsquo;s secret property).\n1 2 3 4 5 6 7 8 9 10 11  db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f\u0026#34;postgres://{db_val(\u0026#39;username\u0026#39;)}:{db_val(\u0026#39;password\u0026#39;)}@{db_val(\u0026#39;host\u0026#39;)}:{db_val(\u0026#39;port\u0026#39;)}/\u0026#34;, ), # ... )   ","permalink":"https://alukach.com/posts/database-uri-from-secret/","summary":"A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what\u0026rsquo;s provided via a RDS DatabaseInstance\u0026rsquo;s secret property).\n1 2 3 4 5 6 7 8 9 10 11  db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f\u0026#34;postgres://{db_val(\u0026#39;username\u0026#39;)}:{db_val(\u0026#39;password\u0026#39;)}@{db_val(\u0026#39;host\u0026#39;)}:{db_val(\u0026#39;port\u0026#39;)}/\u0026#34;, ), # ... )   ","title":"How to generate a database URI from an AWS Secret"},{"content":"I would argue that S3 is basically AWS' best service. It\u0026rsquo;s super cheap, it\u0026rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.\nHowever, when you\u0026rsquo;ve enlisted S3 to manage a large number of files (1M+), it can get complicated to do anything beyond doing simple writes and retrievals. Fortunately, there are a number of helpers available to make it manageable to working with this scale of data. This post aims to capture some common workflows that may be of use when working with huge S3 buckets.\nListing Files The mere act of listing all of the data within a huge S3 bucket is a challenge. S3\u0026rsquo;s list-objects API returns a max of 1000 items per request, meaning you\u0026rsquo;ll have to work through thousands of pages of API responses to fully list all items within the bucket. To make this simpler, we can utilize S3\u0026rsquo;s Inventory.\n Amazon S3 inventory provides comma-separated values (CSV), Apache optimized row columnar (ORC) or Apache Parquet (Parquet) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix.\n Be aware that it can take up to 48 hours to generate an Inventory Report. From that point forward, reports can be generated on a regular interval.\nAn inventory report serves as a great first-step when attempting to do any processing on an entire bucket of files. Often, you don\u0026rsquo;t need to retrieve the inventory report manually from S3. Instead, it can be fed into Athena or S3 Batch Operations as described below.\nHowever, when you do need to access the data locally, downloading and reading all of the gzipped CSV files that make up an inventory report can be somewhat tedious. The following script was written to help with this process. Its output can be piped to a local CSV file to create a single output or sent to another function for processing.\n Stream S3 Inventory Report Python script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import json import csv import gzip import boto3 s3 = boto3.resource(\u0026#39;s3\u0026#39;) def list_keys(bucket, manifest_key): manifest = json.load(s3.Object(bucket, manifest_key).get()[\u0026#39;Body\u0026#39;]) for obj in manifest[\u0026#39;files\u0026#39;]: gzip_obj = s3.Object(bucket_name=bucket, key=obj[\u0026#39;key\u0026#39;]) buffer = gzip.open(gzip_obj.get()[\u0026#34;Body\u0026#34;], mode=\u0026#39;rt\u0026#39;) reader = csv.reader(buffer) for row in reader: yield row if __name__ == \u0026#39;__main__\u0026#39;: bucket = \u0026#39;s3-inventory-output-bucket\u0026#39; manifest_key = \u0026#39;path/to/my/inventory/2019-12-15T00-00Z/manifest.json\u0026#39; for bucket, key, *rest in list_keys(bucket, manifest_key): print(bucket, key, *rest)    Querying files by S3 Properties Sometimes you may need a subset of the files within S3, based some metadata property of the object (e.g. storage class, the key\u0026rsquo;s extension). While you can use the S3 list-objects API to list files beginning with a particular prefix, you can not filter by suffix. To get around this limitation, we can utilize AWS Athena to query over an S3 Inventory report.\n 1. Create a table This example assumes that you chose CSV as the S3 Inventory Output Format. For information on other formats, review the docs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  CREATEEXTERNALTABLEyour_table_name(`bucket`string,keystring,version_idstring,is_latestboolean,is_delete_markerboolean,sizebigint,last_modified_datetimestamp,e_tagstring,storage_classstring,is_multipart_uploadedboolean,replication_statusstring,encryption_statusstring,object_lock_retain_until_datetimestamp,object_lock_modestring,object_lock_legal_hold_statusstring)PARTITIONEDBY(dtstring)ROWFORMATDELIMITEDFIELDSTERMINATEDBY\u0026#39;,\u0026#39;ESCAPEDBY\u0026#39;\\\\\u0026#39;LINESTERMINATEDBY\u0026#39;\\n\u0026#39;STOREDASINPUTFORMAT\u0026#39;org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\u0026#39;OUTPUTFORMAT\u0026#39;org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\u0026#39;LOCATION\u0026#39;s3://destination-prefix/source-bucket/YOUR_CONFIG_ID/hive/\u0026#39;;    2. Add inventory reports partitions 1  MSCKREPAIRTABLEyour_table_name;    3. Query for S3 keys by their filename, size, storage class, etc 1 2 3 4  SELECTstorage_class,count(*)ascountFROMyour_table_nameWHEREdt=\u0026#39;2019-12-22-00-00\u0026#39;GROUPBYstorage_class   More information about querying Storage Inventory files with Athena can be found here.\nProcessing Files Situations may arise where you need to run all (or a large number) of the files within an S3 bucket through some operation. S3 Batch Operations (not to be confused with AWS Batch) is built to do the following:\n copy objects, set object tags or access control lists (ACLs), initiate object restores from Amazon S3 Glacier, or invoke an AWS Lambda function to perform custom actions using your objects.\n With that last feature, invoking an AWS Lambda function, we can utilize Batch Operations to process a massive number of files without dealing without any of the complexity associated with data-processing infrastructure. Instead, we provide the Batch Operations with a CSV or S3 Inventory Manifest file and a Lambda function to run over each file.\nTo work with S3 Batch Operations, the lambda function must return a particular response object to describe if the process succeeded, failed, or failed but should be retried.\n S3 Batch Operation Boilerplate Python script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  import urllib import boto3 from botocore.exceptions import ClientError s3 = boto3.resource(\u0026#34;s3\u0026#34;) TMP_FAILURE = \u0026#34;TemporaryFailure\u0026#34; FAILURE = \u0026#34;PermanentFailure\u0026#34; SUCCESS = \u0026#34;Succeeded\u0026#34; def process_object(src_object): return \u0026#34;TODO: Populate with processing task...\u0026#34; def get_task_id(event): return event[\u0026#34;tasks\u0026#34;][0][\u0026#34;taskId\u0026#34;] def parse_job_parameters(event): # Parse job parameters from Amazon S3 batch operations # jobId = event[\u0026#34;job\u0026#34;][\u0026#34;id\u0026#34;] invocationId = event[\u0026#34;invocationId\u0026#34;] invocationSchemaVersion = event[\u0026#34;invocationSchemaVersion\u0026#34;] return dict( invocationId=invocationId, invocationSchemaVersion=invocationSchemaVersion ) def get_s3_object(event): # Parse Amazon S3 Key, Key Version, and Bucket ARN s3Key = urllib.parse.unquote(event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3Key\u0026#34;]) s3VersionId = event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3VersionId\u0026#34;] # Unused s3BucketArn = event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3BucketArn\u0026#34;] s3Bucket = s3BucketArn.split(\u0026#34;:::\u0026#34;)[-1] return s3.Object(s3Bucket, s3Key) def build_result(status: str, msg: str): return dict(resultCode=status, resultString=msg) def handler(event, context): task_id = get_task_id(event) job_params = parse_job_parameters(event) s3_object = get_s3_object(event) try: output = process_object(s3_object) # Mark as succeeded result = build_result(SUCCESS, output) except ClientError as e: # If request timed out, mark as a temp failure # and Amazon S3 batch operations will make the task for retry. If # any other exceptions are received, mark as permanent failure. errorCode = e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] errorMessage = e.response[\u0026#34;Error\u0026#34;][\u0026#34;Message\u0026#34;] if errorCode == \u0026#34;RequestTimeout\u0026#34;: result = build_result( TMP_FAILURE, \u0026#34;Retry request to Amazon S3 due to timeout.\u0026#34; ) else: result = build_result(FAILURE, f\u0026#34;{errorCode}: {errorMessage}\u0026#34;) except Exception as e: # Catch all exceptions to permanently fail the task result = build_result(FAILURE, f\u0026#34;Exception: {e}\u0026#34;) return { **job_params, \u0026#34;treatMissingKeysAs\u0026#34;: \u0026#34;PermanentFailure\u0026#34;, \u0026#34;results\u0026#34;: [{**result, \u0026#34;taskId\u0026#34;: task_id}], }    S3 Batch Operations will then run every key through this Lambda handler, retry temporary failures, and log its results in result files. The result files are conveniently grouped by success/failure status and linked to from a Manifest Result File.\n Example Manifest Result File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  { \u0026#34;Format\u0026#34;: \u0026#34;Report_CSV_20180820\u0026#34;, \u0026#34;ReportCreationDate\u0026#34;: \u0026#34;2019-04-05T17:48:39.725Z\u0026#34;, \u0026#34;Results\u0026#34;: [ { \u0026#34;TaskExecutionStatus\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;Bucket\u0026#34;: \u0026#34;my-job-reports\u0026#34;, \u0026#34;MD5Checksum\u0026#34;: \u0026#34;83b1c4cbe93fc893f54053697e10fd6e\u0026#34;, \u0026#34;Key\u0026#34;: \u0026#34;job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/6217b0fab0de85c408b4be96aeaca9b195a7daa5.csv\u0026#34; }, { \u0026#34;TaskExecutionStatus\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;Bucket\u0026#34;: \u0026#34;my-job-reports\u0026#34;, \u0026#34;MD5Checksum\u0026#34;: \u0026#34;22ee037f3515975f7719699e5c416eaa\u0026#34;, \u0026#34;Key\u0026#34;: \u0026#34;job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/b2ddad417e94331e9f37b44f1faf8c7ed5873f2e.csv\u0026#34; } ], \u0026#34;ReportSchema\u0026#34;: \u0026#34;Bucket, Key, VersionId, TaskStatus, ErrorCode, HTTPStatusCode, ResultMessage\u0026#34; }    More information about the Complete Report format can be found here.\n At time of writing, S3 Batch Operations cost $0.25 / job + $1 / million S3 objects processed.\nPrice to process 5 million thumbnails in 2hrs:\n S3 Batch Operations: $0.25 + (5 * $1) = $5.25 Lambda: 128MB _ 2000 ms _ 5,000,000 = $21.83 S3 Get Requests: 5,000,000 / 1000 * $0.0004 = $2 S3 Put Requests: 5,000,000 / 1000 * $0.005 = $25 TOTAL: $54.08  Things not discussed in this post If you are looking for more techniques on querying data stored in S3, consider the following:\n Using Athena to query the contents of your files stored in S3 Using S3 Select to select a subset of a single (very large) file stored in S3  ","permalink":"https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/","summary":"I would argue that S3 is basically AWS' best service. It\u0026rsquo;s super cheap, it\u0026rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.","title":"Tips for working with a large number of files in S3"},{"content":"S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.\n ","permalink":"https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/","summary":"S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.\n ","title":"Boilerplate for S3 Batch Operation Lambda"},{"content":"S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)\nHowever, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:\n ","permalink":"https://alukach.com/posts/parsing-s3-inventory-output/","summary":"S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)\nHowever, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:","title":"Parsing S3 Inventory CSV output in Python"},{"content":"Here\u0026rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.\n ","permalink":"https://alukach.com/posts/pil-friendly-s3-file/","summary":"Here\u0026rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.\n ","title":"A PIL-friendly class for S3 objects"},{"content":"Let\u0026rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource\u0026rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12  AWSTemplateFormatVersion:\u0026#39;2010-09-09\u0026#39;Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:|#!/bin/sh echo \u0026#34;Hello world\u0026#34;  In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template. This is no problem with Cloudformation\u0026rsquo;s Fn::Sub instrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  AWSTemplateFormatVersion:\u0026#39;2010-09-09\u0026#39;Parameters:Username:Description:UsernameType:StringMinLength:\u0026#39;1\u0026#39;MaxLength:\u0026#39;255\u0026#39;AllowedPattern:\u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39;ConstraintDescription:must begin with a letter and contain only alphanumericcharacters.Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:!Sub |#!/bin/shecho \u0026#34;Hello ${Username}\u0026#34;  The downside of the Fn::Sub function is that it does not play nice with bash' parameter substitution expressions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  AWSTemplateFormatVersion:\u0026#39;2010-09-09\u0026#39;Parameters:Username:Description:UsernameType:StringMinLength:\u0026#39;1\u0026#39;MaxLength:\u0026#39;255\u0026#39;AllowedPattern:\u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39;ConstraintDescription:must begin with a letter and contain only alphanumericcharacters.Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:!Sub |#!/bin/shecho \u0026#34;Hello ${Username}\u0026#34;FOO=${FOO:-\u0026#39;bar\u0026#39;}  The above template fails validation:\n1 2 3  $ aws cloudformation validate-template --template-body file://test.yaml An error occurred (ValidationError) when calling the ValidateTemplate operation: Template error: variable names in Fn::Sub syntax must contain only alphanumeric characters, underscores, periods, and colons   The work-around is to rely on another intrinsic function: Fn::Join:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  AWSTemplateFormatVersion:\u0026#39;2010-09-09\u0026#39;Parameters:Username:Description:UsernameType:StringMinLength:\u0026#39;1\u0026#39;MaxLength:\u0026#39;255\u0026#39;AllowedPattern:\u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39;ConstraintDescription:must begin with a letter and contain only alphanumericcharacters.Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:!Join- \u0026#39;\\n\u0026#39;- - !Sub |#!/bin/shecho \u0026#34;Hello ${Username}\u0026#34;- |FOO=${FOO:-\u0026#39;bar\u0026#39;}  This allows you to mix CloudFormation substitutions with Bash parameter substititions.\n Bonus While we\u0026rsquo;re talking about CloudFormation, another good trick comes from cloudonaut.io regarding using a Optional Parameter in CloudFormation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  Parameters:KeyName:Description:(Optional) Select an ssh key pair if you will need SSH access to the machineType:StringConditions:HasKeyName:Fn::Not:- Fn::Equals:- \u0026#39;\u0026#39;- Ref:KeyNameResources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumKeyName:Fn::If:- HasKeyName- !Ref KeyName- !Ref AWS::NoValue  Note that the KeyName has Type: String. While Type: AWS::EC2::KeyPair::KeyName would likely be a better user experience as it would render a dropdown of all keys, it does not allow for empty values:\n \u0026hellip; if you use the AWS::EC2::KeyPair::KeyName parameter type, AWS CloudFormation validates the input value against users' existing key pair names before it creates any resources, such as Amazon EC2 instances.\n ","permalink":"https://alukach.com/posts/cloudformation-sub-ref/","summary":"Let\u0026rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource\u0026rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12  AWSTemplateFormatVersion:\u0026#39;2010-09-09\u0026#39;Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:|#!/bin/sh echo \u0026#34;Hello world\u0026#34;  In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template. This is no problem with Cloudformation\u0026rsquo;s Fn::Sub instrinsic function:","title":"Using CloudFormation's Fn::Sub with Bash parameter substitution"},{"content":"When an Esri Web AppBuilder web app is configured with a portalUrl value served from HTTPS, the web app automatically redirects users to HTTPS when visited via HTTP. While this is best-practice in production, it can be a burden in development when you want to quickly run a local version of the web app. Below is a quick script written with Python standard libraries to serve a web app over HTTP. It works by serving a config.json that is modified to use HTTP rather than HTTPS. This allows you to keep config.json using the HTTPS configuration for production but serve the web app via HTTP during development.\n The script should be saved alongside the config.json in the root of the web app. I would recommend running chmod a+x runserver to enable you to execute the server directly via ./runserver. Alternatively, you could install this somewhere on your system path to invoke from any directory (something like cp runserver /usr/local/bin/serve-esri-app for a unix-based system).\n","permalink":"https://alukach.com/posts/serve-esri-webapp-http/","summary":"When an Esri Web AppBuilder web app is configured with a portalUrl value served from HTTPS, the web app automatically redirects users to HTTPS when visited via HTTP. While this is best-practice in production, it can be a burden in development when you want to quickly run a local version of the web app. Below is a quick script written with Python standard libraries to serve a web app over HTTP.","title":"Serve an Esri Web AppBuilder web app from HTTP"},{"content":"Full Disclosure: I am NOT an expert at Jupyter or Anaconda (which I am using in this project), there may be some bad habits below\u0026hellip;\nBelow is a quick scratchpad of the steps I took to serve Jupyter from a subdomain. Jupyter is running behind NGINX on an OpenStack Ubuntu instance and the domain\u0026rsquo;s DNS is set up to use Cloudflare to provides convenient SSL support. I was suprised by the lack of documentation for this process, prompting me to document my steps taken here.\nCloudflare  Set up Cloudflare account, utilizing its provided Name Servers with my domain registration. Set up Cloudflare DNS Record for subdomain (ex jupyter to server from jupyter.mydomain.com). In the image below, the DNS entry for the Jupyter server was \u0026ldquo;greyed-out\u0026rdquo;, relegating it to \u0026ldquo;DNS Only\u0026rdquo; rather than \u0026ldquo;DNS and HTTP Proxy (CDN)\u0026rdquo;.. Now that Cloudflare supports websockets, this is no longer necessary and you\u0026rsquo;re able to take advantage of using Cloudflare as a CDN (admittedly, I\u0026rsquo;m not sure how useful this actually is, but it\u0026rsquo;s worth mentioning).  Ensure Crypto settings are set correctly. You should probably be using Full SSL (Strict) rather than Flexible SSL as shown in the image below, however that is outside the scope of this post.   Install Anaconda Follow instructions described here.\nSet up an Upstart script On the server, you\u0026rsquo;ll want Jupyter to start running as soon as the server starts. We\u0026rsquo;ll use an Upstart script to acheive this.\n# /etc/init/ipython-notebook.conf start on filesystem or runlevel [2345] stop on shutdown # Restart the process if it dies with a signal # or exit code not given by the 'normal exit' stanza. respawn # Give up if restart occurs 10 times in 90 seconds. respawn limit 10 90 description \u0026quot;Jupyter / IPython Notebook Upstart script\u0026quot; setuid \u0026quot;MY_USER\u0026quot; setgid \u0026quot;MY_USER\u0026quot; chdir \u0026quot;/home/MY_USER/notebooks\u0026quot; script exec /home/MY_USER/.anaconda3/bin/jupyter notebook --config='/home/MY_USER/.jupyter/jupyter_notebook_config.py' end script Configure Jupyter Populate Jupyter with required configuration. You should probably auto-generate the configuration first and then just change the applicable variables.\n1 2 3 4 5 6 7 8 9 10 11 12  # .jupyter/jupyter_notebook_config.py c.NotebookApp.allow_origin = \u0026#39;https://jupyter.mydomain.com\u0026#39; c.NotebookApp.notebook_dir = \u0026#39;/home/MY_USER/notebooks\u0026#39; c.NotebookApp.open_browser = False c.NotebookApp.password = \u0026#39;some_password_hash\u0026#39; c.NotebookApp.port = 8888 c.NotebookApp.kernel_spec_manager_class = \u0026#34;nb_conda_kernels.CondaKernelSpecManager\u0026#34; c.NotebookApp.nbserver_extensions = { \u0026#34;nb_conda\u0026#34;: True, \u0026#34;nb_anacondacloud\u0026#34;: True, \u0026#34;nbpresent\u0026#34;: True }   Wire Jupyter up with Nginx To be able to access Jupyter at port 80, we\u0026rsquo;ll need to reverse proxy to the service. Nginx can take care of this for us. Jupyter uses websockets to stream data to the client, so some\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  # /etc/nginx/sites-enabled/jupyter.conf # Based on example: https://gist.github.com/cboettig/8643341bd3c93b62b5c2 upstream jupyter { server 127.0.0.1:8888 fail_timeout=0; } map $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; # Make site accessible from http://localhost/  server_name localhost; client_max_body_size 50M; location / { proxy_pass http://jupyter; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location ~* /(api/kernels/[^/]+/(channels|iopub|shell|stdin)|terminals/websocket)/? { proxy_pass http://jupyter; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support  proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }   ","permalink":"https://alukach.com/posts/serving-jupyter/","summary":"Full Disclosure: I am NOT an expert at Jupyter or Anaconda (which I am using in this project), there may be some bad habits below\u0026hellip;\nBelow is a quick scratchpad of the steps I took to serve Jupyter from a subdomain. Jupyter is running behind NGINX on an OpenStack Ubuntu instance and the domain\u0026rsquo;s DNS is set up to use Cloudflare to provides convenient SSL support. I was suprised by the lack of documentation for this process, prompting me to document my steps taken here.","title":"Hosting Jupyter at a subdomain via Cloudflare"},{"content":"Continuing with the Django Admin Fu post part 1.\nAction with Intermediate Page Sometimes you may need an admin action that, when submitted, takes the user to a form where they provides some additional detail. The docs mention a bit about providing intermediate pages, but not a lot. It states:\n Generally, something like [writing a intermediate page through the admin] isn’t considered a great idea. Most of the time, the best practice will be to return an HttpResponseRedirect and redirect the user to a view you’ve written, passing the list of selected objects in the GET query string. This allows you to provide complex interaction logic on the intermediary pages.\n I do see where the docs are coming from and it would probably be easier to do as advised, but I think there could be something said about keeping all admin logic within the admin page. Doing something like the following would take the user to an intermediate form.\n ","permalink":"https://alukach.com/posts/django-admin-pt-2/","summary":"Continuing with the Django Admin Fu post part 1.\nAction with Intermediate Page Sometimes you may need an admin action that, when submitted, takes the user to a form where they provides some additional detail. The docs mention a bit about providing intermediate pages, but not a lot. It states:\n Generally, something like [writing a intermediate page through the admin] isn’t considered a great idea. Most of the time, the best practice will be to return an HttpResponseRedirect and redirect the user to a view you’ve written, passing the list of selected objects in the GET query string.","title":"Django Admin Fu, part 2"},{"content":"I\u0026rsquo;ve been putting some time into building out the Django Admin site for one of my company\u0026rsquo;s projects. Here are some notes I\u0026rsquo;ve taken about straying away from the beaten path. I find surprisingly little information about how to do these things on StackOverflow or elsewhere. These were put used when working with Django 1.6.7.\nFake The Model, Make The View You may want a form on the Django Admin that exists along side the model views but doesn\u0026rsquo;t actually represent a model. This strays somewhat from what the Django Admin is set up to do (some on the #django channel on Freenode have stated that the admin should only be for CRUD operations on Django models.) None-the-less, if you do want to inject a form into the admin along side your models, this is a method that worked for me.\nIt revolves around generating a fake model that you register to your app\u0026rsquo;s admin view. After that, you create a model admin that inherits from the standard ModelAdmin.\n See more in part 2.\n","permalink":"https://alukach.com/posts/django-admin-pt-1/","summary":"I\u0026rsquo;ve been putting some time into building out the Django Admin site for one of my company\u0026rsquo;s projects. Here are some notes I\u0026rsquo;ve taken about straying away from the beaten path. I find surprisingly little information about how to do these things on StackOverflow or elsewhere. These were put used when working with Django 1.6.7.\nFake The Model, Make The View You may want a form on the Django Admin that exists along side the model views but doesn\u0026rsquo;t actually represent a model.","title":"Django Admin Fu, part 1"},{"content":"Here is a quick dump of some of the better resources that I came across while learning AngularJS.\nStackOverflow: How to \u0026lsquo;think in AngularJS\u0026rsquo; - Great for getting the appropriate mindset.\negghead.io\u0026rsquo;s AngularJS series by John Lindquist - Excellently cut up into discrete segments to cover fundamentals.\nIntroduction to AngularJS - First in a series of developing an Angular app. Then watch [End to End](End to End with Angular JS), Security, Frontend Workflows, Testing\n","permalink":"https://alukach.com/posts/notes-on-learning-angularjs/","summary":"Here is a quick dump of some of the better resources that I came across while learning AngularJS.\nStackOverflow: How to \u0026lsquo;think in AngularJS\u0026rsquo; - Great for getting the appropriate mindset.\negghead.io\u0026rsquo;s AngularJS series by John Lindquist - Excellently cut up into discrete segments to cover fundamentals.\nIntroduction to AngularJS - First in a series of developing an Angular app. Then watch [End to End](End to End with Angular JS), Security, Frontend Workflows, Testing","title":"Learning AngularJS"},{"content":"As I was transitioning from SublimeText2 to SublimeText3, it became apparent that I should keep a copy of my favorite text editor\u0026rsquo;s plugins and settings.\n ","permalink":"https://alukach.com/posts/sublimetext3-setup/","summary":"As I was transitioning from SublimeText2 to SublimeText3, it became apparent that I should keep a copy of my favorite text editor\u0026rsquo;s plugins and settings.\n ","title":"SublimeText3 Setup"},{"content":"I\u0026rsquo;ve been experimenting with Python\u0026rsquo;s Natural Language Toolkit, following along with Steven Bird, Ewan Klein, and Edward Loper\u0026rsquo;s book \u0026ldquo;Natural Language Processing with Python \u0026mdash; Analyzing Text with the Natural Language Toolkit\u0026rdquo; (pdf version).\nSo far, the book\u0026rsquo;s been great. As I\u0026rsquo;m going through the book, I\u0026rsquo;ve been writing down notes relating to the book\u0026rsquo;s examples. I\u0026rsquo;ve made a Github repo to store these notes and experiments that I may be doing using the NLTK here.\n","permalink":"https://alukach.com/posts/nltk-notes/","summary":"I\u0026rsquo;ve been experimenting with Python\u0026rsquo;s Natural Language Toolkit, following along with Steven Bird, Ewan Klein, and Edward Loper\u0026rsquo;s book \u0026ldquo;Natural Language Processing with Python \u0026mdash; Analyzing Text with the Natural Language Toolkit\u0026rdquo; (pdf version).\nSo far, the book\u0026rsquo;s been great. As I\u0026rsquo;m going through the book, I\u0026rsquo;ve been writing down notes relating to the book\u0026rsquo;s examples. I\u0026rsquo;ve made a Github repo to store these notes and experiments that I may be doing using the NLTK here.","title":"Natural Language Toolkit Notes"},{"content":"Becoming tired of typing paths repeatedly in the terminal, I realized that I should be using pushd and popd to be navigating directory structures. For those uninitiated, pushd changes your current directory in a similar fashion to cd but additionally adds the former directory to a stack. You can later return to the former directory by executing popd, popping it from the directory history. Unfortunately, the commands pushd and popd both require at least twice as many characters to type as cd and additionally come with the overhead of having to learnt o use a new command instead of something that is nearly instinctual. Then it came to me: pushd all the time.\nOverriding cd with a muted pushd operates exactly like the standard cd command, with the added benefity that the path history is saved. Furthermore, adding an alias of p to popd allows the previous directory to be popped with minimal effort.\nAdditionally, when exploring the idea, I came across this StackExchange post illustrating a back function, allowing you to switch back and forth between your current and previous directory with removing either from the stack. In the end, this is what I put in my bash profile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # CD is now silent pushd cd() { if [ $# -eq 0 ]; then DIR=\u0026#34;${HOME}\u0026#34; else DIR=\u0026#34;$1\u0026#34; fi builtin pushd \u0026#34;${DIR}\u0026#34; \u0026gt; /dev/null } # Take you back without popd back() { builtin pushd \u0026gt; /dev/null dirs } alias p=\u0026#39;popd\u0026#39; alias b=\u0026#39;back\u0026#39;   ","permalink":"https://alukach.com/posts/pushd-all-day-long/","summary":"Becoming tired of typing paths repeatedly in the terminal, I realized that I should be using pushd and popd to be navigating directory structures. For those uninitiated, pushd changes your current directory in a similar fashion to cd but additionally adds the former directory to a stack. You can later return to the former directory by executing popd, popping it from the directory history. Unfortunately, the commands pushd and popd both require at least twice as many characters to type as cd and additionally come with the overhead of having to learnt o use a new command instead of something that is nearly instinctual.","title":"pushd and popd forever"},{"content":"The other week I found myself up at 2am in Canada setting up a VPN between my home computer (running Ubuntu) in Seattle and my laptop \u0026lt;partyhard.jpg\u0026gt;. I had enabled SSH access on my home computer and had set up port forwarding on my router to allow for access from the outside world ahead of time, but had forgotten that I would need to have a port forwarded for the VPN server as well. I tried to SSH into my home box and access the router\u0026rsquo;s admin interface from the commandline browser (using Lynx and w3m). This was a bad idea and didn\u0026rsquo;t work, as the browser\u0026rsquo;s admin page required JavaScript for some odd reason.\nAnd then I remembered this command:\n1  ssh -D 8080 -Nf login@server.whatever.com   Pointed my browser\u0026rsquo;s connection settings to SOCKS proxy with server as localhost and port at 8080 and BOOM, was able to access my Seattle home\u0026rsquo;s router\u0026rsquo;s config page from Canada. I\u0026rsquo;ve found this trick useful for all sorts of things, typically for one-offs where I need to access a website from the US while in Canada.\nEDIT:\nAnother useful command for when you need to connect to any given port on a remote server is the following:\n1  ssh -N -L [local_port]:[endpoint]:[remote_port] [user]@[host]   ","permalink":"https://alukach.com/posts/ssh-port-forwarding/","summary":"The other week I found myself up at 2am in Canada setting up a VPN between my home computer (running Ubuntu) in Seattle and my laptop \u0026lt;partyhard.jpg\u0026gt;. I had enabled SSH access on my home computer and had set up port forwarding on my router to allow for access from the outside world ahead of time, but had forgotten that I would need to have a port forwarded for the VPN server as well.","title":"SSH Port Forwarding"},{"content":"I\u0026rsquo;m just getting things set up with this new blog. I\u0026rsquo;ve been hearing about this movement towards static-generated blogs for a while now, ever since reading this article about the Obama Campaign\u0026rsquo;s fundraising platform. The idea of stepping away from databases and convulated CMS\u0026rsquo;s and PHP attracted me.\nThis site is built with Jekyll. After seeing how simple the template syntax was (based on LiquidMarkup, not unlike Django or Jinja2\u0026rsquo;s syntax), I was sold. Furthermore, learning that Github would post the site for free made it a no-brainer.\nSo, now this page is up, hosted for free by Github and running a modified version of Zach Holman\u0026rsquo;s Left theme augmented with some pieces from JekyllBootstrap.\n","permalink":"https://alukach.com/posts/hello-world/","summary":"I\u0026rsquo;m just getting things set up with this new blog. I\u0026rsquo;ve been hearing about this movement towards static-generated blogs for a while now, ever since reading this article about the Obama Campaign\u0026rsquo;s fundraising platform. The idea of stepping away from databases and convulated CMS\u0026rsquo;s and PHP attracted me.\nThis site is built with Jekyll. After seeing how simple the template syntax was (based on LiquidMarkup, not unlike Django or Jinja2\u0026rsquo;s syntax), I was sold.","title":"Hello World"}]