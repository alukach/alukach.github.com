[{"content":"Below is a simple script to allow a user to alter RDS databases security groups to allow access from an ECS Service. Useful when we have an observability tool runing in ECS that wants to add RDS data connections.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 from typing import List, Dict import boto3 from botocore.exceptions import ClientError import inquirer def list_ecs_clusters(): ecs = boto3.client(\u0026#34;ecs\u0026#34;) clusters = ecs.list_clusters() cluster_arns = clusters[\u0026#34;clusterArns\u0026#34;] return cluster_arns def list_ecs_services(cluster) -\u0026gt; List[str]: ecs = boto3.client(\u0026#34;ecs\u0026#34;) services = ecs.list_services(cluster=cluster) return services[\u0026#34;serviceArns\u0026#34;] def list_rds_instances() -\u0026gt; Dict[str, str]: rds = boto3.client(\u0026#34;rds\u0026#34;) return rds.describe_db_instances()[\u0026#34;DBInstances\u0026#34;] def get_security_group_from_ecs_service(cluster, service_arn) -\u0026gt; str: ecs = boto3.client(\u0026#34;ecs\u0026#34;) details = ecs.describe_services(cluster=cluster, services=[service_arn]) service = details[\u0026#34;services\u0026#34;][0] deployment = service[\u0026#34;deployments\u0026#34;][0] groups = deployment[\u0026#34;networkConfiguration\u0026#34;][\u0026#34;awsvpcConfiguration\u0026#34;][\u0026#34;securityGroups\u0026#34;] return groups[0] def get_security_group_ids_for_rds_instance(instance_identifier) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34; Returns the security group IDs for a given RDS instance identifier. :param instance_identifier: The identifier of the RDS instance :return: A list of security group IDs associated with the RDS instance \u0026#34;\u0026#34;\u0026#34; rds = boto3.client(\u0026#34;rds\u0026#34;) try: response = rds.describe_db_instances(DBInstanceIdentifier=instance_identifier) db_instances = response[\u0026#34;DBInstances\u0026#34;] if db_instances: # Assuming each instance has at least one security group associated with it security_groups = db_instances[0][\u0026#34;VpcSecurityGroups\u0026#34;] security_group_ids = [sg[\u0026#34;VpcSecurityGroupId\u0026#34;] for sg in security_groups] return security_group_ids else: return [] except Exception as e: print( f\u0026#34;Error fetching security group IDs for RDS instance \u0026#39;{instance_identifier}\u0026#39;: {e}\u0026#34; ) return [] def modify_security_group_rules( security_group_id, protocol, from_port, to_port, source_security_group_id: str, description: str, dry_run: bool, ) -\u0026gt; None: ec2 = boto3.client(\u0026#34;ec2\u0026#34;) if dry_run: print( f\u0026#34;Dry run: Would update Security Group {security_group_id} \u0026#34; f\u0026#34;to allow from {source_security_group_id}\u0026#34; ) return try: ec2.authorize_security_group_ingress( GroupId=security_group_id, IpPermissions=[ { \u0026#34;IpProtocol\u0026#34;: protocol, \u0026#34;FromPort\u0026#34;: from_port, \u0026#34;ToPort\u0026#34;: to_port, \u0026#34;UserIdGroupPairs\u0026#34;: [ { \u0026#34;GroupId\u0026#34;: source_security_group_id, \u0026#34;Description\u0026#34;: description, } ], } ], ) print(f\u0026#34;Security Group {security_group_id} updated successfully.\u0026#34;) except ClientError as e: print(f\u0026#34;Error updating Security Group: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: dry_run = inquirer.confirm( message=\u0026#34;Dry run (no changes will be made)?\u0026#34;, default=True, ) ecs_clusters = list_ecs_clusters() rds_instances = list_rds_instances() # Select ECS Cluster resource_questions = [ inquirer.List( \u0026#34;cluster\u0026#34;, message=\u0026#34;Select ECS Cluster that contains the service that requires databases access\u0026#34;, choices=ecs_clusters, default=lambda answers: next( (cluster for cluster in ecs_clusters if \u0026#34;grafana\u0026#34; in cluster), None ), ), inquirer.List( \u0026#34;service\u0026#34;, message=\u0026#34;Select ECS Service in {cluster} that requires database access\u0026#34;, choices=lambda answers: list_ecs_services(answers[\u0026#34;cluster\u0026#34;]), ), inquirer.Checkbox( \u0026#34;rds_instances\u0026#34;, message=\u0026#34;Select RDS Instances that will be accessed via {cluster}/{service}\u0026#34;, choices=[instance[\u0026#34;DBInstanceIdentifier\u0026#34;] for instance in rds_instances], ), inquirer.Text( \u0026#34;description\u0026#34;, message=\u0026#34;Provide a description for the connection\u0026#34;, default=lambda answers: f\u0026#34;Allow connections from ECS service: {answers[\u0026#39;service\u0026#39;].split(\u0026#39;:\u0026#39;)[-1]}\u0026#34;, ), ] resources = inquirer.prompt(resource_questions) # Get Security Group of selected RDS Instances rds_security_groups = [ get_security_group_ids_for_rds_instance(rds_instance)[0] for rds_instance in resources[\u0026#34;rds_instances\u0026#34;] ] # Get Security Group of selected ECS Service ecs_security_group = get_security_group_from_ecs_service( resources[\u0026#34;cluster\u0026#34;], resources[\u0026#34;service\u0026#34;] ) # Update RDS Instances\u0026#39; Security Groups to allow inbound connections from ECS Service print(f\u0026#34;{rds_security_groups=}\u0026#34;) for rds_sg_id in rds_security_groups: modify_security_group_rules( security_group_id=rds_sg_id, source_security_group_id=ecs_security_group, protocol=\u0026#34;tcp\u0026#34;, to_port=5432, from_port=5432, description=resources[\u0026#34;description\u0026#34;], dry_run=dry_run, ) ","permalink":"https://alukach.com/posts/ecs-rds-security-group-script/","summary":"Below is a simple script to allow a user to alter RDS databases security groups to allow access from an ECS Service. Useful when we have an observability tool runing in ECS that wants to add RDS data connections.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 from typing import List, Dict import boto3 from botocore.","title":"An ECS -\u003e RDS Securty Group Script"},{"content":"Recently, we exported data from a DynamoDB table to S3 in AWS Ion format. However, due to the fact that the DynamoDB table had varied formats for some numeric properties, the export serialized these numeric data columns in a few different formats: as a decimal (1234.), as an Ion decimal type (1234d0), and as a string (\u0026quot;1234\u0026quot;). However, we want to be able to treat these values as a bigint within our Athena queries.\nOur solution was to create a view similar to the following that would convert any of those formats into a bigint:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 SELECT CAST( -- Convert `size` from Ion Decimal string to bigint CAST( SUBSTRING( i.size, 1, CASE STRPOS(i.size, \u0026#39;d\u0026#39;) WHEN 0 THEN LENGTH(i.size) ELSE STRPOS(i.size, \u0026#39;d\u0026#39;) - 1 END ) AS DECIMAL(32, 16) ) * POWER( 10, CASE STRPOS(i.size, \u0026#39;d\u0026#39;) WHEN 0 THEN 0 ELSE CAST( SUBSTRING( i.size, STRPOS(i.size, \u0026#39;d\u0026#39;) + 1, LENGTH(i.size) ) as BIGINT ) END ) as BIGINT ) size FROM table i This works by splitting the values the location of a d character within the value and multiplying the value to the left of the d character by 10 to the power of the value to the right of the d character. Examples:\n1234d1 -\u0026gt; 1234 * 10 ** 1 -\u0026gt; 12340 12345 -\u0026gt; 12345 * 10 ** 0 -\u0026gt; 12345 ","permalink":"https://alukach.com/posts/heterogeneous-ion-decimal-data/","summary":"Recently, we exported data from a DynamoDB table to S3 in AWS Ion format. However, due to the fact that the DynamoDB table had varied formats for some numeric properties, the export serialized these numeric data columns in a few different formats: as a decimal (1234.), as an Ion decimal type (1234d0), and as a string (\u0026quot;1234\u0026quot;). However, we want to be able to treat these values as a bigint within our Athena queries.","title":"Normalizing heterogeneous decimal Ion data in Athena"},{"content":"These are some notes that I have needed to write down about using cookies in web applications. Admittedly, I don\u0026rsquo;t know a lot about cookies and should probably not be considered a source of authority on this topic.\nWhy cookies? Making authenticated anchor tags Can\u0026rsquo;t specify headers with \u0026lt;a\u0026gt; tags.\nCould supply token as query parameter, but that\u0026rsquo;s a security concern due to potential of token being cached with URL.\nNo need to manage JWTs within your application Things that are annoying about JWTs when building frontend applications:\nYou need to choose where to store the JWTs You need to supply the JWT to any code making API requests How cookies? Cookies are typically set by backend code after a user successfully logs in. They are typically signed to verify their authenticity. Often, they simply point to an identifier that tracks a user\u0026rsquo;s \u0026ldquo;session\u0026rdquo; within some stateful service (ie database).\nHow to do cross-origin cookies Backend must set the access-control-allow-credentials header to instruct browsers that it’s okay to pass credentials when making requests (docs). Backend must specify the access-control-allow-origin header to instruct browsers which origins are allowed to access the API. Note that you can’t use * when passing credentials between origins (docs). Backend will need to specify that our cookies have samesite=none if attempting to send cookie to other sites. Note that Same-Site is not the same as Same-Origin, a cookie set with samesite=strict will still be passed between hosts at different subdomains or ports of a domain (docs). Also note that Secure must also be set when samesite=none (docs). Backend must skip auth checks on preflight requests, as browsers don’t send cookies on OPTIONS requests (docs) Frontend needs to provide credentials with requests. 1 await fetch(url, { credentials: true }); ","permalink":"https://alukach.com/posts/notes-on-cookies/","summary":"These are some notes that I have needed to write down about using cookies in web applications. Admittedly, I don\u0026rsquo;t know a lot about cookies and should probably not be considered a source of authority on this topic.\nWhy cookies? Making authenticated anchor tags Can\u0026rsquo;t specify headers with \u0026lt;a\u0026gt; tags.\nCould supply token as query parameter, but that\u0026rsquo;s a security concern due to potential of token being cached with URL.","title":"Notes on Cookies 🍪"},{"content":"A convenience function to assume a IAM Role via STS before running a command.\nAdd the following to your ~/.zshrc (or equivalent) file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 function with-role { readonly role_arn=${1:?\u0026#34;The role_arn must be specified.\u0026#34;} env -S $( aws sts assume-role \\ --role-arn ${role_arn} \\ --role-session-name ${USER} \\ | \\ jq -r \u0026#39;.Credentials | \u0026#34; AWS_ACCESS_KEY_ID=\\(.AccessKeyId) AWS_SECRET_ACCESS_KEY=\\(.SecretAccessKey) AWS_SESSION_TOKEN=\\(.SessionToken) \u0026#34;\u0026#39; ) ${@:2} } This assumes that you have both the AWS CLI and jq installed.\nExample usage:\nwith-role arn:aws:iam::123456789012:role/someSpecialRole aws s3 ls ","permalink":"https://alukach.com/posts/with-role-command/","summary":"A convenience function to assume a IAM Role via STS before running a command.\nAdd the following to your ~/.zshrc (or equivalent) file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 function with-role { readonly role_arn=${1:?\u0026#34;The role_arn must be specified.\u0026#34;} env -S $( aws sts assume-role \\ --role-arn ${role_arn} \\ --role-session-name ${USER} \\ | \\ jq -r \u0026#39;.Credentials | \u0026#34; AWS_ACCESS_KEY_ID=\\(.AccessKeyId) AWS_SECRET_ACCESS_KEY=\\(.SecretAccessKey) AWS_SESSION_TOKEN=\\(.SessionToken) \u0026#34;\u0026#39; ) ${@:2} } This assumes that you have both the AWS CLI and jq installed.","title":"Auto-assume an IAM role before running a command"},{"content":"When building systems to process messages, it\u0026rsquo;s not unlikely to find yourself in a situation where you need to process a number of inputted heterogeneous messages (i.e. messages of varying shapes/types). For example, consider a situation where you are processing messages from an SQS queue via a Lambda function. This post attempts to highlight how this can be achieved in a clean and elegant manner by utilizing Pydantic, Python\u0026rsquo;s typing system, and some helpers from the Python standard library.\nCategorizing messages of unknown type The first thing you likely need to do is identify the type of an inputted message by its properties. We can use Pydantic to model the types of messages we expect to have coming into our system. We can then utilize Pydantic\u0026rsquo;s parse_obj_as function to cast these messages to their respective Pydantic classes.\nIn the following example, we are able to distinguish between messages based on the attributes that they contain:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import pydantic import typing class Email(pydantic.BaseModel): to: typing.List[str] subject: str message: str class Sms(pydantic.BaseModel): to: str message: str # Create a type demonstrating all of the expected messages SupportedMessages = typing.Union[Email, Sms] # Pass in our new type with a list of uncategorized messages messages = pydantic.parse_obj_as( typing.List[SupportedMessages], [ { \u0026#34;to\u0026#34;: [\u0026#39;bill@gmail.com\u0026#39;, \u0026#39;alice@outlook.com\u0026#39;], \u0026#34;subject\u0026#34;: \u0026#34;BBQ Emergency\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Need more ketchup!\u0026#34; }, { \u0026#34;to\u0026#34;: \u0026#34;911\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Burnt finger at BBQ :(\u0026#34; } ] ) # They have now been cast to their appropriate types print(messages) #\u0026gt; [Email(to=[\u0026#39;bill@gmail.com\u0026#39;, \u0026#39;alice@outlook.com\u0026#39;], subject=\u0026#39;BBQ Emergency\u0026#39;, message=\u0026#39;Need more ketchup!\u0026#39;), Sms(to=\u0026#39;911\u0026#39;, message=\u0026#39;Burnt finger at BBQ :(\u0026#39;)] Other times, messages are differentiated based on the value of a particular attribute. The same pattern applies:\nExample of value-based differentiation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import pydantic import typing class Email(pydantic.BaseModel): type: typing.Literal[\u0026#34;email\u0026#34;] to: str message: str class Sms(pydantic.BaseModel): type: typing.Literal[\u0026#34;sms\u0026#34;] to: str message: str # Create a type demonstrating all of the expected messages SupportedMessages = typing.Union[Email, Sms] # Pass in our new type with a list of uncategorized messages messages = pydantic.parse_obj_as( typing.List[SupportedMessages], [ { \u0026#34;type\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;paul@outlook.com\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Head\u0026#39;s up, Ringo has a new idea\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;sms\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;867-5309\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;New phone, who dis?\u0026#34; } ] ) # They have now been cast to their appropriate types print(messages) #\u0026gt; [Email(type=\u0026#39;email\u0026#39;, to=\u0026#39;paul@outlook.com\u0026#39;, message=\u0026#34;Head\u0026#39;s up, Ringo has a new idea\u0026#34;), Sms(type=\u0026#39;sms\u0026#39;, to=\u0026#39;867-5309\u0026#39;, message=\u0026#39;New phone, who dis?\u0026#39;)] Edge Cases Unknown types In the event that a message does not fit any model, a pydantic.ValidationError will be thrown:\nExample of an unexpected message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import pydantic import typing class Email(pydantic.BaseModel): to: typing.List[str] subject: str message: str pydantic.parse_obj_as(Email, {\u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;}) #\u0026gt; Traceback (most recent call last): # File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; # File \u0026#34;pydantic/tools.py\u0026#34;, line 38, in pydantic.tools.parse_obj_as # File \u0026#34;pydantic/main.py\u0026#34;, line 341, in pydantic.main.BaseModel.__init__ # pydantic.error_wrappers.ValidationError: 3 validation errors for ParsingModel[Email] # __root__ -\u0026gt; to # field required (type=value_error.missing) # __root__ -\u0026gt; subject # field required (type=value_error.missing) # __root__ -\u0026gt; message # field required (type=value_error.missing) Similar types You can find yourself in challenging situations when one type is a subset of another:\nExample of a situation where you can differentiate between similar message types 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pydantic import typing class Person(pydantic.BaseModel): name: str class Pet(pydantic.BaseModel): name: str breed: str print(pydantic.parse_obj_as(typing.Union[Person, Pet], {\u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;})) #\u0026gt; Person(name=\u0026#39;Bob\u0026#39;) pydantic.parse_obj_as(typing.Union[Person, Pet], {\u0026#34;name\u0026#34;: \u0026#34;Fido\u0026#34;, \u0026#34;breed\u0026#34;: \u0026#34;poodle\u0026#34;}) #\u0026gt; Person(name=\u0026#39;Fido\u0026#39;) By default, Pydantic permits extra attributes on models. By specifying that extra attributes are forbidden via the extra option, we can help Pydantic narrow in on the correct type.\nExample of using `extra` to help differentiate between similar message types 1 2 3 4 5 6 7 8 9 10 11 class Person(pydantic.BaseModel, extra=pydantic.Extra.forbid): name: str class Pet(pydantic.BaseModel, extra=pydantic.Extra.forbid): name: str breed: str pydantic.parse_obj_as(typing.Union[Person, Pet], {\u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;}) #\u0026gt; Person(name=\u0026#39;Bob\u0026#39;) pydantic.parse_obj_as(typing.Union[Person, Pet], {\u0026#34;name\u0026#34;: \u0026#34;Fido\u0026#34;, \u0026#34;breed\u0026#34;: \u0026#34;poodle\u0026#34;}) #\u0026gt; Pet(name=\u0026#39;Fido\u0026#39;, breed=\u0026#39;poodle\u0026#39;) Processing Messages Now that we have our messages categorized, it\u0026rsquo;s likely that you\u0026rsquo;ll want to process each message according to its type. We could write a long if isinstance(msg, TypeA): ... elif isinstance(msg, TypeB): ..., but that\u0026rsquo;s no fun. Instead, we can reach for Python\u0026rsquo;s functools module, which has a convenient singledispatch decorator.\nFor those of us who aren\u0026rsquo;t function programming wizards (e.g. myself), here are some helpful definitions from Python\u0026rsquo;s glossary:\nsingle dispatch A form of generic function dispatch where the implementation is chosen based on the type of a single argument. generic function A function composed of multiple functions implementing the same operation for different types. Which implementation should be used during a call is determined by the dispatch algorithm. Let\u0026rsquo;s take a look at how that could work:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import functools import pydantic import typing class Foo(pydantic.BaseModel): type: typing.Literal[\u0026#34;foo\u0026#34;] class Bar(pydantic.BaseModel): type: typing.Literal[\u0026#34;foo\u0026#34;] @functools.singledispatch def send(msg): # this is the default sender, which should only ever be called if a message # comes in with a type for which we haven\u0026#39;t registered a handler. in this # situation, we may want to throw an error to signal that we don\u0026#39;t know how # to handle this message; alternatively we may want to have a default handler # that applies to any types without explicitly registered handlers. ... @send.register def handle_foo(msg: Foo): ... @send.register def handle_bar(msg: Bar): ... Example of applying this pattern to our previous Email/SMS types 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import functools import pydantic import typing class Email(pydantic.BaseModel): to: typing.List[str] subject: str message: str class Sms(pydantic.BaseModel): to: str message: str @functools.singledispatch def send(msg): # this is the default sender, which should only ever be called if a message # comes in with a type for which we haven\u0026#39;t registered a handler. in this # situation, we may want to throw an error to signal that we don\u0026#39;t know how # to handle this message; alternatively we may want to have a default handler # that applies to any types without explicitly registered handlers. raise Exception(f\u0026#34;Unexpected message type ({type(msg)=}, {msg})\u0026#34;) @send.register def send_email(msg: Email): print(f\u0026#34;Sending email to {\u0026#39; and \u0026#39;.join(msg.to)}\u0026#34;) @send.register def send_sms(msg: Sms): print(f\u0026#34;Sending SMS to {msg.to}\u0026#34;) def handle_message(message: typing.Dict[str, typing.Any]): parsed_message = pydantic.parse_obj_as(typing.Union[Email, Sms], message) send(parsed_message) handle_message({ \u0026#34;to\u0026#34;: [\u0026#39;bill@gmail.com\u0026#39;, \u0026#39;alice@outlook.com\u0026#39;], \u0026#34;subject\u0026#34;: \u0026#34;BBQ Emergency\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Need more ketchup!\u0026#34; }) #\u0026gt; Sending email to bill@gmail.com and alice@outlook.com handle_message({ \u0026#34;to\u0026#34;: \u0026#34;911\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Burnt finger at BBQ :(\u0026#34; }) #\u0026gt; Sending SMS to 911 Testing This type-based message handling is neat and all, but can we test this? I found it to be a bit challenging to integrate mocking with the @functools.singledispatch, but ended up using a simple context manager to conveniently swap out registered type handlers with mocks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import contextlib import typing import unittest.mock @contextlib.contextmanager def override_registry( dispatch_callable: \u0026#34;_SingleDispatchCallable[Any]\u0026#34;, cls: typing.Type, mock: unittest.mock.Mock, ): \u0026#34;\u0026#34;\u0026#34; Helper to override a singledispatch function with a mock for testing. \u0026#34;\u0026#34;\u0026#34; original = dispatch_callable.registry[cls] dispatch_callable.register(cls, mock) try: yield mock finally: dispatch_callable.register(cls, original) Example of a full set of tests to validate that routing logic is appropriately configured 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import functools import pydantic import typing import contextlib import unittest.mock class Email(pydantic.BaseModel): to: typing.List[str] subject: str message: str class Sms(pydantic.BaseModel): to: str message: str @functools.singledispatch def send(msg): # this is the default sender, which should only ever be called if a message # comes in with a type for which we haven\u0026#39;t registered a handler. in this # situation, we may want to throw an error to signal that we don\u0026#39;t know how # to handle this message; alternatively we may want to have a default handler # that applies to any types without explicitely registered handlers. raise Exception(f\u0026#34;Unexpected message type ({type(msg)=}, {msg})\u0026#34;) @send.register def send_email(msg: Email): print(f\u0026#34;Sending email to {\u0026#39; and \u0026#39;.join(msg.to)}\u0026#34;) @send.register def send_sms(msg: Sms): print(f\u0026#34;Sending SMS to {msg.to}\u0026#34;) def handle_message(message: typing.Dict[str, typing.Any]): parsed_message = pydantic.parse_obj_as(typing.Union[Email, Sms], message) send(parsed_message) @contextlib.contextmanager def override_registry( dispatch_callable: \u0026#34;_SingleDispatchCallable[Any]\u0026#34;, cls: typing.Type, mock: unittest.mock.Mock, ): \u0026#34;\u0026#34;\u0026#34; Helper to override a singledispatch function with a mock for testing. \u0026#34;\u0026#34;\u0026#34; original = dispatch_callable.registry[cls] dispatch_callable.register(cls, mock) try: yield mock finally: dispatch_callable.register(cls, original) def test_handling_email(): \u0026#34;\u0026#34;\u0026#34; Ensure that the system properly handles Email messages. \u0026#34;\u0026#34;\u0026#34; with override_registry( send, Email, unittest.mock.MagicMock() ) as called_mock, override_registry( send, Sms, unittest.mock.MagicMock() ) as not_called_mock: output = handle_message({ \u0026#34;to\u0026#34;: [\u0026#39;bill@gmail.com\u0026#39;, \u0026#39;alice@outlook.com\u0026#39;], \u0026#34;subject\u0026#34;: \u0026#34;BBQ Emergency\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Need more ketchup!\u0026#34; }) assert called_mock.call_count == 1 assert not not_called_mock.call_count def test_handling_sms(): \u0026#34;\u0026#34;\u0026#34; Ensure that the system properly handles SMS messages. \u0026#34;\u0026#34;\u0026#34; with override_registry( send, Sms, unittest.mock.MagicMock() ) as called_mock, override_registry( send, Email, unittest.mock.MagicMock() ) as not_called_mock: output = handle_message({ \u0026#34;to\u0026#34;: \u0026#34;911\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Burnt finger at BBQ :(\u0026#34; }) assert called_mock.call_count == 1 assert not not_called_mock.call_count test_handling_email() test_handling_sms() This pattern is all a bit new to me and fresh in my mind. Hoping it can prove useful to others. I\u0026rsquo;m very open to hearing concerns or suggestions for improvement from others if anyone has them.\n","permalink":"https://alukach.com/posts/message-processing-with-pydantic/","summary":"When building systems to process messages, it\u0026rsquo;s not unlikely to find yourself in a situation where you need to process a number of inputted heterogeneous messages (i.e. messages of varying shapes/types). For example, consider a situation where you are processing messages from an SQS queue via a Lambda function. This post attempts to highlight how this can be achieved in a clean and elegant manner by utilizing Pydantic, Python\u0026rsquo;s typing system, and some helpers from the Python standard library.","title":"Type-based message processing with Pydantic"},{"content":"This post is a quick capture of how to easily secure your FastAPI with any auth provider that provides JWKS.\nBackground: RS256 RS256 is a signing algorithm used to generate and validate JSON Web Tokens (JWTs). Unlike the common HS256 algorithm that uses the same secret string to both generate and validate JWTs, RS256 uses a private key to generate JWTs and a separate public key for validating JWTs:\nRS256 generates an asymmetric signature, which means a private key must be used to sign the JWT and a different public key must be used to verify the signature. [source]\nThis allows you to share your public key and thus enables any service to validate JWTs (provided that the service can read the public key). This makes RS256 a great choice for distributed applications, wherein one service generates auth tokens but many services can independently validate auth tokens.\nNote: You are already using asymmetric cryptographic algorithms. For example, when you access a website over HTTPS, the SSL certificate includes a public key to allow a browser to validate messages sent by the origin server, while the origin server maintains a private key used to sign messages before they are sent. Additionally, when you set up SSH key pair for the purpose of connecting to servers, this key pair consists of a private and public key. The private is kept on your machine while a public key can be stored in a ~/.ssh/authorized_keys file on the server to validate login requests.\nBackground: JWKS The JSON Web Key Set (JWKS) is a set of keys containing the public keys used to verify any JSON Web Token (JWT) issued by the authorization server and signed using the RS256 signing algorithm. [source]\nThe JWKS is needed by each service that will be validating tokens. It can be commonly be found at /.well-known/jwks.json, however theoretically could be distributed in any other means (S3, AWS Parameter Store, etc).\nJWKS Locations Provider Location Example Cognito https://cognito-idp.{region}.amazonaws.com/{user_pool_id}/.well-known/jwks.json https://cognito-idp.us-east-1.amazonaws.com/us-east-1_Wt2sA2K9e/.well-known/jwks.json Auth0 https://YOUR_DOMAIN/.well-known/jwks.json https://example.auth0.com/.well-known/jwks.json FastAPI Integration For a FastAPI application to validate a JWT signed with an RS256 algorithm, it needs to do the following:\nLoad JWKS Retrieve token from the request Validate the token\u0026rsquo;s signature against the JWKS Below, I\u0026rsquo;ve added a simple way to achieve this by taking advantage of FastAPI\u0026rsquo;s dependency injection system and Authlib:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 import logging from functools import lru_cache from authlib.jose import JsonWebToken, JsonWebKey, KeySet, JWTClaims, errors from cachetools import cached, TTLCache from fastapi import FastAPI, Depends, HTTPException, security import requests import pydantic logger = logging.getLogger(__name__) token_scheme = security.HTTPBearer() class Settings(pydantic.BaseSettings): cognito_user_pool_id: str @lru_cache() def get_settings() -\u0026gt; Settings: \u0026#34;\u0026#34;\u0026#34; Load settings (once per app lifetime) \u0026#34;\u0026#34;\u0026#34; return Settings() def get_jwks_url(settings: Settings = Depends(get_settings)) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Build JWKS url \u0026#34;\u0026#34;\u0026#34; pool_id = settings.cognito_user_pool_id region = pool_id.split(\u0026#34;_\u0026#34;)[0] return f\u0026#34;https://cognito-idp.{region}.amazonaws.com/{pool_id}/.well-known/jwks.json\u0026#34; @cached(TTLCache(maxsize=1, ttl=3600)) def get_jwks(url: str = Depends(get_jwks_url)) -\u0026gt; KeySet: \u0026#34;\u0026#34;\u0026#34; Get cached or new JWKS. Cognito does not seem to rotate keys, however to be safe we are lazy-loading new credentials every hour. \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Fetching JWKS from %s\u0026#34;, url) with requests.get(url) as response: response.raise_for_status() return JsonWebKey.import_key_set(response.json()) def decode_token( token: security.HTTPAuthorizationCredentials = Depends(token_scheme), jwks: KeySet = Depends(get_jwks), ) -\u0026gt; JWTClaims: \u0026#34;\u0026#34;\u0026#34; Validate \u0026amp; decode JWT. \u0026#34;\u0026#34;\u0026#34; try: claims = JsonWebToken([\u0026#34;RS256\u0026#34;]).decode( s=token.credentials, key=jwks, claim_options={ # Example of validating audience to match expected value # \u0026#34;aud\u0026#34;: {\u0026#34;essential\u0026#34;: True, \u0026#34;values\u0026#34;: [APP_CLIENT_ID]} } ) if \u0026#34;client_id\u0026#34; in claims: # Insert Cognito\u0026#39;s `client_id` into `aud` claim if `aud` claim is unset claims.setdefault(\u0026#34;aud\u0026#34;, claims[\u0026#34;client_id\u0026#34;]) claims.validate() except errors.JoseError: logger.exception(\u0026#34;Unable to decode token\u0026#34;) raise HTTPException(status_code=403, detail=\u0026#34;Bad auth token\u0026#34;) return claims app = FastAPI() @app.get(\u0026#34;/who-am-i\u0026#34;) def who_am_i(claims=Depends(decode_token)) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Return claims for the provided JWT \u0026#34;\u0026#34;\u0026#34; return claims @app.get(\u0026#34;/auth-test\u0026#34;, dependencies=[Depends(decode_token)]) def auth_test() -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Require auth but not use it as a dependency \u0026#34;\u0026#34;\u0026#34; return True ","permalink":"https://alukach.com/posts/fastapi-rs256-jwt/","summary":"This post is a quick capture of how to easily secure your FastAPI with any auth provider that provides JWKS.\nBackground: RS256 RS256 is a signing algorithm used to generate and validate JSON Web Tokens (JWTs). Unlike the common HS256 algorithm that uses the same secret string to both generate and validate JWTs, RS256 uses a private key to generate JWTs and a separate public key for validating JWTs:\nRS256 generates an asymmetric signature, which means a private key must be used to sign the JWT and a different public key must be used to verify the signature.","title":"Securing FastAPI with JWKS (AWS Cognito, Auth0)"},{"content":"Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.\nWhy is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:\nIt is reliant on long-standing credentials being stored in Github Actions. Some environments are unable to generate such long-standing credentials without serious admin intervention (e.g. environments using CloudTamer/Kion).\nIt grants any user with write-access to the repo with full use of the possibly wide-scoped credentials. Currently, within Github there are not a sufficient ways to limit who or what can be done with the credentials. For example, any user with write-access to the repo could create a workflow action that references the production credentials and uses them to teardown the production environment. This is due to the combination of two factors:\nThe instructions run during the build are entirely specified within the Github repo. This means that anyone can alter them as they see fit. The cloud providers lacks information about the context of a build (e.g. git branch or github user), and is therefore unable to apply or enforce any sort of restrictions regarding what a build can do. Github Environments solves some of these problems, however at time of writing it is only available on public repositories or private Github Enterprise account repositories, making it an unvialable solution for many of our partners.\nSolution: OpenID Connect On Nov 23, 2021 Github Actions announced the general availability of support for OpenID Connect (OIDC). For an in-depth understanding of this, I recommend reviewing the following links:\nAnnouncement: Secure deployments with OpenID Connect \u0026amp; GitHub Actions now generally available Docs: Security hardening your deployments [with OpenID Connect] High level summary With OIDC, you can register Github as an Identity Provider within your cloud platform of choice. When your Github Action workflows run, they can be setup to request short-lived credentials from your cloud provider. When the cloud provider grants the access token, it will be associated with a particular IAM Role. That IAM Role should be set up with the permissions necessary for deploying your application.\nNo need to store credentials. Github Actions workflows will request a short-lived access token at runtime. When a short-lived access token is requested, Github Actions sends an OIDC token with claims describing the context of the workflow (link). These claims can be interogated by the cloud provider and used to determine whether or not a token should be granted. This allows us to hardcode security requirements (e.g. limiting particular IAM Roles to specific Github branches, only allowing executions triggered by specified github usernames) in the cloud provider, providing guard-rails to limit what can be done by any particular user with write-access to a Github repository. Example with AWS Setup AWS Setting up OIDC with AWS is described in depth here, however the following is a quick summary:\nAdd Github as an Identity provider (docs).\nConsole Screenshot Create an IAM Policy for deployment executions. See recommendations for tips on how to craft this policy. Below is an example policy for frontend static website deployments:\nExample policy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;SyncS3Bucket\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-staging-bucket\u0026#34;, \u0026#34;arn:aws:s3:::my-staging-bucket/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;InvalidateCloudfrontDistribution\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudfront:CreateInvalidation\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create role for deployment executions.\nConsole Screenshot Attach your revelant policies. Optionally, specify the role\u0026rsquo;s permission boundaries.\nConsole Screenshot For this example, we\u0026rsquo;ll be naming our role Frontend-Staging-Deployment-Role\nConsole Screenshot By default, the IAM Role created for our OIDC Web Identity contains a condition where the aud claim in our token should match sts.amazonaws.com. However, it is here that we will enforce more strict conditions. The sub claim in our token contains information about the reposoitory and branch. As such, we need to customize our trust relationship to encorce our custom conditions.\nConsole Screenshot In the following example, we configure the trust relationship to enforce that this role can only be used on builds on the my-org/my-repo repository\u0026rsquo;s staging branch:\nExample conditions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::123456789000:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:my-org/my-repo:ref:refs/heads/staging\u0026#34; } } } ] } Setup GitHub Actions Workflow Workflows utilizing OIDC need a few particular elements.\nWe need to customize the permissions of our GITHUB_TOKEN via the permissions block. The workflow will need to be able to write an id-token along with the default permissions of reading the contents of the repository:\nPermissions block 1 2 3 permissions: id-token: write contents: read Add tooling to request the an access token from AWS. For this, the AWS\u0026rsquo; offical \u0026ldquo;Configure AWS Credentials\u0026rdquo; Action works well. To use this, you must provide the ARN of the role that you would like to assume in your execution:\nAWS Configuration step 1 2 3 4 5 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::123456789000:role/Frontend-Staging-Deployment-Role aws-region: us-west-2 Full example A complete example workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 name: Deploy Staging Frontend on: push: branches: - staging permissions: id-token: write contents: read jobs: build: runs-on: ubuntu-latest steps: - name: Setup Node.js uses: actions/setup-node@v2 with: node-version: 12 - name: Check out repository code uses: actions/checkout@v2 - name: Install dependencies run: npm install - name: Build code run: npm run build - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::123456789000:role/Frontend-Staging-Deployment-Role aws-region: us-west-2 - name: Sync with S3 bucket env: BUCKET: my-staging-bucket run: | aws s3 sync \\ ./build \u0026#34;s3://${BUCKET}\u0026#34; \\ --acl public-read \\ --follow-symlinks \\ --delete - name: Invalidate CloudFront env: DISTRIBUTION: EDFDVBD6EXAMPLE run: | aws cloudfront create-invalidation \\ --distribution-id $DISTRIBUTION \\ --paths \u0026#34;/*\u0026#34; In the above example, we have hardcoded the Role ARN, S3 Bucket, and Cloudfront Distribution ID in the workflow file. However, you may prefer to store these values as Github Secrets. This allows the values to be changed without a code change and additionally helps avoid data-leak. An example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: ${{ secrets.STAGING_CD_ROLE_ARN }} aws-region: us-west-2 - name: Sync with S3 bucket env: BUCKET: ${{ secrets.STAGING_BUCKET_NAME }} run: | aws s3 sync \\ ./build \u0026#34;s3://${BUCKET}\u0026#34; \\ --acl public-read \\ --follow-symlinks \\ --delete - name: Invalidate CloudFront env: DISTRIBUTION: ${{ secrets.STAGING_DISTRIBUTION_ID }} run: | aws cloudfront create-invalidation \\ --distribution-id $DISTRIBUTION \\ --paths \u0026#34;/*\u0026#34; Recommendations Each IAM role should relate to a single deployment. For example, you may have a Service-X-Frontend-Staging-Deployment role and a Service-X-Frontend-Production-Deployment role, each referencing specific IAM policies that specify the minimal permissions needed to deploy to its respective environment. Each role should specify which repositories and branches can use the role. Configuring the IAM role\u0026rsquo;s trust relationship is key to enforcing logic around deployment permissions. Understanding the Condition block and the Github OIDC token is paramount. ","permalink":"https://alukach.com/posts/oidc-github-actions/","summary":"Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.\nWhy is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:","title":"Security-conscious cloud deployments from Github Actions via OpenID Connect"},{"content":"Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.\nOther Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.\nBackground: Project Infrastructure Our production frontend is a React application (using Next.js). We build this application into static HTML/CSS/JS files and upload them to an S3 bucket. This bucket has been setup to serve static websites and is served via HTTPS by CloudFront (see #270).\nThe frontend accesses multiple backend APIs (e.g. a STAC API, a FastAPI REST API). Deployment of those APIs is outside of the scope of the frontend codebase.\nCI: Cloud Infrastructure Our goal is to mimic the production frontend deployment to a reasonable degree.\n1. Setup a bucket First, we will need an S3 bucket to store our builds. We created a bucket (s3://project-frontend-ci) and configured it to serve static websites. As a sanity check, we wrote a simple message to a public file that we place at the root of the bucket:\n1 echo \u0026#34;Project frontend CI builds\u0026#34; | aws s3 cp - s3://project-frontend-ci/index.html --acl public-read --content-type text/html We can verify that everything is configured properly by visiting the bucket\u0026rsquo;s website url: http://project-frontend-ci.s3-website.us-east-1.amazonaws.com/\n2. Setup SSL via API Gateway Unfortunately, S3 does not support serving content via SSL (i.e. over HTTPS). As such, we need to use something like API Gateway or CloudFront to accept traffic over HTTPS and to route it to our bucket\u0026rsquo;s content over HTTP. For the sake of simplicity, we chose to use API Gateway for this purpose. For our actual staging environment, we utilize CloudFront to more closely mimic the production environment, however for these temporary CI builds we felt that API Gateway was close-enough.\nWe created an API Gateway HTTP API configured to direct all traffic to our S3 bucket\u0026rsquo;s website URL: We can now visit our bucket over SSL via the API Gateway endpoint: https://0zy9z5ko27.execute-api.us-east-1.amazonaws.com/\n3. Setup a URL for the CI builds The downside of API Gateway or CloudFront is that it produces very unmemorable URLs. Just to be a bit fancy 💅 , we set up a custom URL on domain.com. We settled on ci.project-staging.domain.com to pair nicely with our staging environment (project-staging.domain.com).\nSetting this up is a bit of a multi-step process. Click here to see the details... a. Create SSL Certificate On the AWS account owns the API Gateway HTTP API we just setup, we created an SSL Certificate via AWS Certificate Manager (ACM):\nb. Verify ownership of domain ACM requires that you verify that you have control of a domain before it will grant you an SSL certificate. After creating an SSL certificate, you\u0026rsquo;ll see that it is in \u0026ldquo;Pending Validation\u0026rdquo; status.\nTo verify that we control domain.com, we add a CNAME record to the domain.com hosted zone. Once this is done, we frantically refresh the ACM status page until it states that our domain has been verified.\nc. Setup API Gateway custom domain Back over to API Gateway, we set up a custom domain.\nAfter creating the custom domain, we add an API mapping to our HTTP API.\nd. Creating a DNS entry for our new URL We now want to instruct Route53 to direct all traffic sent to our URL (ci.project-staging.domain.com) to our new API Gateway custom domain. To do this, we copy the API Gateway domain name.\nWe use the copied API Gateway domain name to create a new DNS entry to facilitate this mapping:\nAfter this, we should be able to see our sanity-check message at https://ci.project-staging.domain.com.\nCI: Workflows Building \u0026amp; Deploying Our goal is to build a version of our frontend application for every pull request and have it available at https://ci.project-staging.domain.com. Our chosen strategy was to build each PR and to place the build in a path prefixed with the PR number (i.e. PR 208 should be available at https://ci.project-staging.domain.com/208). To facilitate this, your frontend application must be configured to allow it to be served from non-route paths. In the case of NextJS, this is done via the Base Path configuration.\nBuilding and Deploying the application via Github actions is pretty straightforward.\nExample of a simple build/deploy Github Actions workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 name: Deploy to CI environment on: pull_request: jobs: build-and-deploy: runs-on: ubuntu-latest steps: - name: Cancel Previous Runs uses: styfle/cancel-workflow-action@0.8.0 with: access_token: ${{ github.token }} - name: Checkout uses: actions/checkout@v2 - name: Use Node.js 14 uses: actions/setup-node@v1 with: node-version: 14 - name: Cache node modules uses: actions/cache@v2 env: cache-name: cache-node-modules with: path: node_modules key: ${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles(\u0026#39;**/yarn.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-build-${{ env.cache-name }}- ${{ runner.os }}-build- ${{ runner.os }}- - name: Build and Export id: build env: NEXT_PUBLIC_BASE_URL: https://ci.project-staging.domain.com/${{ github.event.pull_request.number }} NEXT_PUBLIC_STAC_API: ${{ \u0026#39;https://project-staging.domain.com/stac\u0026#39; }} NEXT_PUBLIC_ORDERS_API: ${{ \u0026#39;https://project-staging.domain.com/api\u0026#39; }} run: | yarn install yarn build yarn run next export - name: Configure AWS credentials from staging account uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }} aws-region: us-east-1 - name: Deploy 🚀 run: | aws s3 sync \\ ./out \\ s3://project-frontend-ci/${{ github.event.pull_request.number }} \\ --delete \\ --acl public-read You can see that we pass in our Base URL and external APIs via the env at build time and that we have our AWS credentials available as encrypted secrets.\nNote that, as per the Github docs, the pull_request event only triggers when a PR is opened, updated, or re-opened:\nBy default, a workflow only runs when a pull_request\u0026rsquo;s activity type is opened, synchronize, or reopened.\nAdding a comment on our PR to notify others of the build Once the CI has built and deployed a new instance of our frontend, we want to notify others (e.g. those reviewing PRs) where they can view the build. To do this, we add the following steps to our Github Actions workflow to take place after our build:\nExample of jobs to add comments to a PR 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 jobs: build-and-deploy: steps: # ... - name: Get current time uses: gerred/actions/current-time@master id: current-time - name: Find Comment uses: peter-evans/find-comment@v1 id: find-comment with: issue-number: ${{ github.event.pull_request.number }} comment-author: \u0026#34;github-actions[bot]\u0026#34; body-includes: Latest commit deployed to - name: Create or update comment uses: peter-evans/create-or-update-comment@v1 with: comment-id: ${{ steps.find-comment.outputs.comment-id }} issue-number: ${{ github.event.pull_request.number }} body: | 🚀 Latest commit deployed to https://ci.project-staging.domain.com/${{ github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` * Commit: ${{ github.sha }} (Merging ${{ github.event.pull_request.head.sha }} into ${{ github.event.pull_request.base.sha }}) edit-mode: replace These steps add a new comment to PRs, looking something like this:\nFor later commits to the PR, the original comment will be replaced rather than creating another comment. This helps us avoid littering user\u0026rsquo;s notifications and keeps a clean comment thread.\nAllow users to customize configuration As previously mentioned, the frontend connects to multiple backing APIs. By default, the CI builds point to our staging APIs. However, it\u0026rsquo;s a realistic scenario that a developer would want their custom environment to point to a different API release (e.g. a developer is working on frontend changes in tandem with changes being made to the backend API). To support this, we want to allow developers to manually override certain configurations. To do this, we added the workflow_dispatch trigger to our workflow, allowing for manual workflow runs. We also add inputs for each configuration we want to allow a developer to specify.\nExample of adding workflow_dispatch event with inputs to your workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 name: Deploy to CI environment on: pull_request: workflow_dispatch: inputs: stac-api-url: description: Override STAC API URL default: https://project-staging.domain.com/stac orders-api-url: description: Override Orders API URL default: https://project-staging.domain.com/api deployment-id: description: Unique identifier for build (used to construct path for upload) required: true jobs: build-and-deploy: runs-on: ubuntu-latest steps: # ... - name: Build and Export id: build env: NEXT_PUBLIC_BASE_URL: https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} NEXT_PUBLIC_STAC_API: ${{ github.event.inputs.stac-api-url || \u0026#39;https://project-staging.domain.com/stac\u0026#39; }} NEXT_PUBLIC_ORDERS_API: ${{ github.event.inputs.orders-api-url || \u0026#39;https://project-staging.domain.com/api\u0026#39; }} NEXT_PUBLIC_MB_TOKEN: pk.eyJ1IjoiZGV2c2VlZCIsImEiOiJjazB6YXU2bDUwMWNkM2VvNGNpMnFhOXMxIn0.c30a2TQIfCDF3GlqMdSQ_g NEXT_PUBLIC_GA_ID: GTM-WNP7MLF run: | yarn install yarn build yarn run next export - name: Get current time uses: gerred/actions/current-time@master if: ${{ github.event.pull_request.number }} # ... - name: Find Comment uses: peter-evans/find-comment@v1 if: ${{ github.event.pull_request.number }} # ... - name: Create or update comment uses: peter-evans/create-or-update-comment@v1 if: ${{ github.event.pull_request.number }} # ... You\u0026rsquo;ll note that any place where we originally specified our API configuration or had a dependency on a PR number, we now first try to retrieve the value from github.event.inputs (only available during manual workflow_dispatch events) and otherwise fall back to values used during standard PR builds. This can be achieved by utilizing the OR operator as so: ${{ github.event.inputs.deployment-id || github.event.pull_request.number }}.\nAdditionally, we will only want to comment on a PR during PR builds, so we avoid running the comment steps by adding an if: ${{ github.event.pull_request.number }} clause to each step that we want to skip.\nCleanup After each PR is merged, we want to clean up the past build to avoid unnecessary storage in our CI bucket. This can be achieved with another workflow that cleans up the build whenever a pull request is closed.\nExample of a workflow to destroy CI builds 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 name: Destroy PR Preview on: pull_request: types: [closed] workflow_dispatch: inputs: deployment-id: description: Unique identifier of CI build to be deleted required: true jobs: build-and-deploy: runs-on: ubuntu-latest steps: # ... - name: Destroy 💣 run: | aws s3 rm --recursive s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}/ - name: Get current time uses: gerred/actions/current-time@master if: ${{ github.event.pull_request.number }} id: current-time - name: Find Comment uses: peter-evans/find-comment@v1 if: ${{ github.event.pull_request.number }} id: find-comment with: issue-number: ${{ github.event.pull_request.number }} comment-author: \u0026#34;github-actions[bot]\u0026#34; body-includes: Latest commit deployed to - name: Create or update comment uses: peter-evans/create-or-update-comment@v1 if: ${{ github.event.pull_request.number }} with: comment-id: ${{ steps.find-comment.outputs.comment-id }} issue-number: ${{ github.event.pull_request.number }} body: | --- 🧹 Deleted build at https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` edit-mode: append Our pull request message is then appended with information to let others know that the build environment is no longer available.\nPutting it all together To achieve our goals of deployment, notification, customization, and cleanup, we have settled on these two Github workflows:\n.github/workflows/deploy-pr-preview.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 name: Deploy to CI environment on: pull_request: workflow_dispatch: inputs: stac-api-url: description: Override STAC API URL default: https://project-staging.domain.com/stac orders-api-url: description: Override Orders API URL default: https://project-staging.domain.com/api deployment-id: description: Unique identifier for build (used to construct path for upload) required: true jobs: build-and-deploy: runs-on: ubuntu-latest steps: - name: Cancel Previous Runs uses: styfle/cancel-workflow-action@0.8.0 with: access_token: ${{ github.token }} - name: Checkout uses: actions/checkout@v2 - name: Use Node.js 14 uses: actions/setup-node@v1 with: node-version: 14 - name: Cache node modules uses: actions/cache@v2 env: cache-name: cache-node-modules with: path: node_modules key: ${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles(\u0026#39;**/yarn.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-build-${{ env.cache-name }}- ${{ runner.os }}-build- ${{ runner.os }}- - name: Build and Export id: build env: NEXT_PUBLIC_BASE_URL: https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} NEXT_PUBLIC_STAC_API: ${{ github.event.inputs.stac-api-url || \u0026#39;https://project-staging.domain.com/stac\u0026#39; }} NEXT_PUBLIC_ORDERS_API: ${{ github.event.inputs.orders-api-url || \u0026#39;https://project-staging.domain.com/api\u0026#39; }} NEXT_PUBLIC_MB_TOKEN: pk.eyJ1IjoiZGV2c2VlZCIsImEiOiJjazB6YXU2bDUwMWNkM2VvNGNpMnFhOXMxIn0.c30a2TQIfCDF3GlqMdSQ_g NEXT_PUBLIC_GA_ID: GTM-WNP7MLF run: | yarn install yarn build yarn run next export - name: Configure AWS credentials from staging account uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }} aws-region: us-east-1 - name: Deploy 🚀 run: | aws s3 sync \\ ./out \\ s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} \\ --delete \\ --acl public-read - name: Get current time uses: gerred/actions/current-time@master if: ${{ github.event.pull_request.number }} id: current-time - name: Find Comment uses: peter-evans/find-comment@v1 if: ${{ github.event.pull_request.number }} id: find-comment with: issue-number: ${{ github.event.pull_request.number }} comment-author: \u0026#34;github-actions[bot]\u0026#34; body-includes: Latest commit deployed to - name: Create or update comment uses: peter-evans/create-or-update-comment@v1 if: ${{ github.event.pull_request.number }} with: comment-id: ${{ steps.find-comment.outputs.comment-id }} issue-number: ${{ github.event.pull_request.number }} body: | 🚀 Latest commit deployed to https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` * Commit: ${{ github.sha }} (merging ${{ github.event.pull_request.head.sha }} into ${{ github.event.pull_request.base.sha }}) edit-mode: replace .github/workflows/destroy-pr-preview.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 name: Destroy PR Preview on: pull_request: types: [closed] workflow_dispatch: inputs: deployment-id: description: Unique identifier of CI build to be deleted required: true jobs: build-and-deploy: runs-on: ubuntu-latest steps: - name: Cancel Previous Runs uses: styfle/cancel-workflow-action@0.8.0 with: access_token: ${{ github.token }} - name: Configure AWS credentials from staging account uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.STAGING_AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.STAGING_AWS_SECRET_ACCESS_KEY }} aws-region: us-east-1 - name: Destroy 💣 run: | aws s3 rm --recursive s3://project-frontend-ci/${{ github.event.inputs.deployment-id || github.event.pull_request.number }}/ - name: Get current time uses: gerred/actions/current-time@master if: ${{ github.event.pull_request.number }} id: current-time - name: Find Comment uses: peter-evans/find-comment@v1 if: ${{ github.event.pull_request.number }} id: find-comment with: issue-number: ${{ github.event.pull_request.number }} comment-author: \u0026#34;github-actions[bot]\u0026#34; body-includes: Latest commit deployed to - name: Create or update comment uses: peter-evans/create-or-update-comment@v1 if: ${{ github.event.pull_request.number }} with: comment-id: ${{ steps.find-comment.outputs.comment-id }} issue-number: ${{ github.event.pull_request.number }} body: | --- 🧹 Deleted build at https://ci.project-staging.domain.com/${{ github.event.inputs.deployment-id || github.event.pull_request.number }} * Date: `${{ steps.current-time.outputs.time }}` edit-mode: append This system is very new for us and required some additional changes to other services (i.e. updating CORS rules on our APIs to allow us to use this new URL), but so far it seems to be operating as expected. Hoping that this can help others who are looking to build out better CI preview environments for their applications.\nFollowup this is super cool but was there a reason we couldn’t use netlify or another third party service for this?\nThis is a fair question. We opted to roll our own solution being that the general idea (uploading builds to S3) was something that we were already doing for deployments to our Production and Staging environments. However, if you\u0026rsquo;re starting a new project, you may be interested in achieving per-PR deployments with a third party tool. It appears that most common hosting solutions offer something for this:\nNetlify offers Deploy Previews Vercel offers Preview URLs via its Github Integration AWS Amplify offers Web Previews Surge.sh can be configured to preview URLs via the surge-preview Github Action ","permalink":"https://alukach.com/posts/diy-pr-previews/","summary":"Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.\nOther Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.","title":"Roll your own PR preview CI pipeline"},{"content":"Have you just written a new ✨fancy CLI✨ and want to demo it in your Github Readme? Recording your terminal output is a nice way to demonstrate the experience.\nHere\u0026rsquo;s an example of what we\u0026rsquo;re going to make:\nSteps Install Dependencies asciinema: brew install asciinema svg-term-cli: npm install -g svg-term-cli Setup your terminal Some tips:\nFont/screen size matters. The asciinema output will look just as it does in your terminal. You\u0026rsquo;ll probably want to bump up the font-size and shrink down the terminal so that the text is legible in your README. Choose a nice, generic terminal theme. If using ohmyzsh, I recommend the arrow theme. Lights, camera, action 🎬 The asciinema rec command will start a process to capture the stdout of your terminal session. When you\u0026rsquo;re done, pressing ctrl+d will end recording. You can then opt to save it to the asciinema server or locally. For our needs, we\u0026rsquo;ll save locally.\nPost Production ✂️🎞 asciinema will place the file in a temporary directory. The output \u0026ldquo;cast\u0026rdquo; is a simple newline-delimeted JSON file, which a savvy user could edit by hand to scrub out any mistakes or personal information.\nexample asciinema cast 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 {\u0026#34;version\u0026#34;: 2, \u0026#34;width\u0026#34;: 82, \u0026#34;height\u0026#34;: 20, \u0026#34;timestamp\u0026#34;: 1632512435, \u0026#34;env\u0026#34;: {\u0026#34;SHELL\u0026#34;: \u0026#34;/bin/zsh\u0026#34;, \u0026#34;TERM\u0026#34;: \u0026#34;xterm-256color\u0026#34;}} [0.3937, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [0.401958, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [0.402028, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [1.397944, \u0026#34;o\u0026#34;, \u0026#34;asciinema rec\u0026#34;] [2.038988, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\u0026#34;] [2.039271, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [2.040939, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;asciinema rec\\u0007\\u001b]1;asciinema\\u0007\u0026#34;] [2.12754, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: recording asciicast to /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[0m\\r\\n\\u001b[0;32masciinema: press \u0026lt;ctrl-d\u0026gt; or type \\\u0026#34;exit\\\u0026#34; when you\u0026#39;re done\\u001b[0m\\r\\n\u0026#34;] [2.531341, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [2.539224, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [2.539294, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [3.437282, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [3.573817, \u0026#34;o\u0026#34;, \u0026#34;\\bpy\u0026#34;] [3.751574, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [3.813398, \u0026#34;o\u0026#34;, \u0026#34;h\u0026#34;] [3.893245, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [3.993249, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [4.091641, \u0026#34;o\u0026#34;, \u0026#34;3\u0026#34;] [4.227577, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [4.631549, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [4.759173, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [4.858853, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [4.992652, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [5.028749, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00mpython3 /tmp\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[63C\\u001b[33m \\u001b[00m\\u001b[64D\u0026#34;] [5.257958, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/e\u0026#34;] [5.436696, \u0026#34;o\u0026#34;, \u0026#34;x\u0026#34;] [5.7675, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [5.776614, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00mpython3 /tmp/example.py\\u001b[1m \\u001b[0m\\u001b[K\\u001b[52C\\u001b[33m \\u001b[00m\\u001b[53D\u0026#34;] [6.590538, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m \\b\u0026#34;] [6.590824, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\\u001b[?2004l\u0026#34;] [6.590975, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\n\u0026#34;] [6.592687, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;python3 /tmp/example.py\\u0007\\u001b]1;python3\\u0007\u0026#34;] [6.656795, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\u0026#34;] [6.656895, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.656981, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\u0026#34;] [6.65715, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.657262, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\u0026#34;] [6.657347, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.657453, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.657583, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r#4, est. 0.83s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.657683, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.657779, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\u0026#34;] [6.657931, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658118, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658213, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658368, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658434, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.658526, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.658623, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 0%| | 0/5000 [00:00\u0026lt;?, ?it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.757006, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.757143, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 13%|███ | 647/5000 [00:00\u0026lt;00:00, 6468.12it/s]\\u001b[A\u0026#34;] [6.757228, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 7%|█▌ | 334/5000 [00:00\u0026lt;00:01, 3327.45it/s]\u0026#34;] [6.757531, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.757582, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 5%|█▏ | 254/5000 [00:00\u0026lt;00:01, 2533.69it/s]\\u001b[A\\u001b[A\u0026#34;] [6.757652, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.757714, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 10%|██▎ | 492/5000 [00:00\u0026lt;00:00, 4915.24it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.75778, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.75784, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 3%|▊ | 169/5000 [00:00\u0026lt;00:02, 1683.67it/s]\u0026#34;] [6.757884, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758049, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758097, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 11%|██▋ | 567/5000 [00:00\u0026lt;00:00, 5665.80it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758387, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758455, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 8%|█▉ | 409/5000 [00:00\u0026lt;00:01, 4082.16it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758509, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758555, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 14%|███▍ | 716/5000 [00:00\u0026lt;00:00, 7159.09it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.758726, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.758759, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 16%|███▊ | 790/5000 [00:00\u0026lt;00:00, 7890.77it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.857141, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.85724, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 26%|█████▉ | 1302/5000 [00:00\u0026lt;00:00, 6511.76it/s]\\u001b[A\u0026#34;] [6.857278, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 13%|███▏ | 674/5000 [00:00\u0026lt;00:01, 3368.38it/s]\u0026#34;] [6.85771, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.857762, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 20%|████▋ | 986/5000 [00:00\u0026lt;00:00, 4928.32it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858337, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858396, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 23%|█████▏ | 1134/5000 [00:00\u0026lt;00:00, 5658.36it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858422, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858496, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 29%|██████▌ | 1434/5000 [00:00\u0026lt;00:00, 7170.93it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858554, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858584, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 16%|███▉ | 819/5000 [00:00\u0026lt;00:01, 4089.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858642, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.858674, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 7%|█▌ | 338/5000 [00:00\u0026lt;00:02, 1679.15it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.858728, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.858775, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 10%|██▍ | 508/5000 [00:00\u0026lt;00:01, 2519.60it/s]\\u001b[A\\u001b[A\u0026#34;] [6.858828, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.85887, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 32%|███████▎ | 1585/5000 [00:00\u0026lt;00:00, 7921.04it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.957251, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [6.957276, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 39%|████████▉ | 1954/5000 [00:00\u0026lt;00:00, 6513.12it/s]\\u001b[A\u0026#34;] [6.957367, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 20%|████▋ | 1012/5000 [00:00\u0026lt;00:01, 3372.64it/s]\u0026#34;] [6.958144, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958194, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 30%|██████▊ | 1479/5000 [00:00\u0026lt;00:00, 4919.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958383, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958432, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 34%|███████▊ | 1703/5000 [00:00\u0026lt;00:00, 5671.37it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958528, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958576, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 43%|█████████▉ | 2161/5000 [00:00\u0026lt;00:00, 7212.74it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958631, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.95866, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 25%|█████▋ | 1233/5000 [00:00\u0026lt;00:00, 4110.89it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.958857, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [6.958912, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 48%|██████████▉ | 2384/5000 [00:00\u0026lt;00:00, 7950.66it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.959242, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [6.959296, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 10%|██▍ | 506/5000 [00:00\u0026lt;00:02, 1674.81it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [6.959352, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [6.959393, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 15%|███▋ | 760/5000 [00:00\u0026lt;00:01, 2512.95it/s]\\u001b[A\\u001b[A\u0026#34;] [7.057416, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.057472, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 52%|███████████▉ | 2608/5000 [00:00\u0026lt;00:00, 6519.76it/s]\\u001b[A\u0026#34;] [7.057578, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 27%|██████▏ | 1351/5000 [00:00\u0026lt;00:01, 3376.65it/s]\u0026#34;] [7.058204, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058246, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 39%|█████████ | 1974/5000 [00:00\u0026lt;00:00, 4930.20it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058631, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058679, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 58%|█████████████▎ | 2890/5000 [00:00\u0026lt;00:00, 7240.83it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058766, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058825, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 45%|██████████▍ | 2271/5000 [00:00\u0026lt;00:00, 5666.41it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058875, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 33%|███████▌ | 1647/5000 [00:00\u0026lt;00:00, 4118.91it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.058936, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.058983, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 64%|██████████████▋ | 3182/5000 [00:00\u0026lt;00:00, 7961.79it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.059486, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.059523, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 20%|████▋ | 1012/5000 [00:00\u0026lt;00:01, 2514.31it/s]\\u001b[A\\u001b[A\u0026#34;] [7.059677, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.059715, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 14%|███▏ | 676/5000 [00:00\u0026lt;00:02, 1681.78it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.157472, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.157542, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 65%|███████████████ | 3266/5000 [00:00\u0026lt;00:00, 6538.76it/s]\\u001b[A\u0026#34;] [7.157691, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 34%|███████▊ | 1689/5000 [00:00\u0026lt;00:00, 3376.30it/s]\u0026#34;] [7.158976, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159029, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 49%|███████████▎ | 2468/5000 [00:00\u0026lt;00:00, 4919.98it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159074, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159127, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 41%|█████████▍ | 2061/5000 [00:00\u0026lt;00:00, 4123.10it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159181, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159211, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 57%|█████████████ | 2838/5000 [00:00\u0026lt;00:00, 5659.82it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159252, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159291, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 80%|██████████████████▎ | 3979/5000 [00:00\u0026lt;00:00, 7954.54it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159639, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159686, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 72%|████████████████▋ | 3615/5000 [00:00\u0026lt;00:00, 7217.32it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.159745, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.159798, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 25%|█████▊ | 1264/5000 [00:00\u0026lt;00:01, 2513.88it/s]\\u001b[A\\u001b[A\u0026#34;] [7.159854, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.159911, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 17%|████ | 845/5000 [00:00\u0026lt;00:02, 1683.62it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.25807, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.258125, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 78%|██████████████████ | 3920/5000 [00:00\u0026lt;00:00, 6526.80it/s]\\u001b[A\u0026#34;] [7.25849, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 41%|█████████▎ | 2027/5000 [00:00\u0026lt;00:00, 3368.46it/s]\u0026#34;] [7.259193, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259222, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 59%|█████████████▋ | 2962/5000 [00:00\u0026lt;00:00, 4923.60it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259282, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259315, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 68%|███████████████▋ | 3404/5000 [00:00\u0026lt;00:00, 5657.50it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259372, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259421, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 96%|█████████████████████▉ | 4775/5000 [00:00\u0026lt;00:00, 7952.80it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259472, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.259508, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 49%|███████████▍ | 2474/5000 [00:00\u0026lt;00:00, 4120.16it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.259866, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.259917, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 30%|██████▉ | 1516/5000 [00:00\u0026lt;00:01, 2514.82it/s]\\u001b[A\\u001b[A\u0026#34;] [7.259962, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.260017, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 87%|███████████████████▉ | 4337/5000 [00:00\u0026lt;00:00, 7210.60it/s]\u0026#34;] [7.26007, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.260299, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.260351, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 20%|████▋ | 1014/5000 [00:00\u0026lt;00:02, 1683.28it/s]\u0026#34;] [7.260394, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.288047, \u0026#34;o\u0026#34;, \u0026#34;\\r#8, est. 0.50s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 7944.93it/s]\\r\\n\u0026#34;] [7.353676, \u0026#34;o\u0026#34;, \u0026#34;\\r#7, est. 0.56s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 7192.40it/s]\\r\\n\u0026#34;] [7.359093, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 47%|██████████▊ | 2364/5000 [00:00\u0026lt;00:00, 3363.69it/s]\u0026#34;] [7.359563, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\u0026#34;] [7.359669, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 91%|█████████████████████ | 4573/5000 [00:00\u0026lt;00:00, 6494.58it/s]\\u001b[A\u0026#34;] [7.359716, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.359768, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 58%|█████████████▎ | 2887/5000 [00:00\u0026lt;00:00, 4120.15it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.359776, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.359835, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 79%|██████████████████▎ | 3970/5000 [00:00\u0026lt;00:00, 5648.96it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.360318, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.360379, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 35%|████████▏ | 1768/5000 [00:00\u0026lt;00:01, 2512.84it/s]\\u001b[A\\u001b[A\u0026#34;] [7.360572, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.360613, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 24%|█████▍ | 1183/5000 [00:00\u0026lt;00:02, 1683.97it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.361952, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.362002, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 69%|███████████████▉ | 3455/5000 [00:00\u0026lt;00:00, 4881.44it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.427585, \u0026#34;o\u0026#34;, \u0026#34;\\r#6, est. 0.62s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 6488.86it/s]\\r\\n\u0026#34;] [7.459805, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.459951, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 66%|███████████████▎ | 3322/5000 [00:00\u0026lt;00:00, 4192.55it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r#5, est. 0.71s: 91%|████████████████████▉ | 4557/5000 [00:00\u0026lt;00:00, 5717.60it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.460767, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r#1, est. 1.70s: 40%|█████████▎ | 2021/5000 [00:00\u0026lt;00:01, 2515.40it/s]\\u001b[A\\u001b[A\u0026#34;] [7.462941, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.463183, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 27%|██████▏ | 1352/5000 [00:00\u0026lt;00:02, 1674.75it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.464172, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r#4, est. 0.83s: 79%|██████████████████▏ | 3944/5000 [00:00\u0026lt;00:00, 4851.69it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.464692, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 54%|████████████▍ | 2701/5000 [00:00\u0026lt;00:00, 3306.24it/s]\u0026#34;] [7.547702, \u0026#34;o\u0026#34;, \u0026#34;\\r#5, est. 0.71s: 100%|███████████████████████| 5000/5000 [00:00\u0026lt;00:00, 5620.61it/s]\\r\\n\u0026#34;] [7.566987, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.567052, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 45%|██████████▍ | 2273/5000 [00:00\u0026lt;00:01, 2468.99it/s]\\u001b[A\\u001b[A\u0026#34;] [7.567509, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.567547, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 30%|██████▉ | 1520/5000 [00:00\u0026lt;00:02, 1651.79it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.568657, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.568709, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 75%|█████████████████▏ | 3742/5000 [00:00\u0026lt;00:00, 4081.31it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.570132, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.570181, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 89%|████████████████████▍ | 4430/5000 [00:00\u0026lt;00:00, 4765.18it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.570768, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 61%|█████████████▉ | 3032/5000 [00:00\u0026lt;00:00, 3245.98it/s]\u0026#34;] [7.668, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.668081, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 34%|███████▊ | 1689/5000 [00:01\u0026lt;00:01, 1661.05it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.668858, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.668916, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 83%|███████████████████▏ | 4170/5000 [00:01\u0026lt;00:00, 4138.65it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.669846, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.669918, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 50%|███████████▌ | 2521/5000 [00:01\u0026lt;00:01, 2450.60it/s]\\u001b[A\\u001b[A\u0026#34;] [7.670477, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.670541, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 98%|██████████████████████▌| 4907/5000 [00:01\u0026lt;00:00, 4761.68it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.671051, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 67%|███████████████▍ | 3358/5000 [00:01\u0026lt;00:00, 3247.82it/s]\u0026#34;] [7.690332, \u0026#34;o\u0026#34;, \u0026#34;\\r#4, est. 0.83s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 4841.91it/s]\\r\\n\u0026#34;] [7.768065, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.76816, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 37%|████████▌ | 1860/5000 [00:01\u0026lt;00:01, 1675.78it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.76985, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.769912, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 55%|████████████▋ | 2767/5000 [00:01\u0026lt;00:00, 2453.22it/s]\u0026#34;] [7.769943, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [7.771104, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 74%|████████████████▉ | 3684/5000 [00:01\u0026lt;00:00, 3250.68it/s]\u0026#34;] [7.773417, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.77352, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 92%|█████████████████████ | 4585/5000 [00:01\u0026lt;00:00, 4085.90it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.868367, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.868446, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 41%|█████████▎ | 2031/5000 [00:01\u0026lt;00:01, 1684.31it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.871357, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 80%|██████████████████▍ | 4012/5000 [00:01\u0026lt;00:00, 3257.22it/s]\u0026#34;] [7.871835, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.871897, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 60%|█████████████▊ | 3013/5000 [00:01\u0026lt;00:00, 2440.74it/s]\\u001b[A\\u001b[A\u0026#34;] [7.8774, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\u0026#34;] [7.877474, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 100%|██████████████████████▉| 4995/5000 [00:01\u0026lt;00:00, 4042.20it/s]\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.878852, \u0026#34;o\u0026#34;, \u0026#34;\\r#3, est. 1.00s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 4096.34it/s]\\r\\n\u0026#34;] [7.973438, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 87%|███████████████████▉ | 4338/5000 [00:01\u0026lt;00:00, 3239.50it/s]\u0026#34;] [7.974947, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [7.975032, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 44%|██████████ | 2200/5000 [00:01\u0026lt;00:01, 1653.04it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [7.975125, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [7.975179, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 65%|██████████████▉ | 3258/5000 [00:01\u0026lt;00:00, 2419.88it/s]\\u001b[A\\u001b[A\u0026#34;] [8.076423, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 93%|█████████████████████▍ | 4663/5000 [00:01\u0026lt;00:00, 3213.92it/s]\u0026#34;] [8.077467, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.077557, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 70%|████████████████ | 3501/5000 [00:01\u0026lt;00:00, 2406.05it/s]\\u001b[A\\u001b[A\u0026#34;] [8.079458, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.079529, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 47%|██████████▉ | 2366/5000 [00:01\u0026lt;00:01, 1633.21it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.178546, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 100%|██████████████████████▉| 4985/5000 [00:01\u0026lt;00:00, 3195.52it/s]\u0026#34;] [8.179391, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.179504, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 75%|█████████████████▏ | 3742/5000 [00:01\u0026lt;00:00, 2393.54it/s]\\u001b[A\\u001b[A\u0026#34;] [8.182663, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.182739, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 51%|███████████▋ | 2530/5000 [00:01\u0026lt;00:01, 1619.95it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.183485, \u0026#34;o\u0026#34;, \u0026#34;\\r#2, est. 1.20s: 100%|███████████████████████| 5000/5000 [00:01\u0026lt;00:00, 3275.13it/s]\\r\\n\u0026#34;] [8.281006, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.281153, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 80%|██████████████████▎ | 3982/5000 [00:01\u0026lt;00:00, 2384.53it/s]\\u001b[A\\u001b[A\u0026#34;] [8.285647, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.285786, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 54%|████████████▍ | 2693/5000 [00:01\u0026lt;00:01, 1608.86it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.382223, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.382445, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 84%|███████████████████▍ | 4221/5000 [00:01\u0026lt;00:00, 2377.93it/s]\\u001b[A\\u001b[A\u0026#34;] [8.387564, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.387724, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 57%|█████████████▏ | 2854/5000 [00:01\u0026lt;00:01, 1600.17it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.483547, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.483742, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 89%|████████████████████▌ | 4459/5000 [00:01\u0026lt;00:00, 2369.39it/s]\\u001b[A\\u001b[A\u0026#34;] [8.489688, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.489909, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 60%|█████████████▊ | 3015/5000 [00:01\u0026lt;00:01, 1593.19it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.584999, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.585144, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 94%|█████████████████████▌ | 4696/5000 [00:01\u0026lt;00:00, 2359.34it/s]\u0026#34;] [8.585323, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [8.59157, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.591749, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 64%|██████████████▌ | 3175/5000 [00:01\u0026lt;00:01, 1586.38it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.685366, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\u0026#34;] [8.685433, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 99%|██████████████████████▋| 4932/5000 [00:02\u0026lt;00:00, 2356.50it/s]\u0026#34;] [8.685557, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[A\\u001b[A\u0026#34;] [8.692435, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.692538, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 67%|███████████████▎ | 3334/5000 [00:02\u0026lt;00:01, 1583.22it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.714479, \u0026#34;o\u0026#34;, \u0026#34;\\r#1, est. 1.70s: 100%|███████████████████████| 5000/5000 [00:02\u0026lt;00:00, 2430.65it/s]\\r\\n\u0026#34;] [8.792931, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.793065, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 70%|████████████████ | 3493/5000 [00:02\u0026lt;00:00, 1583.51it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.893839, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.894077, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 73%|████████████████▊ | 3652/5000 [00:02\u0026lt;00:00, 1581.30it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [8.995009, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [8.995199, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 76%|█████████████████▌ | 3811/5000 [00:02\u0026lt;00:00, 1578.37it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.095512, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.095724, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 79%|██████████████████▎ | 3969/5000 [00:02\u0026lt;00:00, 1576.50it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.196695, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.19733, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 83%|██████████████████▉ | 4127/5000 [00:02\u0026lt;00:00, 1573.48it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.299529, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.300196, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 86%|███████████████████▋ | 4285/5000 [00:02\u0026lt;00:00, 1562.31it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.401289, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.401869, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 89%|████████████████████▍ | 4442/5000 [00:02\u0026lt;00:00, 1556.92it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.502155, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.502554, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 92%|█████████████████████▏ | 4598/5000 [00:02\u0026lt;00:00, 1553.27it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.602158, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.602435, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 95%|█████████████████████▊ | 4754/5000 [00:02\u0026lt;00:00, 1553.92it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.703135, \u0026#34;o\u0026#34;, \u0026#34;\\r\\n\\r\\n\\r\\n\u0026#34;] [9.704004, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 98%|██████████████████████▌| 4911/5000 [00:03\u0026lt;00:00, 1555.83it/s]\\u001b[A\\u001b[A\\u001b[A\u0026#34;] [9.761857, \u0026#34;o\u0026#34;, \u0026#34;\\r#0, est. 2.50s: 100%|███████████████████████| 5000/5000 [00:03\u0026lt;00:00, 1610.96it/s]\\r\\n\u0026#34;] [9.776273, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [9.776549, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [9.790267, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [9.790433, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [11.262646, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [11.329092, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: recording finished\\u001b[0m\\r\\n\u0026#34;] [11.329167, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[0;32masciinema: press \u0026lt;enter\u0026gt; to upload to asciinema.org, \u0026lt;ctrl-c\u0026gt; to save locally\\u001b[0m\\r\\n\u0026#34;] [12.316311, \u0026#34;o\u0026#34;, \u0026#34;^C\u0026#34;] [12.316399, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0;32masciinema: asciicast saved to /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[0m\\r\\n\u0026#34;] [12.326841, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [12.326974, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [12.338523, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [12.338637, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [13.935243, \u0026#34;o\u0026#34;, \u0026#34;s\u0026#34;] [14.097389, \u0026#34;o\u0026#34;, \u0026#34;\\bsv\u0026#34;] [14.314987, \u0026#34;o\u0026#34;, \u0026#34;g\u0026#34;] [14.46684, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [15.209977, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [15.350363, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [15.428225, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [15.45927, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term\\u001b[1m \\u001b[0m\\u001b[K\\u001b[67C\\u001b[33m \\u001b[00m\\u001b[68D\u0026#34;] [16.030755, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m -\u0026#34;] [16.194477, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [16.425867, \u0026#34;o\u0026#34;, \u0026#34;i\u0026#34;] [16.508727, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [16.668521, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [16.876205, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [17.019105, \u0026#34;o\u0026#34;, \u0026#34;v\u0026#34;] [17.145906, \u0026#34;o\u0026#34;, \u0026#34;a\u0026#34;] [17.246531, \u0026#34;o\u0026#34;, \u0026#34;r\u0026#34;] [17.421279, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [17.427675, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[57C\\u001b[33m \\u001b[00m\\u001b[58D\u0026#34;] [17.909204, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/f\u0026#34;] [17.994984, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [18.158413, \u0026#34;o\u0026#34;, \u0026#34;l\u0026#34;] [18.209194, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [18.509358, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [18.515598, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[49C\\u001b[33m \\u001b[00m\\u001b[50D\u0026#34;] [19.542164, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/3\u0026#34;] [19.598027, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [19.790912, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [20.75741, \u0026#34;o\u0026#34;, \u0026#34;3\u0026#34;] [21.118463, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [21.375089, \u0026#34;o\u0026#34;, \u0026#34;c\u0026#34;] [21.564873, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [21.829205, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [21.837442, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn\\u001b[1m/\\u001b[0m\\u001b[K\\u001b[15C\\u001b[33m \\u001b[00m\\u001b[16D\u0026#34;] [23.183211, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m/T\u0026#34;] [23.603098, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [23.928987, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [24.172937, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [24.489561, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [25.657377, \u0026#34;o\u0026#34;, \u0026#34;0\u0026#34;] [26.161103, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [26.404008, \u0026#34;o\u0026#34;, \u0026#34;0\u0026#34;] [26.603732, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[31m…\\u001b[39m\u0026#34;] [26.618252, \u0026#34;o\u0026#34;, \u0026#34;\\r\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00msvg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast\\u001b[1m \\u001b[0m\\u001b[K\u0026#34;] [27.742006, \u0026#34;o\u0026#34;, \u0026#34;\\b\\u001b[0m -\u0026#34;] [27.890346, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [28.099862, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [28.166638, \u0026#34;o\u0026#34;, \u0026#34;u\u0026#34;] [28.250838, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [28.3748, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [28.551619, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [29.509291, \u0026#34;o\u0026#34;, \u0026#34;t\u0026#34;] [29.603613, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [29.712099, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [29.955365, \u0026#34;o\u0026#34;, \u0026#34;/\u0026#34;] [30.657315, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [30.843608, \u0026#34;o\u0026#34;, \u0026#34;x\u0026#34;] [31.023734, \u0026#34;o\u0026#34;, \u0026#34;a\u0026#34;] [31.108809, \u0026#34;o\u0026#34;, \u0026#34;m\u0026#34;] [31.159277, \u0026#34;o\u0026#34;, \u0026#34;p\u0026#34;] [31.341015, \u0026#34;o\u0026#34;, \u0026#34;l\u0026#34;] [31.43148, \u0026#34;o\u0026#34;, \u0026#34;e\u0026#34;] [31.533268, \u0026#34;o\u0026#34;, \u0026#34;.\u0026#34;] [31.649616, \u0026#34;o\u0026#34;, \u0026#34;s\u0026#34;] [31.831633, \u0026#34;o\u0026#34;, \u0026#34;v\u0026#34;] [32.007106, \u0026#34;o\u0026#34;, \u0026#34;g\u0026#34;] [32.651262, \u0026#34;o\u0026#34;, \u0026#34; \u0026#34;] [32.923743, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [33.065427, \u0026#34;o\u0026#34;, \u0026#34;-\u0026#34;] [33.208844, \u0026#34;o\u0026#34;, \u0026#34;w\u0026#34;] [33.328877, \u0026#34;o\u0026#34;, \u0026#34;i\u0026#34;] [33.379628, \u0026#34;o\u0026#34;, \u0026#34;n\u0026#34;] [33.44376, \u0026#34;o\u0026#34;, \u0026#34;d\u0026#34;] [33.562365, \u0026#34;o\u0026#34;, \u0026#34;o\u0026#34;] [33.692507, \u0026#34;o\u0026#34;, \u0026#34;w\u0026#34;] [34.074411, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1l\\u001b\u0026gt;\u0026#34;] [34.074507, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] [34.096816, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;svg-term --in --out /tmp/example.svg --window\\u0007\\u001b]1;svg-term\\u0007\u0026#34;] [35.927703, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[1m\\u001b[7m%\\u001b[27m\\u001b[1m\\u001b[0m \\r \\r\u0026#34;] [35.927783, \u0026#34;o\u0026#34;, \u0026#34;\\u001b]2;alukach@ds:~\\u0007\\u001b]1;~\\u0007\u0026#34;] [35.934493, \u0026#34;o\u0026#34;, \u0026#34;\\r\\u001b[0m\\u001b[27m\\u001b[24m\\u001b[J\\u001b[33m~ ➤ \\u001b[00m\\u001b[K\\u001b[76C\\u001b[33m \\u001b[00m\\u001b[77D\u0026#34;] [35.934557, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?1h\\u001b=\\u001b[?2004h\u0026#34;] [36.939863, \u0026#34;o\u0026#34;, \u0026#34;\\u001b[?2004l\\r\\r\\n\u0026#34;] Transform to SVG Now, for us to place a cast into a Github README or Issue, we need to convert the cast to an animated SVG. svg-term-cli allows us to do this with either a raw cast file or the ID of a cast uploaded to asciinema. Adding the --window flag wraps the cast in a MacOS-style terminal window.\n1 svg-term --in /var/folders/3n/3dct2kb54xn1882dpbw0vbdh0000gn/T/tmp0m0d4ri1-ascii.cast --out /tmp/example.svg --window Embedding Now that we have our animated SVG, we just need to put it online and reference it.\nREADME If you\u0026rsquo;re looking to add your SVG to a README, you can include the SVG in the git repo (e.g. in docs/) and reference it in markdown:\n1 ![example cli usage](./docs/example.svg) Issue Unfortunately, Github does not allow users to drag/drop SVGs into Github Issues.\nInstead, you must figure out your own way to upload the SVG to the internet. One simple solution is to upload it to a gist and then reference the images from your Github Issue.\n1 ![](https://gist.githubusercontent.com/alukach/7c510d45080d5b2c1d42a0309ad25411/raw/9e42a8904b8da9a289d45b44491f39a1586293fb/example.svg) Example Process Here\u0026rsquo;s an example of the entire process, from start to finish:\nyes, I did use asciinema to make a recording of me using asciinema to make a recording 🤸‍♀️\n","permalink":"https://alukach.com/posts/animated-terminal-output/","summary":"Have you just written a new ✨fancy CLI✨ and want to demo it in your Github Readme? Recording your terminal output is a nice way to demonstrate the experience.\nHere\u0026rsquo;s an example of what we\u0026rsquo;re going to make:\nSteps Install Dependencies asciinema: brew install asciinema svg-term-cli: npm install -g svg-term-cli Setup your terminal Some tips:\nFont/screen size matters. The asciinema output will look just as it does in your terminal. You\u0026rsquo;ll probably want to bump up the font-size and shrink down the terminal so that the text is legible in your README.","title":"Putting animated SVGs of Terminal Output into Github READMEs"},{"content":"At times, a developer may need to access infrastructure not available on the public internet. A common example of this is accessing a database located in a private subnet, as described in the VPC Scenario docs:\nInstances in the private subnet are back-end servers that don\u0026rsquo;t need to accept incoming traffic from the internet and therefore do not have public IP addresses; however, they can send requests to the internet using the NAT gateway.\nThe common strategy for connecting to one of these devices is to tunnel your traffic through a jump box AKA jump server AKA jump host. This can be achieved by SSH Port Forwarding AKA SSH Tunneling.\nFor a recent project, I needed a convenient way to query private databases in Python to do some repeatable data management operations. Tools like DBeaver have built-in support for connecting to databases over SSH tunnels, however I needed something more scriptable. Standing up a service in AWS would have worked however seemed to be overkill for my simple scripting needs. My goals were to 1) get auth credentials from AWS Secrets Manager (RDS places credentials in Secrets Manager by default, or at least when creating RDS instances via CDK); 2) setup a tunnel through a jumpbox to allow access to the RDS Instance; 3) run SQL queries against the DB. Automating this process in Python was not immediately clear until found the sshtunnel module. After playing around with the code for a bit, I was able to put together a utility class with Pydantic and Psycopg2 to conveniently connect to a private RDS instance via SSH tunneling. I figured I would share in the event that someone ever needs such a tool in the future.\nCode Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 import socket import contextlib import logging from typing import Any, Generator, Tuple, Optional import psycopg2 import psycopg2.extras from pydantic.main import BaseModel from sshtunnel import open_tunnel logger = logging.getLogger(__name__) class Db(BaseModel): dbname: str user: str password: str host: str port: int = 5432 @contextlib.contextmanager def cursor( self, name=None ) -\u0026gt; Generator[Tuple[Any, psycopg2.extras.DictCursor], None, None]: logger.debug(\u0026#34;Connecting to %s\u0026#34;, self.dbname) with psycopg2.connect(**self.dict()) as conn: cursor = conn.cursor(name, cursor_factory=psycopg2.extras.DictCursor) with cursor as curs: logger.debug(\u0026#34;Yielding cursor\u0026#34;) yield conn, curs logger.debug(\u0026#34;Disconnecting from %s\u0026#34;, self.dbname) @contextlib.contextmanager def create_tunnel( self, jumpbox_host: str, local_port: Optional[int] = None, jumpbox_port: int = 22, local_host: str = \u0026#34;127.0.0.1\u0026#34;, jumpbox_username: str = None, ssh_key_password: str = None, ) -\u0026gt; Generator[\u0026#34;Db\u0026#34;, None, None]: \u0026#34;\u0026#34;\u0026#34; Generates an SSH tunnel to DB via jumpbox. \u0026#34;\u0026#34;\u0026#34; if local_port is None: local_port = self._find_free_port() with open_tunnel( (jumpbox_host, jumpbox_port), ssh_username=jumpbox_username, remote_bind_address=(self.host, self.port), local_bind_address=(local_host, local_port), ssh_private_key_password=ssh_key_password, ) as tunnel: logger.debug( \u0026#34;Tunnel to %s through %s established on port %s\u0026#34;, self.host, jumpbox_host, local_port, ) yield self.copy( update={ \u0026#34;host\u0026#34;: tunnel.local_bind_host, \u0026#34;port\u0026#34;: tunnel.local_bind_port, } ) @classmethod def from_rds_credentials(cls, secret): return cls.parse_obj({\u0026#34;user\u0026#34;: secret.pop(\u0026#34;username\u0026#34;), **secret}) @staticmethod def _find_free_port() -\u0026gt; int: # https://stackoverflow.com/a/45690594/728583 with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s: s.bind((\u0026#34;\u0026#34;, 0)) s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) return s.getsockname()[1] if __name__ == \u0026#34;__main__\u0026#34;: import json import boto3 rds_secret = \u0026#34;myRdsDbSecret\u0026#34; # ARN or Secret ID jumpbox_host = \u0026#34;my-jumpbox-hostname\u0026#34; # hostname/ip address of jumpbox credentials = json.load( boto3.client(\u0026#34;secretsmanager\u0026#34;).get_secret_value(SecretId=rds_secret)[ \u0026#34;SecretString\u0026#34; ] ) private_db = Db.from_rds_credentials(credentials) with private_db.create_tunnel(jumpbox_host) as db: with db.cursor() as (conn, cur): cur.execute(\u0026#34;SELECT COUNT(*) FROM my_table;\u0026#34;) print(cur.fetchone()) ","permalink":"https://alukach.com/posts/ssh-tunnels-in-python/","summary":"At times, a developer may need to access infrastructure not available on the public internet. A common example of this is accessing a database located in a private subnet, as described in the VPC Scenario docs:\nInstances in the private subnet are back-end servers that don\u0026rsquo;t need to accept incoming traffic from the internet and therefore do not have public IP addresses; however, they can send requests to the internet using the NAT gateway.","title":"SSH tunnels in Python"},{"content":"Getting area of geometries in WGS-84/EPSG:4326 in square kilometers:\n1 2 3 4 SELECT ST_Area(geometry, false) / 10^6 sq_km FROM my_table ","permalink":"https://alukach.com/posts/wgs-84-epsg-4326-geometry-area-in-sq-km/","summary":"Getting area of geometries in WGS-84/EPSG:4326 in square kilometers:\n1 2 3 4 SELECT ST_Area(geometry, false) / 10^6 sq_km FROM my_table ","title":"Getting area of WGS-84 geometries in SqKm"},{"content":"Below is a very simple example of a script that I write and re-write more often than I would like to admit. It reads input data from a CSV and processes each row concurrently. A progress bar provides updates. Honestly, it\u0026rsquo;s pretty much just the concurrent.futures ThreadPoolExecutor example plus a progress bar.\n","permalink":"https://alukach.com/posts/concurrent-python-example-script/","summary":"Below is a very simple example of a script that I write and re-write more often than I would like to admit. It reads input data from a CSV and processes each row concurrently. A progress bar provides updates. Honestly, it\u0026rsquo;s pretty much just the concurrent.futures ThreadPoolExecutor example plus a progress bar.","title":"Concurrent Python Example Script"},{"content":"Below is a simple script to deploy a Docker image to ECR\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo \u0026#34;${bold}${1}${normal}\u0026#34; 1\u0026gt;\u0026amp;2; } if [ -z \u0026#34;${AWS_ACCOUNT}\u0026#34; ]; then log \u0026#34;Missing a valid AWS_ACCOUNT env variable\u0026#34;; exit 1; else log \u0026#34;Using AWS_ACCOUNT \u0026#39;${AWS_ACCOUNT}\u0026#39;\u0026#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log \u0026#34;🔑 Authenticating...\u0026#34; aws ecr get-login-password \\ --region ${AWS_REGION} \\ | docker login \\ --username AWS \\ --password-stdin \\ ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com log \u0026#34;📦 Building image...\u0026#34; docker build -t ${REPO_NAME} . log \u0026#34;🏷️ Tagging image...\u0026#34; docker tag \\ ${REPO_NAME}:latest \\ ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/${REPO_NAME}:latest log \u0026#34;🚀 Pushing to ECR repo...\u0026#34; docker push \\ ${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/${REPO_NAME}:latest log \u0026#34;💃 Deployment Successful. 🕺\u0026#34; ","permalink":"https://alukach.com/posts/ecr-deployment-script/","summary":"Below is a simple script to deploy a Docker image to ECR\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo \u0026#34;${bold}${1}${normal}\u0026#34; 1\u0026gt;\u0026amp;2; } if [ -z \u0026#34;${AWS_ACCOUNT}\u0026#34; ]; then log \u0026#34;Missing a valid AWS_ACCOUNT env variable\u0026#34;; exit 1; else log \u0026#34;Using AWS_ACCOUNT \u0026#39;${AWS_ACCOUNT}\u0026#39;\u0026#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log \u0026#34;🔑 Authenticating.","title":"An ECR Deployment Script"},{"content":"Alternate title: How to be master of your domain.\nThe basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application\u0026rsquo;s content and services from a single domain. This minimizes a project\u0026rsquo;s TLD footprint while providing project organization and performance along the way.\nWhy Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project. This means that utilizing multiple service-specific subdomains (e.g. api.my-project.big-institution.gov or thumbnails.my-project.big-institution.gov) is an arduous process. To avoid this in a recent project, we settled on adopting a pattern where we use CloudFront to proxy all of our domain\u0026rsquo;s incoming requests to their appropriate service.\nHow it works CloudFront has the ability to support multiple origin configurations (i.e. multiple sources of content). We can utilize the Path Pattern setting to direct web requests by URL path to their appropriate service. CloudFront behaves like a typical router libraries, wherein it routes traffic to the first path with a pattern matching the incoming request and routes requests that don\u0026rsquo;t match route patterns to a default route. For example, our current infrastructure looks like this:\nmy-project.big-institution.gov/ ├── api/* \u0026lt;- Application Load Balancer (ALB) that distributes traffic to order │ management API service running on Elastic Container Service (ECS). │ ├── stac/* \u0026lt;- ALB that distributes traffic to STAC API service running on ECS. │ ├── storage/* \u0026lt;- Private S3 bucket storing private data. Only URLs that have been │ signed with our CloudFront keypair will be successful. │ ├── thumbnails/* \u0026lt;- Public S3 bucket storing thumbnail imagery. │ └── * \u0026lt;- Public S3 website bucket storing our single page application frontend. Single Page Applications An S3 bucket configured for website hosting acts as the origin for our default route. If an incoming request\u0026rsquo;s path does not match routes specified elsewhere within the CloudFront distribution, it is routed to the single page application. To configure the single page application to handle any requests provided (i.e. not just requests sent to paths of existing files within the bucket, such as index.html or app.js), the bucket should be configured with a custom error page in response to 404 errors, returning the applications HTML entrypoint (index.html).\nRequirements To enable the usage of a custom error page, the S3 bucket\u0026rsquo;s website endpoint (i.e. \u0026lt;bucket-name\u0026gt;.s3-website-\u0026lt;region\u0026gt;.amazonaws.com, not \u0026lt;bucket-name\u0026gt;.s3.\u0026lt;region\u0026gt;.amazonaws.com) must be configured as a custom origin for the distribution. Additionally, the bucket must be configured for public access. More information: Using Amazon S3 Buckets Configured as Website Endpoints for Your Origin. Being that the S3 website endpoint does not support SSL, the custom origin\u0026rsquo;s Protocol Policy should be set to HTTP Only.\nMy bucket is private. Can CloudFront serve a website from this bucket?\nIf your bucket is private, the website endpoint will not work (source). You could configure CloudFront to send traffic to the buckets REST API endpoint, however this will prevent you from being able to utilize S3\u0026rsquo;s custom error document feature which may be essential for hosting single page applications on S3. Tools like Next.js and Gatsby.js support rendering HTML documents for all routes, which can avoid the need for custom error pages; however care must be given to ensure that any dynamic portion of the page\u0026rsquo;s routes (e.g. /docs/3, where 3 is the ID of a record to be fetched from an API) must be specified as either a query parameter (e.g. /docs?3) or a hash (e.g. /docs#3).\nCloudFront itself has support for custom error pages. Why can\u0026rsquo;t I use that to enable hosting private S3 buckets as websites?\nWhile it is true that CloudFront can route error responses to custom pages (e.g. sending all 404 responses the contents of s3://my-website-bucket/index.html), these custom error pages apply to the entirety of your CloudFront distribution. This is likely undesirable for any API services hosted by your CloudFront distribution. For example, if a user accesses a RESTful API at http://my-website.com/api/notes/12345 and the API server responds with a 404 of {\u0026quot;details\u0026quot;: \u0026quot;Record not found\u0026quot;}, the response body will be re-written to contain the contents of s3://my-website-bucket/index.html. At time of writing, I am unaware of any capability of applying custom error pages to only certain content-types. A feature such as this might make distribution-wide custom error pages a viable solution.\nAPIs APIs are served as custom origins, with their Domain Name settings pointing to their an ALB\u0026rsquo;s DNS name.\nDoes this work with APIs run with Lambda or EC2?\nAssuming that the service has a DNS name, it can be set up as an origin for CloudFront. This means that for an endpoint handled by a Lambda function, you would need to have it served behind an API Gateway or an ALB.\nRecommended configuration Disable caching by setting the default, minimum, and maximum TTL to 0 seconds. Set AllowedMethods to forward all requests (i.e. GET, HEAD, OPTIONS, PUT, PATCH, POST, and DELETE). Set ForwardedValues so that querystring and the following headers are fowarded: referer, authorization, origin, accept, host Origin Protocol Policy of HTTP Only. Data from S3 Buckets Data from a standard S3 bucket can be configured by pointing to the bucket\u0026rsquo;s REST endpoint (e.g. \u0026lt;bucket-name\u0026gt;.s3.\u0026lt;region\u0026gt;.amazonaws.com). More information: Using Amazon S3 Buckets for Your Origin.\nThis can be a public bucket, in which case would benefit from the CDN and caching provided by CloudFront.\nWhen using a private bucket, CloudFront additionally can serve as a \u0026ldquo;trusted signer\u0026rdquo; to enable an application with access to the CloudFront security keys to create signed URLs/cookies to grant temporary access to particular private content. In order for CloudFront to access content within a private bucket, its Origin Access Identity must be given read privileges within the bucket\u0026rsquo;s policy. More information: Restricting Access to Amazon S3 Content by Using an Origin Access Identity\nCaveats The most substantial issue with this technique is the fact that CloudFront does not have the capability to remove portions of a path from a request\u0026rsquo;s URL. For example, if an API is configured as an origin at https://d1234abcde.cloudfront.net/api, it should be configured to respond to URLs starting with /api. This is often a non-issue, as many server frameworks have builtin support to support being hosted at a non-root path.\nConfiguring FastAPI to be served under a non-root path 1 2 3 4 5 6 7 8 9 10 11 12 from fastapi import FastAPI, APIRouter API_BASE_PATH = \u0026#39;/api\u0026#39; app = FastAPI( title=\u0026#34;Example API\u0026#34;, docs_url=API_BASE_PATH, swagger_ui_oauth2_redirect_url=f\u0026#34;{API_BASE_PATH}/oauth2-redirect\u0026#34;, openapi_url=f\u0026#34;{API_BASE_PATH}/openapi.json\u0026#34;, ) api_router = APIRouter() app.include_router(router, prefix=API_BASE_PATH) Furthermore, if you have an S3 bucket serving content from https://d1234abcde.cloudfront.net/bucket, only keys with a prefix of bucket/ will be available to that origin. In the event that keys are not prefixed with a path matching the origins configured path pattern, there are two options:\nMove all of the files, likely utilizing something like S3 Batch (see #253 for more details) Use a Lambda@Edge function to rewrite the path of any incoming request for a non-cached resource to conform to the key structure of the S3 bucket\u0026rsquo;s objects. Summary After learning this technique, it feels kind of obvious. I\u0026rsquo;m honestly not sure if this is AWS 101 level technique or something that is rarely done; however I never knew of it before this project and therefore felt it was worth sharing.\nA quick summary of some of the advantages that come with using CloudFront for all application endpoints:\nIt feels generally tidier to have all your endpoints placed behind a single domain. No more dealing with ugly ALB, API Gateway, or S3 URLs. This additionally pays off when you are dealing with multiple stages (e.g. prod and dev) of the same service 🧹. SSL is managed and terminated at CloudFront. Everything after that is port 80 non-SSL traffic, simplifying the management of certificates 🔒. All non-SSL traffic can be set to auto-redirect to SSL endpoints ↩️. Out of the box, AWS Shield Standard is applied to CloudFront to provide protection against DDoS attacks 🏰. Static content is regionally cached and served from Edge Locations closer to the viewer 🌏. Dynamic content is also served from Edge Locations, which connect to the origin server via AWS\u0026rsquo; global private network. This is faster than connecting to an origin server over the public internet 🚀. Externally, all data is served from the same domain origin. Goodbye CORS errors 👋! Data egress costs are lower through CloudFront than other services. This can be ensured by only selecting Price Class 100, other price classes can be chosen if enabling a global CDN is worth the higher egress costs 💴. Example An example of a reverse-proxy CloudFront Distribution written with CDK in Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 from aws_cdk import ( aws_s3 as s3, aws_certificatemanager as certmgr, aws_iam as iam, aws_cloudfront as cf, aws_elasticloadbalancingv2 as elbv2, core, ) class CloudfrontDistribution(core.Construct): def __init__( self, scope: core.Construct, id: str, api_lb: elbv2.ApplicationLoadBalancer, assets_bucket: s3.Bucket, website_bucket: s3.Bucket, domain_name: str = None, using_gcc_acct: bool = False, **kwargs, ) -\u0026gt; None: super().__init__(scope, id, **kwargs) oai = cf.OriginAccessIdentity( self, \u0026#34;Identity\u0026#34;, comment=\u0026#34;Allow CloudFront to access S3 Bucket\u0026#34;, ) if not using_gcc_acct: self.grant_oai_read(oai, assets_bucket) certificate = ( certmgr.Certificate(self, \u0026#34;Certificate\u0026#34;, domain_name=domain_name) if domain_name else None ) self.distribution = cf.CloudFrontWebDistribution( self, core.Stack.of(self).stack_name, alias_configuration=( cf.AliasConfiguration( acm_cert_ref=certificate.certificate_arn, names=[domain_name] ) if certificate else None ), comment=core.Stack.of(self).stack_name, origin_configs=[ # Frontend Website cf.SourceConfiguration( # NOTE: Can\u0026#39;t use S3OriginConfig because we want to treat our # bucket as an S3 Website Endpoint rather than an S3 REST API # Endpoint. This allows us to use a custom error document to # direct all requests to a single HTML document (as required # to host an SPA). custom_origin_source=cf.CustomOriginConfig( domain_name=website_bucket.bucket_website_domain_name, origin_protocol_policy=cf.OriginProtocolPolicy.HTTP_ONLY, # In website-mode, S3 only serves HTTP # noqa: E501 ), behaviors=[cf.Behavior(is_default_behavior=True)], ), # API load balancer cf.SourceConfiguration( custom_origin_source=cf.CustomOriginConfig( domain_name=api_lb.load_balancer_dns_name, origin_protocol_policy=cf.OriginProtocolPolicy.HTTP_ONLY, ), behaviors=[ cf.Behavior( path_pattern=\u0026#34;/api*\u0026#34;, # No trailing slash to permit access to root path of API # noqa: E501 allowed_methods=cf.CloudFrontAllowedMethods.ALL, forwarded_values={ \u0026#34;query_string\u0026#34;: True, \u0026#34;headers\u0026#34;: [ \u0026#34;referer\u0026#34;, \u0026#34;authorization\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;accept\u0026#34;, \u0026#34;host\u0026#34;, # Required to prevent API\u0026#39;s redirects on trailing slashes directing users to ALB endpoint # noqa: E501 ], }, # Disable caching default_ttl=core.Duration.seconds(0), min_ttl=core.Duration.seconds(0), max_ttl=core.Duration.seconds(0), ) ], ), # Assets cf.SourceConfiguration( s3_origin_source=cf.S3OriginConfig( s3_bucket_source=assets_bucket, origin_access_identity=oai, ), behaviors=[ cf.Behavior( path_pattern=\u0026#34;/storage/*\u0026#34;, trusted_signers=[\u0026#34;self\u0026#34;], ) ], ), ], ) self.assets_path = f\u0026#34;https://{self.distribution.domain_name}/storage\u0026#34; core.CfnOutput(self, \u0026#34;Endpoint\u0026#34;, value=self.distribution.domain_name) def grant_oai_read(self, oai: cf.OriginAccessIdentity, bucket: s3.Bucket): \u0026#34;\u0026#34;\u0026#34; To grant read access to our OAI, at time of writing we can not simply use `bucket.grant_read(oai)`. This is due to the fact that we are looking up our bucket by its name. For more information, see the following: https://stackoverflow.com/a/60917015/728583. As a work-around, we can manually assigned a policy statement, however this does not work in situations where a policy is already applied to the bucket (e.g. in GCC environments). \u0026#34;\u0026#34;\u0026#34; policy_statement = iam.PolicyStatement( actions=[\u0026#34;s3:GetObject*\u0026#34;, \u0026#34;s3:List*\u0026#34;], resources=[bucket.bucket_arn, f\u0026#34;{bucket.bucket_arn}/storage*\u0026#34;], principals=[], ) policy_statement.add_canonical_user_principal( oai.cloud_front_origin_access_identity_s3_canonical_user_id ) assets_policy = s3.BucketPolicy(self, \u0026#34;AssetsPolicy\u0026#34;, bucket=bucket) assets_policy.document.add_statements(policy_statement) Additional reading Amazon S3 + Amazon CloudFront: A Match Made in the Cloud Dynamic Whole Site Delivery with Amazon CloudFront ","permalink":"https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/","summary":"Alternate title: How to be master of your domain.\nThe basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application\u0026rsquo;s content and services from a single domain. This minimizes a project\u0026rsquo;s TLD footprint while providing project organization and performance along the way.\nWhy Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project.","title":"Using CloudFront as a Reverse Proxy"},{"content":"A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what\u0026rsquo;s provided via a RDS DatabaseInstance\u0026rsquo;s secret property).\n1 2 3 4 5 6 7 8 9 10 11 db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f\u0026#34;postgres://{db_val(\u0026#39;username\u0026#39;)}:{db_val(\u0026#39;password\u0026#39;)}@{db_val(\u0026#39;host\u0026#39;)}:{db_val(\u0026#39;port\u0026#39;)}/\u0026#34;, ), # ... ) ","permalink":"https://alukach.com/posts/database-uri-from-secret/","summary":"A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what\u0026rsquo;s provided via a RDS DatabaseInstance\u0026rsquo;s secret property).\n1 2 3 4 5 6 7 8 9 10 11 db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f\u0026#34;postgres://{db_val(\u0026#39;username\u0026#39;)}:{db_val(\u0026#39;password\u0026#39;)}@{db_val(\u0026#39;host\u0026#39;)}:{db_val(\u0026#39;port\u0026#39;)}/\u0026#34;, ), # ... ) ","title":"How to generate a database URI from an AWS Secret"},{"content":"I would argue that S3 is basically AWS\u0026rsquo; best service. It\u0026rsquo;s super cheap, it\u0026rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.\nHowever, when you\u0026rsquo;ve enlisted S3 to manage a large number of files (1M+), it can get complicated to do anything beyond doing simple writes and retrievals. Fortunately, there are a number of helpers available to make it manageable to working with this scale of data. This post aims to capture some common workflows that may be of use when working with huge S3 buckets.\nListing Files The mere act of listing all of the data within a huge S3 bucket is a challenge. S3\u0026rsquo;s list-objects API returns a max of 1000 items per request, meaning you\u0026rsquo;ll have to work through thousands of pages of API responses to fully list all items within the bucket. To make this simpler, we can utilize S3\u0026rsquo;s Inventory.\nAmazon S3 inventory provides comma-separated values (CSV), Apache optimized row columnar (ORC) or Apache Parquet (Parquet) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix.\nBe aware that it can take up to 48 hours to generate an Inventory Report. From that point forward, reports can be generated on a regular interval.\nAn inventory report serves as a great first-step when attempting to do any processing on an entire bucket of files. Often, you don\u0026rsquo;t need to retrieve the inventory report manually from S3. Instead, it can be fed into Athena or S3 Batch Operations as described below.\nHowever, when you do need to access the data locally, downloading and reading all of the gzipped CSV files that make up an inventory report can be somewhat tedious. The following script was written to help with this process. Its output can be piped to a local CSV file to create a single output or sent to another function for processing.\nStream S3 Inventory Report Python script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json import csv import gzip import boto3 s3 = boto3.resource(\u0026#39;s3\u0026#39;) def list_keys(bucket, manifest_key): manifest = json.load(s3.Object(bucket, manifest_key).get()[\u0026#39;Body\u0026#39;]) for obj in manifest[\u0026#39;files\u0026#39;]: gzip_obj = s3.Object(bucket_name=bucket, key=obj[\u0026#39;key\u0026#39;]) buffer = gzip.open(gzip_obj.get()[\u0026#34;Body\u0026#34;], mode=\u0026#39;rt\u0026#39;) reader = csv.reader(buffer) for row in reader: yield row if __name__ == \u0026#39;__main__\u0026#39;: bucket = \u0026#39;s3-inventory-output-bucket\u0026#39; manifest_key = \u0026#39;path/to/my/inventory/2019-12-15T00-00Z/manifest.json\u0026#39; for bucket, key, *rest in list_keys(bucket, manifest_key): print(bucket, key, *rest) Querying files by S3 Properties Sometimes you may need a subset of the files within S3, based some metadata property of the object (e.g. storage class, the key\u0026rsquo;s extension). While you can use the S3 list-objects API to list files beginning with a particular prefix, you can not filter by suffix. To get around this limitation, we can utilize AWS Athena to query over an S3 Inventory report.\n1. Create a table This example assumes that you chose CSV as the S3 Inventory Output Format. For information on other formats, review the docs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CREATE EXTERNAL TABLE your_table_name( `bucket` string, key string, version_id string, is_latest boolean, is_delete_marker boolean, size bigint, last_modified_date timestamp, e_tag string, storage_class string, is_multipart_uploaded boolean, replication_status string, encryption_status string, object_lock_retain_until_date timestamp, object_lock_mode string, object_lock_legal_hold_status string ) PARTITIONED BY (dt string) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; ESCAPED BY \u0026#39;\\\\\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\u0026#39; LOCATION \u0026#39;s3://destination-prefix/source-bucket/YOUR_CONFIG_ID/hive/\u0026#39;; 2. Add inventory reports partitions 1 MSCK REPAIR TABLE your_table_name; 3. Query for S3 keys by their filename, size, storage class, etc 1 2 3 4 SELECT storage_class, count(*) as count FROM your_table_name WHERE dt = \u0026#39;2019-12-22-00-00\u0026#39; GROUP BY storage_class More information about querying Storage Inventory files with Athena can be found here.\nProcessing Files Situations may arise where you need to run all (or a large number) of the files within an S3 bucket through some operation. S3 Batch Operations (not to be confused with AWS Batch) is built to do the following:\ncopy objects, set object tags or access control lists (ACLs), initiate object restores from Amazon S3 Glacier, or invoke an AWS Lambda function to perform custom actions using your objects.\nWith that last feature, invoking an AWS Lambda function, we can utilize Batch Operations to process a massive number of files without dealing without any of the complexity associated with data-processing infrastructure. Instead, we provide the Batch Operations with a CSV or S3 Inventory Manifest file and a Lambda function to run over each file.\nTo work with S3 Batch Operations, the lambda function must return a particular response object to describe if the process succeeded, failed, or failed but should be retried.\nS3 Batch Operation Boilerplate Python script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 import urllib import boto3 from botocore.exceptions import ClientError s3 = boto3.resource(\u0026#34;s3\u0026#34;) TMP_FAILURE = \u0026#34;TemporaryFailure\u0026#34; FAILURE = \u0026#34;PermanentFailure\u0026#34; SUCCESS = \u0026#34;Succeeded\u0026#34; def process_object(src_object): return \u0026#34;TODO: Populate with processing task...\u0026#34; def get_task_id(event): return event[\u0026#34;tasks\u0026#34;][0][\u0026#34;taskId\u0026#34;] def parse_job_parameters(event): # Parse job parameters from Amazon S3 batch operations # jobId = event[\u0026#34;job\u0026#34;][\u0026#34;id\u0026#34;] invocationId = event[\u0026#34;invocationId\u0026#34;] invocationSchemaVersion = event[\u0026#34;invocationSchemaVersion\u0026#34;] return dict( invocationId=invocationId, invocationSchemaVersion=invocationSchemaVersion ) def get_s3_object(event): # Parse Amazon S3 Key, Key Version, and Bucket ARN s3Key = urllib.parse.unquote(event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3Key\u0026#34;]) s3VersionId = event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3VersionId\u0026#34;] # Unused s3BucketArn = event[\u0026#34;tasks\u0026#34;][0][\u0026#34;s3BucketArn\u0026#34;] s3Bucket = s3BucketArn.split(\u0026#34;:::\u0026#34;)[-1] return s3.Object(s3Bucket, s3Key) def build_result(status: str, msg: str): return dict(resultCode=status, resultString=msg) def handler(event, context): task_id = get_task_id(event) job_params = parse_job_parameters(event) s3_object = get_s3_object(event) try: output = process_object(s3_object) # Mark as succeeded result = build_result(SUCCESS, output) except ClientError as e: # If request timed out, mark as a temp failure # and Amazon S3 batch operations will make the task for retry. If # any other exceptions are received, mark as permanent failure. errorCode = e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] errorMessage = e.response[\u0026#34;Error\u0026#34;][\u0026#34;Message\u0026#34;] if errorCode == \u0026#34;RequestTimeout\u0026#34;: result = build_result( TMP_FAILURE, \u0026#34;Retry request to Amazon S3 due to timeout.\u0026#34; ) else: result = build_result(FAILURE, f\u0026#34;{errorCode}: {errorMessage}\u0026#34;) except Exception as e: # Catch all exceptions to permanently fail the task result = build_result(FAILURE, f\u0026#34;Exception: {e}\u0026#34;) return { **job_params, \u0026#34;treatMissingKeysAs\u0026#34;: \u0026#34;PermanentFailure\u0026#34;, \u0026#34;results\u0026#34;: [{**result, \u0026#34;taskId\u0026#34;: task_id}], } S3 Batch Operations will then run every key through this Lambda handler, retry temporary failures, and log its results in result files. The result files are conveniently grouped by success/failure status and linked to from a Manifest Result File.\nExample Manifest Result File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;Format\u0026#34;: \u0026#34;Report_CSV_20180820\u0026#34;, \u0026#34;ReportCreationDate\u0026#34;: \u0026#34;2019-04-05T17:48:39.725Z\u0026#34;, \u0026#34;Results\u0026#34;: [ { \u0026#34;TaskExecutionStatus\u0026#34;: \u0026#34;succeeded\u0026#34;, \u0026#34;Bucket\u0026#34;: \u0026#34;my-job-reports\u0026#34;, \u0026#34;MD5Checksum\u0026#34;: \u0026#34;83b1c4cbe93fc893f54053697e10fd6e\u0026#34;, \u0026#34;Key\u0026#34;: \u0026#34;job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/6217b0fab0de85c408b4be96aeaca9b195a7daa5.csv\u0026#34; }, { \u0026#34;TaskExecutionStatus\u0026#34;: \u0026#34;failed\u0026#34;, \u0026#34;Bucket\u0026#34;: \u0026#34;my-job-reports\u0026#34;, \u0026#34;MD5Checksum\u0026#34;: \u0026#34;22ee037f3515975f7719699e5c416eaa\u0026#34;, \u0026#34;Key\u0026#34;: \u0026#34;job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/b2ddad417e94331e9f37b44f1faf8c7ed5873f2e.csv\u0026#34; } ], \u0026#34;ReportSchema\u0026#34;: \u0026#34;Bucket, Key, VersionId, TaskStatus, ErrorCode, HTTPStatusCode, ResultMessage\u0026#34; } More information about the Complete Report format can be found here.\nAt time of writing, S3 Batch Operations cost $0.25 / job + $1 / million S3 objects processed.\nPrice to process 5 million thumbnails in 2hrs:\nS3 Batch Operations: $0.25 + (5 \\* $1) = $5.25 Lambda: 128MB * 2000 ms * 5,000,000 = $21.83 S3 Get Requests: 5,000,000 / 1000 \\* $0.0004 = $2 S3 Put Requests: 5,000,000 / 1000 \\* $0.005 = $25 TOTAL: $54.08 Things not discussed in this post If you are looking for more techniques on querying data stored in S3, consider the following:\nUsing Athena to query the contents of your files stored in S3 Using S3 Select to select a subset of a single (very large) file stored in S3 ","permalink":"https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/","summary":"I would argue that S3 is basically AWS\u0026rsquo; best service. It\u0026rsquo;s super cheap, it\u0026rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.","title":"Tips for working with a large number of files in S3"},{"content":"S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.\n","permalink":"https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/","summary":"S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.","title":"Boilerplate for S3 Batch Operation Lambda"},{"content":"S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)\nHowever, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:\n","permalink":"https://alukach.com/posts/parsing-s3-inventory-output/","summary":"S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)\nHowever, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:","title":"Parsing S3 Inventory CSV output in Python"},{"content":"Here\u0026rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.\n","permalink":"https://alukach.com/posts/pil-friendly-s3-file/","summary":"Here\u0026rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.","title":"A PIL-friendly class for S3 objects"},{"content":"Let\u0026rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource\u0026rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: | #!/bin/sh echo \u0026#34;Hello world\u0026#34; In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template. This is no problem with Cloudformation\u0026rsquo;s Fn::Sub instrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: Username: Description: Username Type: String MinLength: \u0026#39;1\u0026#39; MaxLength: \u0026#39;255\u0026#39; AllowedPattern: \u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39; ConstraintDescription: must begin with a letter and contain only alphanumeric characters. Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: !Sub | #!/bin/sh echo \u0026#34;Hello ${Username}\u0026#34; The downside of the Fn::Sub function is that it does not play nice with bash\u0026rsquo; parameter substitution expressions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: Username: Description: Username Type: String MinLength: \u0026#39;1\u0026#39; MaxLength: \u0026#39;255\u0026#39; AllowedPattern: \u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39; ConstraintDescription: must begin with a letter and contain only alphanumeric characters. Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: !Sub | #!/bin/sh echo \u0026#34;Hello ${Username}\u0026#34; FOO=${FOO:-\u0026#39;bar\u0026#39;} The above template fails validation:\n1 2 3 $ aws cloudformation validate-template --template-body file://test.yaml An error occurred (ValidationError) when calling the ValidateTemplate operation: Template error: variable names in Fn::Sub syntax must contain only alphanumeric characters, underscores, periods, and colons The work-around is to rely on another intrinsic function: Fn::Join:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: Username: Description: Username Type: String MinLength: \u0026#39;1\u0026#39; MaxLength: \u0026#39;255\u0026#39; AllowedPattern: \u0026#39;[a-zA-Z][a-zA-Z0-9]*\u0026#39; ConstraintDescription: must begin with a letter and contain only alphanumeric characters. Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: !Join - \u0026#39;\\n\u0026#39; - - !Sub | #!/bin/sh echo \u0026#34;Hello ${Username}\u0026#34; - | FOO=${FOO:-\u0026#39;bar\u0026#39;} This allows you to mix CloudFormation substitutions with Bash parameter substititions.\nBonus While we\u0026rsquo;re talking about CloudFormation, another good trick comes from cloudonaut.io regarding using a Optional Parameter in CloudFormation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Parameters: KeyName: Description: (Optional) Select an ssh key pair if you will need SSH access to the machine Type: String Conditions: HasKeyName: Fn::Not: - Fn::Equals: - \u0026#39;\u0026#39; - Ref: KeyName Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium KeyName: Fn::If: - HasKeyName - !Ref KeyName - !Ref AWS::NoValue Note that the KeyName has Type: String. While Type: AWS::EC2::KeyPair::KeyName would likely be a better user experience as it would render a dropdown of all keys, it does not allow for empty values:\n\u0026hellip; if you use the AWS::EC2::KeyPair::KeyName parameter type, AWS CloudFormation validates the input value against users\u0026rsquo; existing key pair names before it creates any resources, such as Amazon EC2 instances.\n","permalink":"https://alukach.com/posts/cloudformation-sub-ref/","summary":"Let\u0026rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource\u0026rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:\n1 2 3 4 5 6 7 8 9 10 11 12 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: | #!/bin/sh echo \u0026#34;Hello world\u0026#34; In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template.","title":"Using CloudFormation's Fn::Sub with Bash parameter substitution"},{"content":"When an Esri Web AppBuilder web app is configured with a portalUrl value served from HTTPS, the web app automatically redirects users to HTTPS when visited via HTTP. While this is best-practice in production, it can be a burden in development when you want to quickly run a local version of the web app. Below is a quick script written with Python standard libraries to serve a web app over HTTP. It works by serving a config.json that is modified to use HTTP rather than HTTPS. This allows you to keep config.json using the HTTPS configuration for production but serve the web app via HTTP during development.\nThe script should be saved alongside the config.json in the root of the web app. I would recommend running chmod a+x runserver to enable you to execute the server directly via ./runserver. Alternatively, you could install this somewhere on your system path to invoke from any directory (something like cp runserver /usr/local/bin/serve-esri-app for a unix-based system).\n","permalink":"https://alukach.com/posts/serve-esri-webapp-http/","summary":"When an Esri Web AppBuilder web app is configured with a portalUrl value served from HTTPS, the web app automatically redirects users to HTTPS when visited via HTTP. While this is best-practice in production, it can be a burden in development when you want to quickly run a local version of the web app. Below is a quick script written with Python standard libraries to serve a web app over HTTP.","title":"Serve an Esri Web AppBuilder web app from HTTP"},{"content":"Full Disclosure: I am NOT an expert at Jupyter or Anaconda (which I am using in this project), there may be some bad habits below\u0026hellip;\nBelow is a quick scratchpad of the steps I took to serve Jupyter from a subdomain. Jupyter is running behind NGINX on an OpenStack Ubuntu instance and the domain\u0026rsquo;s DNS is set up to use Cloudflare to provides convenient SSL support. I was suprised by the lack of documentation for this process, prompting me to document my steps taken here.\nCloudflare Set up Cloudflare account, utilizing its provided Name Servers with my domain registration. Set up Cloudflare DNS Record for subdomain (ex jupyter to server from jupyter.mydomain.com). In the image below, the DNS entry for the Jupyter server was \u0026ldquo;greyed-out\u0026rdquo;, relegating it to \u0026ldquo;DNS Only\u0026rdquo; rather than \u0026ldquo;DNS and HTTP Proxy (CDN)\u0026rdquo;.. Now that Cloudflare supports websockets, this is no longer necessary and you\u0026rsquo;re able to take advantage of using Cloudflare as a CDN (admittedly, I\u0026rsquo;m not sure how useful this actually is, but it\u0026rsquo;s worth mentioning). Ensure Crypto settings are set correctly. You should probably be using Full SSL (Strict) rather than Flexible SSL as shown in the image below, however that is outside the scope of this post. Install Anaconda Follow instructions described here.\nSet up an Upstart script On the server, you\u0026rsquo;ll want Jupyter to start running as soon as the server starts. We\u0026rsquo;ll use an Upstart script to acheive this.\n# /etc/init/ipython-notebook.conf start on filesystem or runlevel [2345] stop on shutdown # Restart the process if it dies with a signal # or exit code not given by the \u0026#39;normal exit\u0026#39; stanza. respawn # Give up if restart occurs 10 times in 90 seconds. respawn limit 10 90 description \u0026#34;Jupyter / IPython Notebook Upstart script\u0026#34; setuid \u0026#34;MY_USER\u0026#34; setgid \u0026#34;MY_USER\u0026#34; chdir \u0026#34;/home/MY_USER/notebooks\u0026#34; script exec /home/MY_USER/.anaconda3/bin/jupyter notebook --config=\u0026#39;/home/MY_USER/.jupyter/jupyter_notebook_config.py\u0026#39; end script Configure Jupyter Populate Jupyter with required configuration. You should probably auto-generate the configuration first and then just change the applicable variables.\n1 2 3 4 5 6 7 8 9 10 11 12 # .jupyter/jupyter_notebook_config.py c.NotebookApp.allow_origin = \u0026#39;https://jupyter.mydomain.com\u0026#39; c.NotebookApp.notebook_dir = \u0026#39;/home/MY_USER/notebooks\u0026#39; c.NotebookApp.open_browser = False c.NotebookApp.password = \u0026#39;some_password_hash\u0026#39; c.NotebookApp.port = 8888 c.NotebookApp.kernel_spec_manager_class = \u0026#34;nb_conda_kernels.CondaKernelSpecManager\u0026#34; c.NotebookApp.nbserver_extensions = { \u0026#34;nb_conda\u0026#34;: True, \u0026#34;nb_anacondacloud\u0026#34;: True, \u0026#34;nbpresent\u0026#34;: True } Wire Jupyter up with Nginx To be able to access Jupyter at port 80, we\u0026rsquo;ll need to reverse proxy to the service. Nginx can take care of this for us. Jupyter uses websockets to stream data to the client, so some\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # /etc/nginx/sites-enabled/jupyter.conf # Based on example: https://gist.github.com/cboettig/8643341bd3c93b62b5c2 upstream jupyter { server 127.0.0.1:8888 fail_timeout=0; } map $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; # Make site accessible from http://localhost/ server_name localhost; client_max_body_size 50M; location / { proxy_pass http://jupyter; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } location ~* /(api/kernels/[^/]+/(channels|iopub|shell|stdin)|terminals/websocket)/? { proxy_pass http://jupyter; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } ","permalink":"https://alukach.com/posts/serving-jupyter/","summary":"Full Disclosure: I am NOT an expert at Jupyter or Anaconda (which I am using in this project), there may be some bad habits below\u0026hellip;\nBelow is a quick scratchpad of the steps I took to serve Jupyter from a subdomain. Jupyter is running behind NGINX on an OpenStack Ubuntu instance and the domain\u0026rsquo;s DNS is set up to use Cloudflare to provides convenient SSL support. I was suprised by the lack of documentation for this process, prompting me to document my steps taken here.","title":"Hosting Jupyter at a subdomain via Cloudflare"},{"content":"Continuing with the Django Admin Fu post part 1.\nAction with Intermediate Page Sometimes you may need an admin action that, when submitted, takes the user to a form where they provides some additional detail. The docs mention a bit about providing intermediate pages, but not a lot. It states:\nGenerally, something like [writing a intermediate page through the admin] isn’t considered a great idea. Most of the time, the best practice will be to return an HttpResponseRedirect and redirect the user to a view you’ve written, passing the list of selected objects in the GET query string. This allows you to provide complex interaction logic on the intermediary pages.\nI do see where the docs are coming from and it would probably be easier to do as advised, but I think there could be something said about keeping all admin logic within the admin page. Doing something like the following would take the user to an intermediate form.\n","permalink":"https://alukach.com/posts/django-admin-pt-2/","summary":"Continuing with the Django Admin Fu post part 1.\nAction with Intermediate Page Sometimes you may need an admin action that, when submitted, takes the user to a form where they provides some additional detail. The docs mention a bit about providing intermediate pages, but not a lot. It states:\nGenerally, something like [writing a intermediate page through the admin] isn’t considered a great idea. Most of the time, the best practice will be to return an HttpResponseRedirect and redirect the user to a view you’ve written, passing the list of selected objects in the GET query string.","title":"Django Admin Fu, part 2"},{"content":"I\u0026rsquo;ve been putting some time into building out the Django Admin site for one of my company\u0026rsquo;s projects. Here are some notes I\u0026rsquo;ve taken about straying away from the beaten path. I find surprisingly little information about how to do these things on StackOverflow or elsewhere. These were put used when working with Django 1.6.7.\nFake The Model, Make The View You may want a form on the Django Admin that exists along side the model views but doesn\u0026rsquo;t actually represent a model. This strays somewhat from what the Django Admin is set up to do (some on the #django channel on Freenode have stated that the admin should only be for CRUD operations on Django models.) None-the-less, if you do want to inject a form into the admin along side your models, this is a method that worked for me.\nIt revolves around generating a fake model that you register to your app\u0026rsquo;s admin view. After that, you create a model admin that inherits from the standard ModelAdmin.\nSee more in part 2.\n","permalink":"https://alukach.com/posts/django-admin-pt-1/","summary":"I\u0026rsquo;ve been putting some time into building out the Django Admin site for one of my company\u0026rsquo;s projects. Here are some notes I\u0026rsquo;ve taken about straying away from the beaten path. I find surprisingly little information about how to do these things on StackOverflow or elsewhere. These were put used when working with Django 1.6.7.\nFake The Model, Make The View You may want a form on the Django Admin that exists along side the model views but doesn\u0026rsquo;t actually represent a model.","title":"Django Admin Fu, part 1"},{"content":"Here is a quick dump of some of the better resources that I came across while learning AngularJS.\nStackOverflow: How to \u0026rsquo;think in AngularJS\u0026rsquo; - Great for getting the appropriate mindset.\negghead.io\u0026rsquo;s AngularJS series by John Lindquist - Excellently cut up into discrete segments to cover fundamentals.\nIntroduction to AngularJS - First in a series of developing an Angular app. Then watch [End to End](End to End with Angular JS), Security, Frontend Workflows, Testing\n","permalink":"https://alukach.com/posts/notes-on-learning-angularjs/","summary":"Here is a quick dump of some of the better resources that I came across while learning AngularJS.\nStackOverflow: How to \u0026rsquo;think in AngularJS\u0026rsquo; - Great for getting the appropriate mindset.\negghead.io\u0026rsquo;s AngularJS series by John Lindquist - Excellently cut up into discrete segments to cover fundamentals.\nIntroduction to AngularJS - First in a series of developing an Angular app. Then watch [End to End](End to End with Angular JS), Security, Frontend Workflows, Testing","title":"Learning AngularJS"},{"content":"As I was transitioning from SublimeText2 to SublimeText3, it became apparent that I should keep a copy of my favorite text editor\u0026rsquo;s plugins and settings.\n","permalink":"https://alukach.com/posts/sublimetext3-setup/","summary":"As I was transitioning from SublimeText2 to SublimeText3, it became apparent that I should keep a copy of my favorite text editor\u0026rsquo;s plugins and settings.","title":"SublimeText3 Setup"},{"content":"I\u0026rsquo;ve been experimenting with Python\u0026rsquo;s Natural Language Toolkit, following along with Steven Bird, Ewan Klein, and Edward Loper\u0026rsquo;s book \u0026ldquo;Natural Language Processing with Python \u0026mdash; Analyzing Text with the Natural Language Toolkit\u0026rdquo; (pdf version).\nSo far, the book\u0026rsquo;s been great. As I\u0026rsquo;m going through the book, I\u0026rsquo;ve been writing down notes relating to the book\u0026rsquo;s examples. I\u0026rsquo;ve made a Github repo to store these notes and experiments that I may be doing using the NLTK here.\n","permalink":"https://alukach.com/posts/nltk-notes/","summary":"I\u0026rsquo;ve been experimenting with Python\u0026rsquo;s Natural Language Toolkit, following along with Steven Bird, Ewan Klein, and Edward Loper\u0026rsquo;s book \u0026ldquo;Natural Language Processing with Python \u0026mdash; Analyzing Text with the Natural Language Toolkit\u0026rdquo; (pdf version).\nSo far, the book\u0026rsquo;s been great. As I\u0026rsquo;m going through the book, I\u0026rsquo;ve been writing down notes relating to the book\u0026rsquo;s examples. I\u0026rsquo;ve made a Github repo to store these notes and experiments that I may be doing using the NLTK here.","title":"Natural Language Toolkit Notes"},{"content":"Becoming tired of typing paths repeatedly in the terminal, I realized that I should be using pushd and popd to be navigating directory structures. For those uninitiated, pushd changes your current directory in a similar fashion to cd but additionally adds the former directory to a stack. You can later return to the former directory by executing popd, popping it from the directory history. Unfortunately, the commands pushd and popd both require at least twice as many characters to type as cd and additionally come with the overhead of having to learnt o use a new command instead of something that is nearly instinctual. Then it came to me: pushd all the time.\nOverriding cd with a muted pushd operates exactly like the standard cd command, with the added benefity that the path history is saved. Furthermore, adding an alias of p to popd allows the previous directory to be popped with minimal effort.\nAdditionally, when exploring the idea, I came across this StackExchange post illustrating a back function, allowing you to switch back and forth between your current and previous directory with removing either from the stack. In the end, this is what I put in my bash profile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # CD is now silent pushd cd() { if [ $# -eq 0 ]; then DIR=\u0026#34;${HOME}\u0026#34; else DIR=\u0026#34;$1\u0026#34; fi builtin pushd \u0026#34;${DIR}\u0026#34; \u0026gt; /dev/null } # Take you back without popd back() { builtin pushd \u0026gt; /dev/null dirs } alias p=\u0026#39;popd\u0026#39; alias b=\u0026#39;back\u0026#39; ","permalink":"https://alukach.com/posts/pushd-all-day-long/","summary":"Becoming tired of typing paths repeatedly in the terminal, I realized that I should be using pushd and popd to be navigating directory structures. For those uninitiated, pushd changes your current directory in a similar fashion to cd but additionally adds the former directory to a stack. You can later return to the former directory by executing popd, popping it from the directory history. Unfortunately, the commands pushd and popd both require at least twice as many characters to type as cd and additionally come with the overhead of having to learnt o use a new command instead of something that is nearly instinctual.","title":"pushd and popd forever"},{"content":"The other week I found myself up at 2am in Canada setting up a VPN between my home computer (running Ubuntu) in Seattle and my laptop \u0026lt;partyhard.jpg\u0026gt;. I had enabled SSH access on my home computer and had set up port forwarding on my router to allow for access from the outside world ahead of time, but had forgotten that I would need to have a port forwarded for the VPN server as well. I tried to SSH into my home box and access the router\u0026rsquo;s admin interface from the commandline browser (using Lynx and w3m). This was a bad idea and didn\u0026rsquo;t work, as the browser\u0026rsquo;s admin page required JavaScript for some odd reason.\nAnd then I remembered this command:\n1 ssh -D 8080 -Nf login@server.whatever.com Pointed my browser\u0026rsquo;s connection settings to SOCKS proxy with server as localhost and port at 8080 and BOOM, was able to access my Seattle home\u0026rsquo;s router\u0026rsquo;s config page from Canada. I\u0026rsquo;ve found this trick useful for all sorts of things, typically for one-offs where I need to access a website from the US while in Canada.\nEDIT:\nAnother useful command for when you need to connect to any given port on a remote server is the following:\n1 ssh -N -L [local_port]:[endpoint]:[remote_port] [user]@[host] ","permalink":"https://alukach.com/posts/ssh-port-forwarding/","summary":"The other week I found myself up at 2am in Canada setting up a VPN between my home computer (running Ubuntu) in Seattle and my laptop \u0026lt;partyhard.jpg\u0026gt;. I had enabled SSH access on my home computer and had set up port forwarding on my router to allow for access from the outside world ahead of time, but had forgotten that I would need to have a port forwarded for the VPN server as well.","title":"SSH Port Forwarding"},{"content":"I\u0026rsquo;m just getting things set up with this new blog. I\u0026rsquo;ve been hearing about this movement towards static-generated blogs for a while now, ever since reading this article about the Obama Campaign\u0026rsquo;s fundraising platform. The idea of stepping away from databases and convulated CMS\u0026rsquo;s and PHP attracted me.\nThis site is built with Jekyll. After seeing how simple the template syntax was (based on LiquidMarkup, not unlike Django or Jinja2\u0026rsquo;s syntax), I was sold. Furthermore, learning that Github would post the site for free made it a no-brainer.\nSo, now this page is up, hosted for free by Github and running a modified version of Zach Holman\u0026rsquo;s Left theme augmented with some pieces from JekyllBootstrap.\n","permalink":"https://alukach.com/posts/hello-world/","summary":"I\u0026rsquo;m just getting things set up with this new blog. I\u0026rsquo;ve been hearing about this movement towards static-generated blogs for a while now, ever since reading this article about the Obama Campaign\u0026rsquo;s fundraising platform. The idea of stepping away from databases and convulated CMS\u0026rsquo;s and PHP attracted me.\nThis site is built with Jekyll. After seeing how simple the template syntax was (based on LiquidMarkup, not unlike Django or Jinja2\u0026rsquo;s syntax), I was sold.","title":"Hello World"},{"content":"\u003c!DOCTYPE html\u003e Loom Embed Wizard Loom Embed Generator Enter your Loom share URL below:\nHide Share Button (removes the button linking out to the Loom page)\nHide Embed Top Bar (removes the top bar that includes the title, owner, and share link)\nHide Title (hides the video title from the embedded video)\nHide Owner (hides the video creator's avatar before the video plays)\nMute (starts the video muted)\nAutoplay (immediately plays the embedded video once loaded)\nTimestamp (e.g., 80s or 1m20s): Preview: Embed Code: ","permalink":"https://alukach.com/loom-embed-wizard/","summary":"\u003c!DOCTYPE html\u003e Loom Embed Wizard Loom Embed Generator Enter your Loom share URL below:\nHide Share Button (removes the button linking out to the Loom page)\nHide Embed Top Bar (removes the top bar that includes the title, owner, and share link)\nHide Title (hides the video title from the embedded video)\nHide Owner (hides the video creator's avatar before the video plays)\nMute (starts the video muted)\nAutoplay (immediately plays the embedded video once loaded)","title":""}]