<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>aws on anthony lukach</title><link>https://alukach.com/tags/aws/</link><description>Recent content in aws on anthony lukach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Anthony Lukach</copyright><lastBuildDate>Mon, 09 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>An ECR Deployment Script</title><link>https://alukach.com/posts/ecr_deployment_script/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/ecr_deployment_script/</guid><description>Below is a simple script to deploy a Docker image to ECR&amp;hellip;
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo &amp;#34;${bold}${1}${normal}&amp;#34; 1&amp;gt;&amp;amp;2; } if [ -z &amp;#34;${AWS_ACCOUNT}&amp;#34; ]; then log &amp;#34;Missing a valid AWS_ACCOUNT env variable&amp;#34;; exit 1; else log &amp;#34;Using AWS_ACCOUNT &amp;#39;${AWS_ACCOUNT}&amp;#39;&amp;#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log &amp;#34;ðŸ”‘ Authenticating.</description></item><item><title>Using CloudFront as a Reverse Proxy</title><link>https://alukach.com/posts/using_cloudfont_as_a_reverse_proxy/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/using_cloudfont_as_a_reverse_proxy/</guid><description>Alternate title: How to be master of your domain.
The basic idea of this ticket is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application&amp;rsquo;s content and services from a single domain. This minimizes a project&amp;rsquo;s TLD footprint while providing project organization and performance along the way.
Why Within the large organizations, it can be a pain to get a subdomain for a project.</description></item><item><title>How to generate a database URI from an AWS Secret</title><link>https://alukach.com/posts/database_uri_from_secret/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/database_uri_from_secret/</guid><description>A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what&amp;rsquo;s provided via a RDS DatabaseInstance&amp;rsquo;s secret property).
1 2 3 4 5 6 7 8 9 10 11 db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f&amp;#34;postgres://{db_val(&amp;#39;username&amp;#39;)}:{db_val(&amp;#39;password&amp;#39;)}@{db_val(&amp;#39;host&amp;#39;)}:{db_val(&amp;#39;port&amp;#39;)}/&amp;#34;, ), # ... )</description></item><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips_for_working_with_a_large_number_of_files_in_s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips_for_working_with_a_large_number_of_files_in_s3/</guid><description>I would argue that S3 is basically AWS' best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Using CloudFormation's Fn::Sub with Bash parameter substitution</title><link>https://alukach.com/posts/cloudformation-sub-ref/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/cloudformation-sub-ref/</guid><description>Let&amp;rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource&amp;rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:
1 2 3 4 5 6 7 8 9 10 11 12 AWSTemplateFormatVersion:&amp;#39;2010-09-09&amp;#39;Resources:VPNServerInstance:Type:AWS::EC2::InstanceProperties:ImageId:ami-efd0428fInstanceType:m3.mediumUserData:Fn::Base64:|#!/bin/sh echo &amp;#34;Hello world&amp;#34; In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template. This is no problem with Cloudformation&amp;rsquo;s Fn::Sub instrinsic function:</description></item></channel></rss>