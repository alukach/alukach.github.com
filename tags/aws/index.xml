<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Aws on anthony lukach</title><link>https://alukach.com/tags/aws/</link><description>Recent content in Aws on anthony lukach</description><generator>Hugo -- 0.128.0</generator><language>en-us</language><lastBuildDate>Fri, 19 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Hosting websites from private S3 buckets</title><link>https://alukach.com/posts/hosting-websites-from-private-s3-buckets/</link><pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/hosting-websites-from-private-s3-buckets/</guid><description>At work, we were alerted to an outage of an S3-backed frontend. The frontend was returning 403 responses. This left us scratching our head, as no deployment had occurred recently. After doing some digging, we found that AWS account administrators had applied a new policy to make all S3 buckets private (this is an account-wide setting, overriding bucket-level settings). ðŸ†’ ðŸ†’ ðŸ†’
So how can we configure Cloudfront to access private S3 buckets?</description></item><item><title>An ECS -> RDS Security Group Script</title><link>https://alukach.com/posts/ecs-rds-security-group-script/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/ecs-rds-security-group-script/</guid><description>Below is a simple script to allow a user to alter RDS databases security groups to allow access from an ECS Service. Useful when we have an observability tool runing in ECS that wants to add RDS data connections.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 from typing import List, Dict import boto3 from botocore.</description></item><item><title>Normalizing heterogeneous decimal Ion data in Athena</title><link>https://alukach.com/posts/heterogeneous-ion-decimal-data/</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/heterogeneous-ion-decimal-data/</guid><description>Recently, we exported data from a DynamoDB table to S3 in AWS Ion format. However, due to the fact that the DynamoDB table had varied formats for some numeric properties, the export serialized these numeric data columns in a few different formats: as a decimal (1234.), as an Ion decimal type (1234d0), and as a string (&amp;quot;1234&amp;quot;). However, we want to be able to treat these values as a bigint within our Athena queries.</description></item><item><title>Security-conscious cloud deployments from Github Actions via OpenID Connect</title><link>https://alukach.com/posts/oidc-github-actions/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/oidc-github-actions/</guid><description>Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.
Why is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:</description></item><item><title>An ECR Deployment Script</title><link>https://alukach.com/posts/ecr-deployment-script/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/ecr-deployment-script/</guid><description>Below is a simple script to deploy a Docker image to ECR&amp;hellip;
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo &amp;#34;${bold}${1}${normal}&amp;#34; 1&amp;gt;&amp;amp;2; } if [ -z &amp;#34;${AWS_ACCOUNT}&amp;#34; ]; then log &amp;#34;Missing a valid AWS_ACCOUNT env variable&amp;#34;; exit 1; else log &amp;#34;Using AWS_ACCOUNT &amp;#39;${AWS_ACCOUNT}&amp;#39;&amp;#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log &amp;#34;ðŸ”‘ Authenticating.</description></item><item><title>Using CloudFront as a Reverse Proxy</title><link>https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/</guid><description>Alternate title: How to be master of your domain.
The basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application&amp;rsquo;s content and services from a single domain. This minimizes a project&amp;rsquo;s TLD footprint while providing project organization and performance along the way.
Why Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project.</description></item><item><title>How to generate a database URI from an AWS Secret</title><link>https://alukach.com/posts/database-uri-from-secret/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/database-uri-from-secret/</guid><description>A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what&amp;rsquo;s provided via a RDS DatabaseInstance&amp;rsquo;s secret property).
1 2 3 4 5 6 7 8 9 10 11 db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f&amp;#34;postgres://{db_val(&amp;#39;username&amp;#39;)}:{db_val(&amp;#39;password&amp;#39;)}@{db_val(&amp;#39;host&amp;#39;)}:{db_val(&amp;#39;port&amp;#39;)}/&amp;#34;, ), # ... )</description></item><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</guid><description>I would argue that S3 is basically AWS&amp;rsquo; best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Using CloudFormation's Fn::Sub with Bash parameter substitution</title><link>https://alukach.com/posts/cloudformation-sub-ref/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/cloudformation-sub-ref/</guid><description>Let&amp;rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource&amp;rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:
1 2 3 4 5 6 7 8 9 10 11 12 AWSTemplateFormatVersion: &amp;#39;2010-09-09&amp;#39; Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: | #!/bin/sh echo &amp;#34;Hello world&amp;#34; In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template.</description></item></channel></rss>