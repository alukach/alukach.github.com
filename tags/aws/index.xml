<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>aws on anthony lukach</title><link>https://alukach.com/tags/aws/</link><description>Recent content in aws on anthony lukach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 21 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Normalizing heterogeneous decimal Ion data in Athena</title><link>https://alukach.com/posts/heterogeneous-ion-decimal-data/</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/heterogeneous-ion-decimal-data/</guid><description>Recently, we exported data from a DynamoDB table to S3 in AWS Ion format. However, due to the fact that the DynamoDB table had varied formats for some numeric properties, the export serialized these numeric data columns in a few different formats: as a decimal (1234.), as an Ion decimal type (123d1), and as a string (&amp;quot;1234&amp;quot;).
When querying this data in Athena, the following SQL did the trick to convert any of those formats into a bigint:</description></item><item><title>Security-conscious cloud deployments from Github Actions via OpenID Connect</title><link>https://alukach.com/posts/oidc-github-actions/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/oidc-github-actions/</guid><description>Goals This ticket is focused on how we can securely deploy to a major cloud provider environment (e.g. AWS, Azure, GCP) from within our Github Actions workflows.
Why is this challenging? A naive solution to this problem is to generate some cloud provider credentials (e.g. AWS Access Keys) and to store them as a Github Secret. Our Github Actions can then utilize these credentials in its workflows. However, this technique contains a number of concerns:</description></item><item><title>An ECR Deployment Script</title><link>https://alukach.com/posts/ecr-deployment-script/</link><pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/ecr-deployment-script/</guid><description>Below is a simple script to deploy a Docker image to ECR&amp;hellip;
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 set -e log () { local bold=$(tput bold) local normal=$(tput sgr0) echo &amp;#34;${bold}${1}${normal}&amp;#34; 1&amp;gt;&amp;amp;2; } if [ -z &amp;#34;${AWS_ACCOUNT}&amp;#34; ]; then log &amp;#34;Missing a valid AWS_ACCOUNT env variable&amp;#34;; exit 1; else log &amp;#34;Using AWS_ACCOUNT &amp;#39;${AWS_ACCOUNT}&amp;#39;&amp;#34;; fi AWS_REGION=${AWS_REGION:-us-east-1} REPO_NAME=${REPO_NAME:-my/repo} log &amp;#34;ðŸ”‘ Authenticating.</description></item><item><title>Using CloudFront as a Reverse Proxy</title><link>https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/using-cloudfont-as-a-reverse-proxy/</guid><description>Alternate title: How to be master of your domain.
The basic idea of this post is to demonstrate how CloudFront can be utilized as a serverless reverse-proxy, allowing you to host all of your application&amp;rsquo;s content and services from a single domain. This minimizes a project&amp;rsquo;s TLD footprint while providing project organization and performance along the way.
Why Within large organizations, bureaucracy can make it a challenge to obtain a subdomain for a project.</description></item><item><title>How to generate a database URI from an AWS Secret</title><link>https://alukach.com/posts/database-uri-from-secret/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/database-uri-from-secret/</guid><description>A quick note about how to generate a database URI (or any other derived string) from an AWS SecretsManager SecretTargetAttachment (such as what&amp;rsquo;s provided via a RDS DatabaseInstance&amp;rsquo;s secret property).
1 2 3 4 5 6 7 8 9 10 11 db = rds.DatabaseInstance( # ... ) db_val = lambda field: db.secret.secret_value_from_json(field).to_string() task_definition.add_container( environment=dict( # ... PGRST_DB_URI=f&amp;#34;postgres://{db_val(&amp;#39;username&amp;#39;)}:{db_val(&amp;#39;password&amp;#39;)}@{db_val(&amp;#39;host&amp;#39;)}:{db_val(&amp;#39;port&amp;#39;)}/&amp;#34;, ), # ... )</description></item><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</guid><description>I would argue that S3 is basically AWS&amp;rsquo; best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Using CloudFormation's Fn::Sub with Bash parameter substitution</title><link>https://alukach.com/posts/cloudformation-sub-ref/</link><pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/cloudformation-sub-ref/</guid><description>Let&amp;rsquo;s say that you need to inject a large bash script into a CloudFormation AWS::EC2::Instance Resource&amp;rsquo;s UserData property. CloudFormation makes this easy with the Fn::Base64 intrinsic function:
1 2 3 4 5 6 7 8 9 10 11 12 AWSTemplateFormatVersion: &amp;#39;2010-09-09&amp;#39; Resources: VPNServerInstance: Type: AWS::EC2::Instance Properties: ImageId: ami-efd0428f InstanceType: m3.medium UserData: Fn::Base64: | #!/bin/sh echo &amp;#34;Hello world&amp;#34; In your bash script, you may even want to reference a parameter created elsewhere in the CloudFormation template.</description></item></channel></rss>