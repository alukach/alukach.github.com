<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>S3 on anthony lukach</title><link>https://alukach.com/tags/s3/</link><description>Recent content in S3 on anthony lukach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 21 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/s3/index.xml" rel="self" type="application/rss+xml"/><item><title>Roll your own PR preview CI pipeline</title><link>https://alukach.com/posts/diy-pr-previews/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/diy-pr-previews/</guid><description>Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.
Other Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.</description></item><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</guid><description>I would argue that S3 is basically AWS&amp;rsquo; best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Parsing S3 Inventory CSV output in Python</title><link>https://alukach.com/posts/parsing-s3-inventory-output/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/parsing-s3-inventory-output/</guid><description>S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)
However, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:</description></item><item><title>A PIL-friendly class for S3 objects</title><link>https://alukach.com/posts/pil-friendly-s3-file/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/pil-friendly-s3-file/</guid><description>Here&amp;rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.</description></item></channel></rss>