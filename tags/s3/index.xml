<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>s3 on anthony lukach</title><link>https://alukach.com/tags/s3/</link><description>Recent content in s3 on anthony lukach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Anthony Lukach</copyright><lastBuildDate>Sat, 30 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/s3/index.xml" rel="self" type="application/rss+xml"/><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</guid><description>I would argue that S3 is basically AWS' best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Parsing S3 Inventory CSV output in Python</title><link>https://alukach.com/posts/parsing-s3-inventory-output/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/parsing-s3-inventory-output/</guid><description>S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)
However, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:</description></item><item><title>A PIL-friendly class for S3 objects</title><link>https://alukach.com/posts/pil-friendly-s3-file/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/pil-friendly-s3-file/</guid><description>Here&amp;rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.</description></item></channel></rss>