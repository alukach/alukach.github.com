<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>S3 on anthony lukach</title><link>https://alukach.com/tags/s3/</link><description>Recent content in S3 on anthony lukach</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 19 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://alukach.com/tags/s3/index.xml" rel="self" type="application/rss+xml"/><item><title>Hosting websites from private S3 buckets</title><link>https://alukach.com/posts/hosting-websites-from-private-s3-buckets/</link><pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/hosting-websites-from-private-s3-buckets/</guid><description>At work, we were alerted to an outage of an S3-backed frontend. The frontend was returning 403 responses. This left us scratching our head, as no deployment had occurred recently. After doing some digging, we found that AWS account administrators had applied a new policy to make all S3 buckets private (this is an account-wide setting, overriding bucket-level settings). ðŸ†’ ðŸ†’ ðŸ†’
So how can we configure Cloudfront to access private S3 buckets?</description></item><item><title>Roll your own PR preview CI pipeline</title><link>https://alukach.com/posts/diy-pr-previews/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/diy-pr-previews/</guid><description>Goal We want a CI pipeline that will build and deploy an instance of our frontend application for every PR created in our frontend repo. Additionally, we want to be able to easily spin up applications with overridden configuration to allow developers to test the frontend against experimental backends. Finally, we want a reporting mechanism to inform developers when and where these deployed environments are available.
Other Options Before you jump into this, consider that there are out-of-the-box solutions to solve this problem mentioned in the followup at the bottom of this page.</description></item><item><title>Tips for working with a large number of files in S3</title><link>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/tips-for-working-with-a-large-number-of-files-in-s3/</guid><description>I would argue that S3 is basically AWS&amp;rsquo; best service. It&amp;rsquo;s super cheap, it&amp;rsquo;s basically infinitely scalable, and it never goes down (except for when it does). Part of its beauty is its simplicity. You give it a file and a key to identify that file, you can have faith that it will store it without issue. You give it a key, you can have faith that it will return the file represented by that key, assuming there is one.</description></item><item><title>Boilerplate for S3 Batch Operation Lambda</title><link>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</link><pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/aws-s3-batch-operation-lambda-boilerplate/</guid><description>S3 Batch Operation provide a simple way to process a number of files stored in an S3 bucket with a Lambda function. However, the Lambda function must return particular Response Codes. Below is an example of a Lambda function written in Python that works with AWS S3 Batch Operations.</description></item><item><title>Parsing S3 Inventory CSV output in Python</title><link>https://alukach.com/posts/parsing-s3-inventory-output/</link><pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/parsing-s3-inventory-output/</guid><description>S3 Inventory is a great way to access a large number of keys in an S3 Bucket. Its output is easily parsed by AWS Athena, enabling queries across the key names (e.g. find all keys ending with .png)
However, sometimes you just need to list all of the keys mentioned in the S3 Inventory output (e.g. populating an SQS queue with every keyname mentioned in an inventory output). The following code is an example of doing such task in Python:</description></item><item><title>A PIL-friendly class for S3 objects</title><link>https://alukach.com/posts/pil-friendly-s3-file/</link><pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate><guid>https://alukach.com/posts/pil-friendly-s3-file/</guid><description>Here&amp;rsquo;s a quick example of creating an file-like object in Python that represents an object on S3 and plays nicely with PIL. This ended up being overkill for my needs but I figured somebody might get some use out of it.</description></item></channel></rss>